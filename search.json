[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COMP/STAT212 Portfolio",
    "section": "",
    "text": "Welcome\nWelcome to my online portfolio for COMP/STAT112 course taken at Macalester College. Please, use the side bar on the left for navigation.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "src/pv/pv-01.html",
    "href": "src/pv/pv-01.html",
    "title": "Professional Viz Sample",
    "section": "",
    "text": "Add content here",
    "crumbs": [
      "Prof Viz",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Professional Viz Sample</span>"
    ]
  },
  {
    "objectID": "src/tt/hw01-tt.html",
    "href": "src/tt/hw01-tt.html",
    "title": "TidyTuesday 09/02/2025",
    "section": "",
    "text": "TidyTuesday Section\nExplore the week’s TidyTuesday challenge. Develop a research question, then answer it through a short data story with effective visualization(s). Provide sufficient background for readers to grasp your narrative.\nCode#Necessary Libraries\n#| include: false\n#| echo: false\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(hrbrthemes)\nlibrary(viridis)",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>TidyTuesday 09/02/2025</span>"
    ]
  },
  {
    "objectID": "src/tt/hw01-tt.html#importing-data",
    "href": "src/tt/hw01-tt.html#importing-data",
    "title": "TidyTuesday 09/02/2025",
    "section": "Importing Data",
    "text": "Importing Data\nReading in the data both about the families and genus of the frogs as well as individual frog ID events. The individual observation events were recorded frog calls by citizen scientists in Australia. The frogs were then identified via their calls by experts.\n\nCodeFrog_ID &lt;- read.csv(\"tidytuesday/data/2025/2025-09-02/frogID_data.csv\")\nFrog_names &lt;- read.csv(\"tidytuesday/data/2025/2025-09-02/frog_names.csv\")\n\n\nResearch Question: What subfamilies are the most abundant in Australia, and when are they the most abundant?",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>TidyTuesday 09/02/2025</span>"
    ]
  },
  {
    "objectID": "src/tt/hw01-tt.html#exploring-the-data",
    "href": "src/tt/hw01-tt.html#exploring-the-data",
    "title": "TidyTuesday 09/02/2025",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nIn order to create visualizations and have subfamilies corresponding to each observation I had to join the two different data sets by their scientific name.\n\nCode#Joining data sets together via scientific name and getting month of the observations\nFrog_expanded &lt;- \n  left_join(Frog_ID, Frog_names, by = \"scientificName\") |&gt; \n  mutate(eventDate = as.Date(eventDate)) |&gt; \n  mutate(month = format(eventDate, \"%b\")) \n\n\nI then created a table that shows the counts of each subfamily of frog for each month over the year of 2023 in order to get an idea of what family was most abundant and when.\n\nCode#Demonstrating the count of each subfamily for each month\nFrog_expanded |&gt; \n  na.omit(subfamily) |&gt; #omiting observations that do not have a subfamily, excludes &gt;9000 observations\n  count(month, subfamily) |&gt; \n  mutate(month = factor(month, levels = month.abb, ordered = TRUE)) |&gt; \n  arrange(month)\n\n   month     subfamily     n\n1    Jan         Hylid  8664\n2    Jan  Microhylidae   155\n3    Jan  Myobatrachid  7021\n4    Jan         Ranid    12\n5    Jan          Toad   282\n6    Feb         Hylid  3675\n7    Feb  Microhylidae   103\n8    Feb  Myobatrachid  4735\n9    Feb         Ranid     5\n10   Feb          Toad   201\n11   Mar         Hylid  1516\n12   Mar  Microhylidae    53\n13   Mar  Myobatrachid  4697\n14   Mar         Ranid    15\n15   Mar          Toad   112\n16   Apr         Hylid   883\n17   Apr  Microhylidae    21\n18   Apr  Myobatrachid  5810\n19   Apr         Ranid    22\n20   Apr          Toad    36\n21   May         Hylid   630\n22   May  Microhylidae     7\n23   May  Myobatrachid  3270\n24   May         Ranid    13\n25   May          Toad     9\n26   Jun         Hylid  1291\n27   Jun  Microhylidae    30\n28   Jun  Myobatrachid  5002\n29   Jun         Ranid    13\n30   Jun          Toad    12\n31   Jul         Hylid  1476\n32   Jul  Microhylidae    60\n33   Jul  Myobatrachid  7471\n34   Jul         Ranid    21\n35   Jul          Toad    36\n36   Aug         Hylid  2634\n37   Aug  Microhylidae    24\n38   Aug  Myobatrachid 12016\n39   Aug         Ranid    18\n40   Aug          Toad    18\n41   Sep         Hylid  5992\n42   Sep  Microhylidae    48\n43   Sep  Myobatrachid 12323\n44   Sep         Ranid    12\n45   Sep          Toad    74\n46   Oct         Hylid  6851\n47   Oct  Microhylidae    28\n48   Oct  Myobatrachid  8677\n49   Oct         Ranid    11\n50   Oct          Toad    59\n51   Nov         Hylid 11195\n52   Nov  Microhylidae    38\n53   Nov  Myobatrachid  9862\n54   Nov         Ranid    19\n55   Nov          Toad   209\n\n\nIt is important to note that in this case there are no observations for the month of December, so we cannot make a blanket statement saying the months Nov-Jan for example, as strickly speaking it is not true.",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>TidyTuesday 09/02/2025</span>"
    ]
  },
  {
    "objectID": "src/tt/hw01-tt.html#visualizations",
    "href": "src/tt/hw01-tt.html#visualizations",
    "title": "TidyTuesday 09/02/2025",
    "section": "Visualizations",
    "text": "Visualizations\nIn order to get a better idea of the abundance of each family in comparison to the overall density for each month I created the following visualization.\n\nCode#Shows the overall amount of frogs over the year compared to the \nFrog_expanded |&gt; \n  na.omit(subfamily) |&gt;\n  ggplot(aes(x = eventDate)) + \n  geom_density(aes(fill = subfamily), alpha = 0.5) +\n  geom_histogram(aes(y = after_stat(density)), binwidth = 29, alpha = 0.5) + \n  theme_minimal() + \n  scale_fill_viridis_d() + \n  labs(x = \"Date\", y = \"Density\", title = \"Density of Frogs and their Subfamilies over Time\", subtitle = \"2023\", fill = \"Subfamily\")\n\n\n\n\n\n\n\nAs we can see in above visualization is that frog calls tend to be more abundant October to January which is spring to summer in Australia. Toads most abundant around January-Febuary, late season. In comparison, Hylids are most abundant in December and January, mid to late season. Myobatrachid peaked in October as well as November. Ranid stay pretty consistent throughout the year. However the visuals layering of all the different subfamilies make it difficult to pull out exact measurements. For example, no trends for Microhylidae can be distinguished. The follow visualization splits all the different subfamiles so that they can be better compared against one another rather than the general trend.\n\nCode#Shows the distribution of each subfamily over the course of a year\nFrog_expanded |&gt; \n  na.omit(subfamily) |&gt;\n  ggplot(aes(x = eventDate, fill = subfamily)) +\n  geom_density() +\n  facet_wrap(~subfamily) + \n  labs(title = \"Distribution of Subfamilies\", subtitle = \"2023\", x = \"Date\", y = \"Probability Density\", fill = \"Subfamily\") + \n  theme_minimal() + \n  scale_fill_viridis_d()\n\n\n\n\n\n\n\nFrom this we can tell much more distinctly that Frogs that fall under the Microhylidae family tend to peak in January-Feburary while staying relatively consistent the rest of the year. However while this visual allows you to compare the density of each species relative to the total of each species not the total number of frogs or count. The following visualization addresses this issue.\n\nCode# Function to hide every other label\nevery_other_label &lt;- function(x) {\n      labels &lt;- as.character(x)\n      labels[seq(2, length(labels), 2)] &lt;- \"\"\n      return(labels)}\n\n#Shows the proportion of the different frog families over the course of the year \nFrog_expanded |&gt;\n  na.omit(subfamily) |&gt;\n  mutate(month = factor(month, levels = month.abb, ordered = TRUE)) |&gt; \n  ggplot(aes(x = month, , fill = subfamily)) + \n  geom_bar() + \n  facet_wrap(~subfamily) + \n  scale_fill_viridis_d() + \n  scale_x_discrete(labels = every_other_label) + \n  labs(x = \"Month\", y = \"Number of Frogs\", title = \"The Number of Frogs of each Subfamily per Month\", subtitle = \"2023\", fill = \"Subfamily\")\n\n\n\n\n\n\n\nThrough this visual we can clearly see that the Myobatrachid of frogs are the most abundant year round and make up most of the frogs that were documented in Australia in 2023. Hylids are the second most common subfamily, followed by Toad and Microhylidae and lastly Ranid. However because the last 3 categories are all significantly smaller than Hylids and Myobatrachid it is difficult to compare them.\nNevertheless we have answered the research question that Myobatrachids are the most abundant Frog subfamily in Australia and are more relatively abundant during the months of August to November.",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>TidyTuesday 09/02/2025</span>"
    ]
  },
  {
    "objectID": "src/tt/hw02-tt.html",
    "href": "src/tt/hw02-tt.html",
    "title": "TidyTuesday 09/09/2025",
    "section": "",
    "text": "TidyTuesday Section (optional)\nExplore the week’s TidyTuesday challenge. Develop a research question, then answer it through a short data story with effective visualization(s). Provide sufficient background for readers to grasp your narrative.\nCode#Necessary Libraries\n#| include: false\n#| echo: false\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(hrbrthemes)\nlibrary(viridis)\nlibrary(tidytuesdayR)",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>TidyTuesday 09/09/2025</span>"
    ]
  },
  {
    "objectID": "src/tt/hw02-tt.html#tidytuesday-section-optional",
    "href": "src/tt/hw02-tt.html#tidytuesday-section-optional",
    "title": "TidyTuesday 09/09/2025",
    "section": "",
    "text": "Instructions\n\n\n\nYou can count work on this week’s TidyTuesday toward the exceptional work required for an A in the Homework component.",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>TidyTuesday 09/09/2025</span>"
    ]
  },
  {
    "objectID": "src/tt/hw02-tt.html#importing-data",
    "href": "src/tt/hw02-tt.html#importing-data",
    "title": "TidyTuesday 09/09/2025",
    "section": "Importing Data",
    "text": "Importing Data\nReading in the data about individual countries as well as their rank by year. In this case they were scored based on the number of destinations that passport could travel to with no visa requried. This plays into the rank of the passport which is essentially its travel power.\n\nCodett_data &lt;- tt_load(\"2025-09-09\")\ncountry &lt;- tt_data$country_lists\nyear &lt;- tt_data$rank_by_year\n\n\nResearch Question: How has the power of different regions passports shifted over time? What is the most powerful passport?",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>TidyTuesday 09/09/2025</span>"
    ]
  },
  {
    "objectID": "src/tt/hw02-tt.html#exploring-the-data",
    "href": "src/tt/hw02-tt.html#exploring-the-data",
    "title": "TidyTuesday 09/09/2025",
    "section": "Exploring the Data",
    "text": "Exploring the Data\n\nCode#Joining the \ncountry_expanded &lt;- \n  full_join(country, year)\n\n\n\nCodecountry_expanded |&gt; \n  mutate(visa_free_count = as.numeric(visa_free_count)) |&gt; \n  group_by(region) |&gt; \n  summarise(Mean_Visa = mean(visa_free_count), Mean_rank = mean(rank))\n\n# A tibble: 7 × 3\n  region      Mean_Visa Mean_rank\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 AFRICA           49.7      77.5\n2 AMERICAS        109.       36.0\n3 ASIA             68.4      65.6\n4 CARIBBEAN        95.7      45.8\n5 EUROPE          131.       22.4\n6 MIDDLE EAST      57.0      74.0\n7 OCEANIA          92.6      45.2\n\n\nIn this table we can see that that Europe has the best overall mean rank for its countries and that passports from Europe tend to be let into a lot more countries. This is then followed by passports from the Americas, then the Caribbean. In last place is Africa.",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>TidyTuesday 09/09/2025</span>"
    ]
  },
  {
    "objectID": "src/tt/hw02-tt.html#visualizations",
    "href": "src/tt/hw02-tt.html#visualizations",
    "title": "TidyTuesday 09/09/2025",
    "section": "Visualizations",
    "text": "Visualizations\nIn order to get a better idea on how these trends have changed overtime I created the following visualizations.\n\nCoderegion &lt;- country_expanded |&gt; \n  mutate(visa_free_count = as.numeric(visa_free_count)) |&gt; \n  group_by(region, year) |&gt; \n  summarise(Mean_Visa = mean(visa_free_count), Mean_rank = mean(rank))\n\nregion |&gt; \n  ggplot() + \n  geom_line(aes(x = year, y = Mean_Visa, color = region), stat = \"smooth\", size = 2) + \n  theme_classic(base_size = 13) +  \n  scale_y_continuous(limits = c(0,200), breaks = seq(0, 200, by = 50)) + \n  scale_x_continuous(limits = c(2006,2025), expand = c(0,0)) + \n  theme(\n      panel.grid.minor = element_blank(),\n      axis.ticks.x = element_blank(),\n      plot.title = element_text(face = \"bold\", size = 15),\n      plot.title.position = \"plot\", \n  ) + \n   scale_color_viridis_d() + \n  labs(title = \"Average number of visa free entries over time\", subtitle = \"per region of the world\", x = \"Year\", y = \"Mean Visa Free Count\", color = \"Region\", caption = \"Made by: Tori Duckworth, Date: Sep.15,2025, Source: TidyTuesday 09-09-25\") + \n  geom_vline(aes(xintercept = 2013), linetype = \"dotted\", color = \"gray50\") + \n  geom_text(aes(x = 2014, y = 200, label = \"2013\"), color = \"gray50\", size = 4) + \n  labs(alt = \"alt.text = line graph showing the average number of countries a passport can enter without a visa per region over time. We can clearly see that Europe has the most passports that can visit the most counties without visas, followed by the Americas, and despite rising overtime they have not changed position. Following behind them is the caribbean followed by oceania, which are now about the same level in recent years. Then passports from Asia, the Middle east and Africa follow. There is a line marking 2013 as that is when there seems to be a global shift in the trend of allowing more visa free travel, going from a steady about 45 degree angle incline to plataueing for a year, followed by a much slower climb\")\n\n\n\n\n\n\n\nIn this visualization we can clearly see that the mean number of countries each region’s passports allow people into without visas has increase overtime. In fact they were all following a similar slope until 2013 where there was a significant bump and then most regions plateaued before steadily rising a lot slower. It is unclear what global event could have caused this stalling in the rise of freedom to travel, but the pace has not quite recovered.\n\nCodeBest_passport &lt;- country_expanded |&gt; \n  filter(region == \"EUROPE\") |&gt; \n  mutate(rank = as.numeric(rank)) |&gt; \n  group_by(country) |&gt; \n  filter(any(rank == 1)) |&gt; \n  ungroup()\n  \nBest_passport |&gt;   \n  ggplot() +\n  geom_line(aes(x = year, y = rank, color = country), stat = \"smooth\", size = 2) + \n  theme_classic(base_size = 13) +  \n  scale_y_continuous(limits = c(0,7), breaks = seq(0, 7, by = 2)) + \n  scale_x_continuous(limits = c(2006,2025), expand = c(0,0)) + \n  theme(\n      panel.grid.minor = element_blank(),\n      axis.ticks.x = element_blank(),\n      plot.title = element_text(face = \"bold\", size = 15),\n      plot.title.position = \"plot\", \n  ) + \n   scale_color_viridis_d() + \n  labs(title = \"Best Passport Overtime\", subtitle = \"from Europe\", x = \"Year\", y = \"Rank\", color = \"Country\", caption = \"Made by: Tori Duckworth, Date: Sep.15,2025, Source: TidyTuesday 09-09-25\") + \n  scale_y_reverse() + \n  labs(alt = \"This graph shows 8 European countries who have all been ranked number one at some point from 2006-2025.  Denmark was ranked the best for about 8 years, until around 2013 when both Sweden and the UK were around first. Then Germany became the top rated passport and remained that way until 2025 when Spain over took it. There is a lot of fluctiation between countries and over time.\")\n\n\n\n\n\n\n\nBy looking at this graph, it seems that Denmark was ranked the best for about 8 years, until around 2013 when both Sweden and the UK were around first. Then Germany became the top rated passport and remained that way until 2025 when Spain over took it. However, while this graph shows the changes it doesn’t give us a definitive answer, which I do in the following short table.\n\nCodeBest_passport |&gt; \n  group_by(country) |&gt; \n  summarise(Mean_rank = mean(rank)) |&gt; \n  arrange(Mean_rank) \n\n# A tibble: 8 × 2\n  country        Mean_rank\n  &lt;chr&gt;              &lt;dbl&gt;\n1 Germany             2.2 \n2 Finland             2.4 \n3 Denmark             2.65\n4 Sweden              2.9 \n5 Italy               3.3 \n6 Spain               3.65\n7 France              3.95\n8 United Kingdom      4.15\n\n\nThrough this we can in fact see that Germany has been most consistently the top rated passport. However Finland and Denmark are not far behind. Despite one time taking the first place spot, the UK is last of the 8 European countries in terms of consistent rank.\nAll in all these visualizations tell us that the rate of the number of countries each passport can get a holder in somewhere has slowed down compared to the early 2000s, although still is increasing. Individiuals from Europes passports on average get them into many more countries without a visa, around 131 to be exact, while if your are a German passport holder you have the ultimate travel power.",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>TidyTuesday 09/09/2025</span>"
    ]
  },
  {
    "objectID": "src/tt/hw03-tt.html",
    "href": "src/tt/hw03-tt.html",
    "title": "TidyTuesday 09/16/2025",
    "section": "",
    "text": "TidyTuesday Section (optional)\nExplore the week’s TidyTuesday challenge. Develop a research question, then answer it through a short data story with effective visualization(s). Provide sufficient background for readers to grasp your narrative.\nCode#Necessary Libraries\n#| include: false\n#| echo: false\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(hrbrthemes)\nlibrary(viridis)\nlibrary(tidytuesdayR)\nlibrary(stringr)\nlibrary(patchwork)\n##Importing the Data\nReading in the Tidytuesday data for last week, which takes a look at recipes gathered Allrecipes.com. The first data set – all_recipes – has a collection of regular recipes and the second data set – cuisines – contains recipes organized by their country of origin, and has fewer data points than the all_recipies data set.\nCodett_data &lt;- tt_load(\"2025-09-16\")\nall_recipes &lt;- tt_data$all_recipes\ncuisines &lt;- tt_data$cuisines\nResearch Questions: What recipe takes the longest and what is the fastest? Does a certain cuisine take longer than others, how much of it is prep vs cook time?",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>TidyTuesday 09/16/2025</span>"
    ]
  },
  {
    "objectID": "src/tt/hw03-tt.html#tidytuesday-section-optional",
    "href": "src/tt/hw03-tt.html#tidytuesday-section-optional",
    "title": "TidyTuesday 09/16/2025",
    "section": "",
    "text": "Instructions\n\n\n\nYou can count work on this week’s TidyTuesday toward the exceptional work required for an A in the Homework component.",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>TidyTuesday 09/16/2025</span>"
    ]
  },
  {
    "objectID": "src/tt/hw03-tt.html#exploring-the-data",
    "href": "src/tt/hw03-tt.html#exploring-the-data",
    "title": "TidyTuesday 09/16/2025",
    "section": "Exploring the Data",
    "text": "Exploring the Data\n\nCode#Joining the Data \nrecipes_expanded &lt;- full_join(all_recipes, cuisines)\n\n\n#Finding recipe with longest total time\n\nCoderecipes_expanded |&gt; \n  slice_max(total_time, with_ties = TRUE)\n\n# A tibble: 1 × 17\n  name      url   author date_published ingredients calories   fat carbs protein\n  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;  &lt;date&gt;         &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Homemade… http… MSGYP… 2023-06-27     4 cups sug…       NA    NA    NA      NA\n# ℹ 8 more variables: avg_rating &lt;dbl&gt;, total_ratings &lt;dbl&gt;, reviews &lt;dbl&gt;,\n#   prep_time &lt;dbl&gt;, cook_time &lt;dbl&gt;, total_time &lt;dbl&gt;, servings &lt;dbl&gt;,\n#   country &lt;chr&gt;\n\n\nThe recipe that takes the longest is homemade wine, which funnily enough has only 5 min prep time and 0 cook time but needs to ferment for about 42 days. It does make 32 servings though, probably for a wine tasting party.\n#Finding recipe with shortest total time\n\nCoderecipes_expanded |&gt; \n  filter(total_time == 1) #over 300 had cooking times of zero, which was most likely put in place because no time was given, which is why I started with == to 1.\n\n# A tibble: 4 × 17\n  name      url   author date_published ingredients calories   fat carbs protein\n  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;  &lt;date&gt;         &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Pumpkin … http… Penny  2022-09-16     ¼ cup grou…       22     1     5       0\n2 Teriyaki… http… Judy   2022-08-08     ⅔ cup soy …       43    NA     8       2\n3 3-Ingred… http… IBNSH… 2025-06-05     1 tablespo…       17     1     1       1\n4 Sauteed … http… theda… 2025-02-03     2 tablespo…       80     6     4       2\n# ℹ 8 more variables: avg_rating &lt;dbl&gt;, total_ratings &lt;dbl&gt;, reviews &lt;dbl&gt;,\n#   prep_time &lt;dbl&gt;, cook_time &lt;dbl&gt;, total_time &lt;dbl&gt;, servings &lt;dbl&gt;,\n#   country &lt;chr&gt;\n\n\nThe 4 recipies that take the “shortest” amount of time are all sauces/spice mixes. Pumpkin pie spice and 3 ingredient Potsticker dip, do sound like they would take minimal prep time which is why their authors have most likely just marked the time as 1 minute. However teriyaki sauce and sauteed oyster mushrooms in garlic butter sound like they might take more time in reality. Since these are user entered – volunteer data – there doens’t have to be accuracy for reporting, which is most likely why these recipes show up here. It is important to then understand that user error might be a serious bias skewing or limited analysis.\n#Cusine with longest average time\n\nCodecuisines |&gt; \n  group_by(country) |&gt; \n  summarise(Mean_time = mean(total_time)) |&gt; \n  arrange(desc(Mean_time))\n\n# A tibble: 49 × 2\n   country             Mean_time\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 Amish and Mennonite      849.\n 2 Turkish                  353.\n 3 Portuguese               353.\n 4 Canadian                 271.\n 5 German                   270.\n 6 Korean                   263.\n 7 Belgian                  245 \n 8 Russian                  239.\n 9 Jamaican                 225.\n10 Jewish                   218.\n# ℹ 39 more rows\n\n\nIt appears Amish and Mennonite food takes the longest to make, most likely because they do things more traditionally and preserve a lot of food which takes time. In comparison on average Colombian food takes the least average amount of time. A lot of the mean times however are quite long because they are most likely being skewed by a few large data points or recipes that take a long time.",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>TidyTuesday 09/16/2025</span>"
    ]
  },
  {
    "objectID": "src/tt/hw03-tt.html#visualizations",
    "href": "src/tt/hw03-tt.html#visualizations",
    "title": "TidyTuesday 09/16/2025",
    "section": "Visualizations",
    "text": "Visualizations\n#Prepping the Data for the Visualization\n\nCodeMean_times_cuisines &lt;-  cuisines |&gt; \n  mutate(other_time = (total_time - (prep_time + cook_time))) |&gt; \n  group_by(country) |&gt; \n  summarise(mean_prep_time = mean(prep_time), mean_cook_time = mean(cook_time), mean_other_time = mean(other_time)) |&gt; \n  pivot_longer(\n    cols = c(mean_prep_time, mean_cook_time, mean_other_time),\n    names_to = \"work_type\", \n    values_to = \"time_dedicated\"\n  ) |&gt; \n  group_by(country) |&gt; \n  mutate(percent = time_dedicated/sum(time_dedicated)) |&gt; \n  ungroup(country)\n\n\n\nCodeOverall_times_recipies &lt;-  recipes_expanded |&gt; \n  mutate(other_time = (total_time - (prep_time + cook_time))) |&gt; \n  summarise(mean_prep_time = mean(prep_time), mean_cook_time = mean(cook_time), mean_other_time = mean(other_time)) |&gt; \n  pivot_longer(\n    cols = c(mean_prep_time, mean_cook_time, mean_other_time),\n    names_to = \"work_type\", \n    values_to = \"time_dedicated\"\n  ) |&gt; \n  mutate(percent = time_dedicated/sum(time_dedicated))\n\n\n#Vizualization\n\nCodeOverall_times_recipies |&gt; \n  ggplot(aes(x = \"\", y = time_dedicated, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") + \n  coord_polar(\"y\", start = 0) + \n  geom_text(aes( x = 1.7, label = paste0(round(percent*100), \"%\")),\n                color = \"black\", position = position_stack(vjust = 0.5)) + \n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\", title = \"Average percent of cooking time dedicated to each task\")  \n\n\n\n\n\n\n\nThis graphs shows for all recipes the amount of time dedicated to each type of task for cooking. As we can see here the majority of time is in fact in the “other” category, which is often a longer period of time of letting the dish sit to either cool or rise or ferment, etc. Prep takes the least time out of total cooking time as it is often just cutting something up, which is an easy task. Cooking often involves heat and takes a bit longer. It is possible this pie chart is skewed by data points mentioned above, like the homemade wine where it has to ferment for 42 days.\n#Spliting into groups of 4 to vizualize\n\nCodeMean_times_cuisines_A &lt;- Mean_times_cuisines |&gt; \n  filter(str_detect(country, \"^A\"))\n\n\n\nCodePies_A &lt;- Mean_times_cuisines_A |&gt; \n  ggplot(aes(x = \"\", y = percent, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0) + \n  facet_grid(.~country) +\n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\") + \n  theme(\n    axis.line = element_blank(),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      axis.ticks.length = unit(0, \"pt\")\n    )\n\n\n\nCodeMean_times_cuisines_B &lt;- Mean_times_cuisines |&gt; \n  filter(str_detect(country, \"^B|^Caj\"))\n\n\n\nCodePies_B &lt;- Mean_times_cuisines_B |&gt; \n  ggplot(aes(x = \"\", y = percent, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0) + \n  facet_grid(.~country) +\n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\") + \n  theme(\n    axis.line = element_blank(),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      axis.ticks.length = unit(0, \"pt\")\n    )\n\n\n\nCodeMean_times_cuisines_C &lt;- Mean_times_cuisines |&gt; \n  filter(str_detect(country, \"^Can|^Ch|Co\"))\n\n\n\nCodePies_C &lt;- Mean_times_cuisines_C |&gt; \n  ggplot(aes(x = \"\", y = percent, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0) + \n  facet_grid(.~country) +\n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\") + \n  theme(\n    axis.line = element_blank(),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      axis.ticks.length = unit(0, \"pt\")\n    )\n\n\n\nCodeMean_times_cuisines_D &lt;- Mean_times_cuisines |&gt; \n  filter(str_detect(country, \"^Cu|^D|^Fil\"))\n\n\n\nCodePies_D &lt;- Mean_times_cuisines_D |&gt; \n  ggplot(aes(x = \"\", y = percent, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0) + \n  facet_grid(.~country) +\n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\") + \n  theme(\n    axis.line = element_blank(),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      axis.ticks.length = unit(0, \"pt\")\n    )\n\n\n\nCodeMean_times_cuisines_E &lt;- Mean_times_cuisines |&gt; \n  filter(str_detect(country, \"^Fin|^Fr|^G\"))\n\n\n\nCodePies_E &lt;- Mean_times_cuisines_E |&gt; \n  ggplot(aes(x = \"\", y = percent, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0) + \n  facet_grid(.~country) +\n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\") + \n  theme(\n    axis.line = element_blank(),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      axis.ticks.length = unit(0, \"pt\")\n    )\n\n\n\nCodeMean_times_cuisines_F &lt;- Mean_times_cuisines |&gt; \n  filter(str_detect(country, \"^I\"))\n\n\n\nCodePies_F &lt;- Mean_times_cuisines_F |&gt; \n  ggplot(aes(x = \"\", y = percent, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0) + \n  facet_grid(.~country) +\n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\") + \n  theme(\n    axis.line = element_blank(),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      axis.ticks.length = unit(0, \"pt\")\n    )\n\n\n\nCodeMean_times_cuisines_G &lt;- Mean_times_cuisines |&gt; \n  filter(str_detect(country, \"^J|^K\"))\n\n\n\nCodePies_G &lt;- Mean_times_cuisines_G |&gt; \n  ggplot(aes(x = \"\", y = percent, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0) + \n  facet_grid(.~country) +\n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\") + \n  theme(\n    axis.line = element_blank(),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      axis.ticks.length = unit(0, \"pt\")\n    )\n\n\n\nCodeMean_times_cuisines_H &lt;- Mean_times_cuisines |&gt; \n  filter(str_detect(country, \"^L|^M|^N|^Pa\"))\n\n\n\nCodePies_H &lt;- Mean_times_cuisines_H |&gt; \n  ggplot(aes(x = \"\", y = percent, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0) + \n  facet_grid(.~country) +\n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\") + \n  theme(\n    axis.line = element_blank(),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      axis.ticks.length = unit(0, \"pt\")\n    )\n\n\n\nCodeMean_times_cuisines_I &lt;- Mean_times_cuisines |&gt; \n  filter(str_detect(country, \"^Pe|^Po\"))\n\n\n\nCodePies_I &lt;- Mean_times_cuisines_I |&gt; \n  ggplot(aes(x = \"\", y = percent, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0) + \n  facet_grid(.~country) +\n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\") + \n  theme(\n    axis.line = element_blank(),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      axis.ticks.length = unit(0, \"pt\")\n    )\n\n\n\nCodeMean_times_cuisines_J &lt;- Mean_times_cuisines |&gt; \n  filter(str_detect(country, \"^Pu|^R|^Sc|^Soul\"))\n\n\n\nCodePies_J &lt;- Mean_times_cuisines_J |&gt; \n  ggplot(aes(x = \"\", y = percent, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0) + \n  facet_grid(.~country) +\n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\") + \n  theme(\n    axis.line = element_blank(),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      axis.ticks.length = unit(0, \"pt\")\n    )\n\n\n\nCodeMean_times_cuisines_K &lt;- Mean_times_cuisines |&gt; \n  filter(str_detect(country, \"^Sout|^Sp|^Swe\"))\n\n\n\nCodePies_K &lt;- Mean_times_cuisines_K |&gt; \n  ggplot(aes(x = \"\", y = percent, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0) + \n  facet_grid(.~country) +\n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\") + \n  theme(\n    axis.line = element_blank(),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      axis.ticks.length = unit(0, \"pt\")\n    )\n\n\n\nCodeMean_times_cuisines_L &lt;- Mean_times_cuisines |&gt; \n  filter(str_detect(country, \"^Swi|^T\"))\n\n\n\nCodePies_L &lt;- Mean_times_cuisines_L |&gt; \n  ggplot(aes(x = \"\", y = percent, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0) + \n  facet_grid(.~country) +\n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\") + \n  theme(\n    axis.line = element_blank(),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      axis.ticks.length = unit(0, \"pt\")\n    )\n\n\n\nCodeMean_times_cuisines_M &lt;- Mean_times_cuisines |&gt; \n  filter(str_detect(country, \"^V\"))\n\n\n\nCodePies_M &lt;- Mean_times_cuisines_M |&gt; \n  ggplot(aes(x = \"\", y = percent, group = work_type, colour = work_type, fill = work_type)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0) + \n  facet_grid(.~country) +\n  theme_void() + \n  scale_color_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  scale_fill_viridis_d(labels = c(\"mean_cook_time\" = \"Cooking\", \"mean_other_time\" = \"Other\", \"mean_prep_time\" = \"Prep\")) + \n  labs(fill = \"Type of work\", colour = \"Type of work\") + \n  theme(\n    axis.line = element_blank(),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      axis.ticks.length = unit(0, \"pt\")\n    )\n\n\n#Combining the plots\n\nCode(Pies_A / Pies_B / Pies_C / Pies_D / Pies_E / Pies_F / Pies_G / Pies_H / Pies_I / Pies_J / Pies_K / Pies_L / Pies_M) + plot_layout(guides = \"collect\",heights = c()) & theme(legend.position = \"bottom\", plot.margin = unit(c(0, 0, 0, 0), \"cm\"))\n\n\n\n\n\n\n\nUnfortunately I couldn’t figure out how to make the spacing between each set of 4 smaller however this graph shows the rough amount of time each type of work takes per cuisine. There are some interesting trends, Amish and Mennonite have the most amount of time dedicated to other tasks, most likely because they do not use modern technology and have to rely on older method to store food, so fermentation and other longer term methods are impolyed more to preserve food. Southern recipes, Tex-Mex, Soul food spend about half of cooking time actually cooking the food on average. In america at least there could be some overlap in these cuisine categories. However Swiss cuisine also spends a lot of time cooking the food as well, and there is less overlap there. Chilean and Cajun and Creole spend the most time cooking out of all the cuisines, while Norwegians spend the most time preping the food. It would be interesting in the future to dig deep and pull out what are the most common ingredients in each cuisine and see if that relates at all to the different work categories.",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>TidyTuesday 09/16/2025</span>"
    ]
  },
  {
    "objectID": "src/ica/Review.html",
    "href": "src/ica/Review.html",
    "title": "Cleaning SFO Weather Data",
    "section": "",
    "text": "Exercise\nCarryout the following steps to clean and save the San Francisco Weather data. Make sure to download and add the data file to your portfolio repository as instructed.\nCodelibrary(tidyverse)\nlibrary(readr) #To get read_csv in the CSV\n\n#Step 1\nweather_data &lt;- read_csv(\"../../data/raw/weather.csv\") #Reading in the Data\nCode#Step 2\n#Cleaning out large value in PrecipYr\nweather_clean &lt;- weather_data |&gt; \n  mutate(PrecipYr &lt;- na_if(PrecipYr, 99999))\nCode#Step 3\n#Creating variable that shows the day of the year\nweather_clean &lt;- weather_data |&gt; \n  arrange(Month, Day) |&gt; \n  mutate(dateInYear &lt;- seq(from = 1, to = 365, by = 1))\nCode#Step 4\n#Creating variable that shows 3 letter month abbreviation\nweather_clean &lt;- weather_data |&gt; \n  mutate(month_name &lt;- month.abb[Month])\nCode#Step 5\n#Saving the data to another source \nwrite_csv(weather_clean, file = \"../../data/processed/weather_clean.csv\")",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cleaning SFO Weather Data</span>"
    ]
  },
  {
    "objectID": "src/ica/Review.html#exercise",
    "href": "src/ica/Review.html#exercise",
    "title": "Cleaning SFO Weather Data",
    "section": "",
    "text": "Read in the weather data in this file with the correct relative file path after you move it to the instructed location.\n\n\n\nThere is a variable that has values that don’t make sense in the data context. Figure out which variable this is and clean it up by making those values missing using na_if().\n\n\n\nCreate a variable called dateInYear that indicates the day of the year (1-365) for each case. (Jan 1 should be 1, and Dec 31 should be 365).\n\n\n\nCreate a variable called month_name that shows the 3-letter abbreviation for each case.\n\n\n\nSave the wrangled data to the data/processed/ folder using write_csv(). Name this file weather_clean.csv. Look up the documentation for this function by typing ?write_csv in the Console. You’ll need to write an appropriate relative path.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cleaning SFO Weather Data</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_viz.html",
    "href": "src/ica/Advanced_data_viz.html",
    "title": "3 Advanced Data Viz",
    "section": "",
    "text": "🧩 Learning Goals\nBy the end of this lesson, you should be able to:",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>3 Advanced Data Viz</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_viz.html#learning-goals",
    "href": "src/ica/Advanced_data_viz.html#learning-goals",
    "title": "3 Advanced Data Viz",
    "section": "",
    "text": "Navigate ggplot2 reference page to find needed functions for a desired visualization\nNavigate the different sections of a function help page to construct desired plot features, in particular,\n\nNavigate the Usage section to identify arguments that must be set\nNavigate the Arguments section to understand how arguments work\nNavigate the Aesthetics section to learn how plot appearance can be controlled\nNavigate the Examples section for some usage examples\n\n\nIdentify when to use different data arguments within ggplot() and geom_() layers",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>3 Advanced Data Viz</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_viz.html#introduction",
    "href": "src/ica/Advanced_data_viz.html#introduction",
    "title": "3 Advanced Data Viz",
    "section": "Introduction 1\n",
    "text": "Introduction 1\n\nIn this lesson, we are going to recreate NYTimes 2015 Temperature Visualization (html) using data from San Francisco (SFO) in 2011.\n\n\nScreenshot of NYTimes 2015 Temperature Visualization",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>3 Advanced Data Viz</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_viz.html#reading-data",
    "href": "src/ica/Advanced_data_viz.html#reading-data",
    "title": "3 Advanced Data Viz",
    "section": "Reading Data",
    "text": "Reading Data\nRun the code chunk below to load the tidyverse package and read in the San Francisco weather data.\n\nCodelibrary(tidyverse)\nweather &lt;- read_csv(\"https://mac-stat.github.io/data/sfo_weather.csv\")",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>3 Advanced Data Viz</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_viz.html#understanding-data",
    "href": "src/ica/Advanced_data_viz.html#understanding-data",
    "title": "3 Advanced Data Viz",
    "section": "Understanding Data",
    "text": "Understanding Data\nBelow is the codebook of the data. Familiarize yourself with the meaning of each variable. Use the codebook as a reference when using the data.\n\n\nMonth: Month of the year (1-12)\n\nDay: Day within the month (1-31)\n\nLow/High: Low/high temperature this day\n\nNormalLow/NormalHigh: Typical low/high temperature for this day of the year\n\nRecordLow/RecordHigh: Record low/high temperature for this day of the year\n\nLowYr/HighYr: Year in which the record low/high was observed\n\nPrecip: Amount of precipitation (inches) this day\n\nRecordPrecip: Record amount of precipitation for this day of the year\n\nPrecipYr: Year in which the record precipitation was observed\n\ndate: The actual date in 2011 for this day in YYYY-MM-DD format\n\ndateInYear: What day of the year is it? (1-365)\n\nRecord: Logical (TRUE/FALSE) indicating whether this day had a high temperature record\n\nRecordText: Text that displays the record high for this day (\"Record high: ##\")\n\nRecordP: Logical (TRUE/FALSE) indicating whether this day had a precipitation record\n\nCulmPrec: Cumulative precipitation for the month up to this day",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>3 Advanced Data Viz</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_viz.html#exercise-1",
    "href": "src/ica/Advanced_data_viz.html#exercise-1",
    "title": "3 Advanced Data Viz",
    "section": "Exercise 1",
    "text": "Exercise 1\nExamine the NYTimes 2015 Temperature Visualization (html) then answer the following questions.\nData Storytelling\n\nRelate the intro paragraph: “Scientists declared that 2015 was Earth’s hottest year on record…” to the design of the visualization. In particular, based on the intro paragraph,\n\nWhat key message/claim does NYTimes want readers to be able to explore? NYT wants readers to explore their own cities and see how 2015 compares to the historical average temperature over the course of a year regardless of location.\nHow did this goal inform what information is displayed in the visualization? It means that there is a part that shows the historical average and then one that shows the daily average from 2025 overlayed on one another. Then it also provides the option to switch through cities to compare how they have changed easier.\n\n\n\nAesthetic Mapping\n\nWhat specific variables (from the data codebook) underlie the visualization?\nHow do these variables map to aesthetics of the visual elements, eg, position, size, shape, and color of glyphs?\n\nFor x most likely the dateInYear or date variable, for y it is the Record Low/High overlayed with NormalHigh/NormalLow. Depending on the city there is RecordPrecip. Date in year makes up the x axis and is spkit into months very nicely, which each day making a small rectangular box. The record High and low are a redish pink cover overlayed on the grey to allow for comparison. The high and low make up either end of the box or each day, so longer boxes have more of a range of temperatures. Grey tends to take a more even slope/change as it are historical averages. They move from city to city up or down normally situating around a certain average temperature.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>3 Advanced Data Viz</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_viz.html#exercise-2",
    "href": "src/ica/Advanced_data_viz.html#exercise-2",
    "title": "3 Advanced Data Viz",
    "section": "Exercise 2",
    "text": "Exercise 2\nNavigate the Geoms section of the ggplot2 reference page to find a geom that corresponds to the visual elements in the temperature plot. Using both the small thumbnail visuals on the right and the names of the geom’s, brainstorm some possibilities for geom’s you might use to recreate the temperature visualization.\n\n\n\n\n\n\nNavigating Documentation / Reference Pages\n\n\n\nYou need to navigate the geoms further by opening up their reference pages to understand if a particular geom is suitable for our task. Let’s look at the geom_point documentation page to learn how to read a documentation page..\nThe Usage section shows all of the possible inputs (arguments) to the geom. These are all of the ways that a geom can be customized. Just looking at the argument names can help give a hint as to what arguments might fit our needs.\nThe Arguments section, on the other hand, explains in detail what each argument does and the possible values the argument can take. The mapping, data, and ... arguments will be the most commonly used by far.\n\n\nmapping is the argument that is being used when we specify which variables should link or map to the plot aesthetics (the code inside aes()).\n\ndata is the argument where we specify the dataset containing the variables that the geom is using.\n\n... is used for fixed aesthetics (ones that don’t correspond to a variable), eg, to set the color of all points, we use color = \"red\" and to set the size of all points, we use size = 3.\n\nThe Aesthetics section of a geom documentation page gives information on how the visual elements of the geom correspond to data. For example, the geom_point documentation page shows that x and y aesthetics are available. It also shows some new aesthetics like stroke.\n\n\n\n\n\n\n\n\ndata Argument\n\n\n\nPreviously you have used one dataset per plot by specifying that as the first argument of ggplot(). However, multiple data sets can be passed into ggplot as shown in the example below.\n\nCodedata(diamonds)\n\ndiamonds_avg_price &lt;- diamonds |&gt;\n  group_by(carat) |&gt;\n  summarize(avg_price = mean(price)) |&gt;\n  arrange(carat)\ndiamonds_avg_price &lt;- diamonds_avg_price[seq(1, nrow(diamonds_avg_price), 3), ]\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_point() +\n  geom_point(\n    data = diamonds_avg_price,\n    aes(x = carat, y = avg_price),\n    color = \"deepskyblue\",\n    size = 3\n  )\n\n\n\n\n\n\n\n\n\nLook at the geom_linerange documentation page and start off your temperature visualization with the record lows and highs. Your plot should look like the one below. The hex code of the used light tan color is #ECEBE3.\n\n\nSFO Weather Records in 2011\n\n\nCodeggplot(weather) +\n  geom_linerange(aes(x = dateInYear, ymax = RecordHigh, ymin = RecordLow), color = \"#ECEBE3\" ) + \n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeyboard Shortcuts\n\n\n\nAs you work on this plot, try to use some new keyboard shortcuts. Focus on the following:\n\nInsert code chunk: Ctrl+Alt+I (Windows). Option+Command+I (Mac).\nRun current code chunk: Ctrl+Shift+Enter (Windows). Command+Shift+Return (Mac).\nRun current line/currently selected lines: Ctrl+Enter (Windows). Command+Return (Mac).",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>3 Advanced Data Viz</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_viz.html#exercise-3",
    "href": "src/ica/Advanced_data_viz.html#exercise-3",
    "title": "3 Advanced Data Viz",
    "section": "Exercise 3",
    "text": "Exercise 3\nIn your visualization, also display the usual temperatures (NormalLow and NormalHigh) and actual 2011 temperatures (Low and High). Your plot should look like the one below. The hex code of the color used for the usual temperatures is \"#C8B8BA\" and for the color used for actual temperatures is \"#A90248\".\n\n\nSFO observed, Average, and Record Daily Temperatures in 2011\n\n\nCodeggplot(weather) +\n  geom_linerange(aes(x = dateInYear, ymax = RecordHigh, ymin = RecordLow), color = \"#ECEBE3\" ) + \n  geom_linerange(aes(x = dateInYear, ymax = NormalHigh, ymin = NormalLow), color = \"#C8B8BA\") + \n  geom_linerange(aes(x = dateInYear, ymax = High, ymin= Low), color = \"#A90248\") + \n  theme_classic() \n\n\n\n\n\n\n\n\n\n\n\n\n\nFiner Control\n\n\n\nIf you’d like finer control of the width of these lines/rectangles, check out the geom_rect documentation page.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>3 Advanced Data Viz</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_viz.html#exercise-4",
    "href": "src/ica/Advanced_data_viz.html#exercise-4",
    "title": "3 Advanced Data Viz",
    "section": "Exercise 4",
    "text": "Exercise 4\nRecreate the visual demarcations of the months by adding vertical lines separating the months. Brainstorm how we might draw those vertical lines. What geom might we use? What subset of the data might we use in that geom layer to draw lines only at the month divisions?\n\nCode#Small data set with end of the month for each month, \nMonth_data &lt;- weather |&gt; \n  group_by(Month) |&gt; \n  filter(Day == max(Day)) |&gt; \n  ungroup()\n\nggplot() +\n  geom_linerange(data = weather, aes(x = dateInYear, ymax = RecordHigh, ymin = RecordLow), color = \"#ECEBE3\" ) + \n  geom_linerange(data = weather, aes(x = dateInYear, ymax = NormalHigh, ymin = NormalLow), color = \"#C8B8BA\") + \n  geom_linerange(data = weather, aes(x = dateInYear, ymax = High, ymin= Low), color = \"#A90248\") + \n  #geom_rect(data = weather, aes(x = dateInYear, ymax = High, ymin= Low), fill = \"#A90248\",alpha = 0.3, inherit.aes = FALSE) + \n  theme_classic() + \n  geom_vline(data = Month_data, aes(xintercept = dateInYear), linetype = \"dotted\")",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>3 Advanced Data Viz</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_viz.html#exercise-5",
    "href": "src/ica/Advanced_data_viz.html#exercise-5",
    "title": "3 Advanced Data Viz",
    "section": "Exercise 5",
    "text": "Exercise 5\nChange the x-axis labels so that the month names display in the center of each month’s slice of the plot.\n\n\n\n\n\n\nMonth Names\n\n\n\nR has built-in variables called month.abb and month.name that contain abbreviated and full month names.\n\n\n\nCode#Small data set with end of the month for each month, \nMonth_data &lt;- weather |&gt; \n  group_by(Month) |&gt; \n  filter(Day == max(Day)) |&gt; \n  ungroup() \n\nMid_month &lt;- weather |&gt; \n  group_by(Month) |&gt; \n  summarize(across(where(is.numeric), ~mean(.x, na.rm = TRUE)))\n\nmonth_numeric &lt;- as.numeric(format(month, format = \"%U\"))\nmonth_label &lt;- format(month, format = \"%b\")\n\nggplot(weather) +\n  geom_linerange(aes(x = dateInYear, ymax = RecordHigh, ymin = RecordLow), color = \"#ECEBE3\" ) + \n  geom_linerange(aes(x = dateInYear, ymax = NormalHigh, ymin = NormalLow), color = \"#C8B8BA\") + \n  geom_linerange(aes(x = dateInYear, ymax = High, ymin= Low), color = \"#A90248\") + \n  scale_x_continuous(breaks = Mid_month$dateInYear, labels = month.abb) + \n  #geom_rect(data = weather, aes(x = dateInYear, ymax = High, ymin= Low), fill = \"#A90248\",alpha = 0.3, inherit.aes = FALSE) + \n  theme_classic() + \n  geom_vline(data = Month_data, aes(xintercept = dateInYear), linetype = \"dotted\")\n\n\n\n\n\n\n\nTry to figuring out this new challenge using search engines and large language models:\n\nSearch Engines. Use Google to search for possible solutions using the jargon that is most likely to return the most relevant results. Record search queries and your thought process in selecting which search results to look at first.\nLLMs. Use ChatGPT or Gemini with prompts that will most efficiently get you the desired results. Record the chat prompts used and output given. Evaluate the output. Do you fully understand the code generated? How can you tell that the generated code is correct?",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>3 Advanced Data Viz</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_viz.html#exercise-6",
    "href": "src/ica/Advanced_data_viz.html#exercise-6",
    "title": "3 Advanced Data Viz",
    "section": "Exercise 6",
    "text": "Exercise 6\nCreate a precipitation plot that looks like the following. Note that\n\nThe triangles point to precipitation records–refer to the data codebook above for the RecordP variable.\nThe numbers on the plot indicate the total precipitation for the month–search the hjust and vjust options to adjust the alignment of the numbers.\nThe blue and tan colors hex codes are \"#32a3d8\" and \"#ebeae2\", respectively.\n\n\n\nSFO Precipitation in 2011\n\n\nCodeMonth_average &lt;- weather |&gt; \n  group_by(Month) |&gt; \n  summarize(CulmPrec = max(CulmPrec), dateInYear = max(dateInYear))\n\nRecord &lt;- weather |&gt; \n  filter(RecordP == TRUE) \n  \n\nggplot(weather, aes(x = dateInYear, y = CulmPrec)) +\n  geom_area(fill = \"#ebeae2\", alpha = 0.5) +\n  geom_line(color = \"#32a3d8\", size = 0.5) +  \n  geom_text(data = Month_average, aes(label = round(CulmPrec, 2)), \n            vjust = -0.5, size = 3) +\n  geom_point(data = Record, aes(x = dateInYear, y = CulmPrec), \n             shape = 25, fill = \"black\", size = 2) + \n  theme_minimal()",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>3 Advanced Data Viz</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_viz.html#done",
    "href": "src/ica/Advanced_data_viz.html#done",
    "title": "3 Advanced Data Viz",
    "section": "Done!",
    "text": "Done!\n\nCheck the ICA Instructions for how to (a) push your code to GitHub and (b) update your portfolio website",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>3 Advanced Data Viz</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_viz.html#footnotes",
    "href": "src/ica/Advanced_data_viz.html#footnotes",
    "title": "3 Advanced Data Viz",
    "section": "",
    "text": "The exercise in this lesson are inspired by an assignment from the Concepts in Computing with Data course at UC Berkeley taught by Dr. Deborah Nolan.↩︎",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>3 Advanced Data Viz</span>"
    ]
  },
  {
    "objectID": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html",
    "href": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html",
    "title": "4 Advanced Spatial Viz P1",
    "section": "",
    "text": "🧩 Learning Goals\nBy the end of this lesson, you should be able to:",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 Advanced Spatial Viz P1</span>"
    ]
  },
  {
    "objectID": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#learning-goals",
    "href": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#learning-goals",
    "title": "4 Advanced Spatial Viz P1",
    "section": "",
    "text": "Understand the basics of a CRS (coordinate reference system)\nUnderstand and recognize different spatial file types and data types in R\nImplement some of the basic plotting with the sf package\nUnderstand foundational ideas in working with spatial data (aggregating spatial point data to a spatial region, joining spatial data sets)",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 Advanced Spatial Viz P1</span>"
    ]
  },
  {
    "objectID": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#additional-resources",
    "href": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#additional-resources",
    "title": "4 Advanced Spatial Viz P1",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nSpatial Data Science with Applications in R book: web\n\nSpatial Data Science with R and terra Resources: web\n\nLeaflet in R Package: web\n\nCRAN task view on spatial analysis: web",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 Advanced Spatial Viz P1</span>"
    ]
  },
  {
    "objectID": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#setup",
    "href": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#setup",
    "title": "4 Advanced Spatial Viz P1",
    "section": "Setup",
    "text": "Setup\nFor this activity, create the following directory structure in your portfolio repository under src/ica folder:\nportfolio\n└─ src\n   └─ ica\n      └─ 04_adv_maps\n         ├─ code\n         │  └─ 04-adv-maps-1-notes.qmd\n         ├─ data\n         │  └─ ...  ← saving data here during this activity\n         └─ figures\n            └─ ...  ← saving created maps here during this activity\nFirst load required packages.\n\nCode#Install these packages first\n\n# install.packages(c(\"sf\",\"elevatr\",\"terra\",\"stars\",\"tidycensus\"))\n# install.packages('devtools')\n# devtools::install_github(\"ropensci/USAboundaries\")\n# install.packages(\"USAboundariesData\", repos = \"https://ropensci.r-universe.dev\", type = \"source\")\n\n\nlibrary(tidyverse)\nlibrary(sf) # tools for working with spatial vector data (GIS functionality, mapping)\nlibrary(elevatr) # access to raster elevation maps\nlibrary(terra)\nlibrary(stars)\nlibrary(tidycensus) # spatial data for the US with census information\nlibrary(USAboundaries) # access to boundaries for US states, counties, zip codes, and congressional districts",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 Advanced Spatial Viz P1</span>"
    ]
  },
  {
    "objectID": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#spatial-data-in-r",
    "href": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#spatial-data-in-r",
    "title": "4 Advanced Spatial Viz P1",
    "section": "Spatial Data in R",
    "text": "Spatial Data in R\nSee Spatial Data Appendix for basics of CRS and spatial data types.\nDownload Shapefiles\n\nNavigate to the following URLs to download the spatial data files we’ll be using in this activity. Put these files in the data folder of your 04_adv_maps folder.\n\n\nMN cities: https://gisdata.mn.gov/dataset/loc-pop-centers\n\nFile type: shapefile (.shp)\nFile name: shp_loc_pop_centers.zip (Unzip this after downloading.)\n\n\nMN water: https://gisdata.mn.gov/dataset/us-mn-state-metc-water-lakes-rivers\n\nFile type: shapefile (.shp)\nFile name: shp_water_lakes_rivers.zip (Unzip this after downloading.)\n\n\nRead in Files\n\nRead in the MN cities and MN water shapefiles by entering the correct relative paths in st_read(). Tab completion will be very helpful here: type part of a directory or file name and hit tab to autocomplete or bring up a dropdown of options.\n\n\n\nCode# The sf package comes with a North Carolina shapefile:\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nReading layer `nc' from data source \n  `/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\nCode# Read in shapefiles just downloaded\nmn_cities &lt;- st_read(\"../data/shp_loc_pop_centers/city_and_township_population_centers.shp\")\n\nReading layer `city_and_township_population_centers' from data source \n  `/Users/toriduckworth/Documents/course_work/stat212/class_activities/portfolio-duckwor37/src/ica/04_adv_maps/data/shp_loc_pop_centers/city_and_township_population_centers.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1081 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 190832.6 ymin: 4816672 xmax: 747463.4 ymax: 5468045\nProjected CRS: NAD83 / UTM zone 15N\n\nCodemn_water &lt;- st_read(\"../data/shp_water_lakes_rivers/LakesAndRivers.shp\")\n\nReading layer `LakesAndRivers' from data source \n  `/Users/toriduckworth/Documents/course_work/stat212/class_activities/portfolio-duckwor37/src/ica/04_adv_maps/data/shp_water_lakes_rivers/LakesAndRivers.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2313 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 419538.6 ymin: 4922700 xmax: 522665 ymax: 5029945\nProjected CRS: NAD83 / UTM zone 15N\n\n\nThe sf package reads in spatial data in data.frame-like format. Using the class() function we can check the class (type) of object that we just read in. Note the presence of the “sf” and “data.frame” classes:\n\nCodeclass(nc)\n\n[1] \"sf\"         \"data.frame\"\n\nCodeclass(mn_cities)\n\n[1] \"sf\"         \"data.frame\"\n\nCodeclass(mn_water)\n\n[1] \"sf\"         \"data.frame\"\n\n\nWhen we read in spatial objects, it is useful to check what CRS underlies the data. We can do that with st_crs() from the sf package:\n\nCodest_crs(nc)\n\nCoordinate Reference System:\n  User input: NAD27 \n  wkt:\nGEOGCRS[\"NAD27\",\n    DATUM[\"North American Datum 1927\",\n        ELLIPSOID[\"Clarke 1866\",6378206.4,294.978698213898,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4267]]\n\n\nWe can treat sf objects similarly to ordinary datasets when using ggplot2 to make spatial visualizations:\n\nCodeggplot(nc) +\n    geom_sf() +\n    theme_classic() +\n    labs(title = \"NAD27\")\n\n\n\n\n\n\n\nChange CRS\n\nLet’s explore how changing the CRS changes the map. The st_transform() function in sf re-expresses a spatial object using a user-supplied CRS. The crs argument takes a string descriptor of the CRS. We can find these descriptors via https://epsg.io. In the example below, I searched for “South Carolina”.\n\n\nCodenc_transformed &lt;- nc |&gt; st_transform(crs = \"EPSG:32133\")\nst_crs(nc_transformed)\n\nCoordinate Reference System:\n  User input: EPSG:32133 \n  wkt:\nPROJCRS[\"NAD83 / South Carolina\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"SPCS83 South Carolina zone (meter)\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",31.8333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-81,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",34.8333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",32.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",609600,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United States (USA) - South Carolina - counties of Abbeville; Aiken; Allendale; Anderson; Bamberg; Barnwell; Beaufort; Berkeley; Calhoun; Charleston; Cherokee; Chester; Chesterfield; Clarendon; Colleton; Darlington; Dillon; Dorchester; Edgefield; Fairfield; Florence; Georgetown; Greenville; Greenwood; Hampton; Horry; Jasper; Kershaw; Lancaster; Laurens; Lee; Lexington; Marion; Marlboro; McCormick; Newberry; Oconee; Orangeburg; Pickens; Richland; Saluda; Spartanburg; Sumter; Union; Williamsburg; York.\"],\n        BBOX[32.05,-83.36,35.21,-78.52]],\n    ID[\"EPSG\",32133]]\n\nCodeggplot(nc_transformed) +\n    geom_sf() +\n    theme_classic()\n\n\n\n\n\n\n\nThe goal is to use https://epsg.io to find two CRSs that result in a North Carolina map that is noticeably different from the original in the NAD27 CRS.\nTake a look at the function below that re-maps a spatial object using a new CRS.\n\nRead through the function to get a sense for how this code works.\n\nspatial_obj and new_crs are called arguments (function inputs).\n\nAdd one more argument called title to this function. Use this input to set the plot title.\n\n\nUse your function to make two new maps using your chosen CRSs.\n\n\nCodetransform_and_plot &lt;- function(spatial_obj, new_crs, title) {\n    spatial_obj |&gt; \n        st_transform(crs = new_crs) |&gt; \n        ggplot() +\n            geom_sf() +\n            theme_classic() + \n    labs(title = title)\n}\n\n# Example usage of this function (using a South Carolina CRS)\ntransform_and_plot(nc, new_crs = \"EPSG:32133\", title = \"South Carolina CRS\")\n\n\n\n\n\n\n\n\nCode#First map with different CRS \ntransform_and_plot(nc, new_crs = \"EPSG:3414\", title = \"Singapore CRS\")\n\n\n\n\n\n\n\n\nCode#Second map with different CRS \ntransform_and_plot(nc, new_crs = \"EPSG:3576\", title = \"Northen Hemisphere CRS (North of 45)\")\n\n\n\n\n\n\n\nVerify your understanding: If you had point location data that was not in the NAD27 CRS, what would you expect about the accuracy of how they would be overlaid on the original North Carolina map?\nIf you you had a point location data that was not in the NAD27 CRS, the accuracy of how it would be overlaid would be low. Since it is a map that is designed to fit another region of the world it will warp this section.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 Advanced Spatial Viz P1</span>"
    ]
  },
  {
    "objectID": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#mn-map-with-multiple-layers",
    "href": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#mn-map-with-multiple-layers",
    "title": "4 Advanced Spatial Viz P1",
    "section": "MN Map with Multiple Layers",
    "text": "MN Map with Multiple Layers\nGoal: create a map of MN with different layers of information (city point locations, county polygon boundaries, rivers as lines and polygons, and a raster elevation map).\nGet County Boundaries\n\nWe’ve already read in city location and water information from external shapefiles. We can access county boundaries with the us_counties() function in the USAboundaries package.\n\n\nCode# Load country boundaries data as sf object\nmn_counties &lt;- USAboundaries::us_counties(resolution = \"high\", states = \"Minnesota\")\n\n# Take care of duplicate column names (there are two identical \"state_name\" columns)\nnames_counties &lt;- names(mn_counties)\nnames(mn_counties)[names_counties == \"state_name\"] &lt;- c(\"state_name1\", \"state_name2\")\n\n\nUnifying CRSs Across Different Spatial Datasets\n\nWe first need to ensure that the CRS is the same for all spatial datasets.\n\n\nCheck the CRS for the mn_cities, mn_water, and mn_counties datasets.\nIf the datasets don’t all have the same CRS, use st_transform() to update the datasets to have the same CRS as mn_cities. You can use crs = st_crs(mn_cities) within st_transform().\n\n\nCode#checking CRS for datasets \nst_crs(mn_cities)\n\nCoordinate Reference System:\n  User input: NAD83 / UTM zone 15N \n  wkt:\nPROJCRS[\"NAD83 / UTM zone 15N\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"UTM zone 15N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-93,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    ID[\"EPSG\",26915]]\n\nCodest_crs(mn_water)\n\nCoordinate Reference System:\n  User input: NAD83 / UTM zone 15N \n  wkt:\nPROJCRS[\"NAD83 / UTM zone 15N\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"UTM zone 15N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-93,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    ID[\"EPSG\",26915]]\n\nCodest_crs(mn_counties)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\nCode#transforming mn_counties to match\nmn_counties &lt;- mn_counties |&gt; \n  st_transform(crs = st_crs(mn_cities))\n\n\nCounties + Cities\n\nCreate a map where city locations are overlaid on a map of county boundaries.\n\n\nYou will need to call geom_sf() twice.\nMake the map background white.\nInstall the ggthemes package, and add the following layer to use a clean map theme: + ggthemes::theme_map()\n\n\n\nCodeggplot() + \n  geom_sf(data = mn_counties, fill = \"white\") + \n  geom_sf(data = mn_cities) + \n  ggthemes::theme_map() \n\n\n\n\n\n\n\nCustomize Colors\n\nWe can use traditional ggplot2 aesthetics (e.g., fill, color) to display location specific attributes. Below we only plot large cities, and we color and size cities according to their population.\n\n\nCodeggplot() +\n    geom_sf(data = mn_counties, fill = \"white\") + \n    geom_sf(data = mn_cities |&gt; filter(Population &gt;= 10000), mapping = aes(color = Population, size = Population)) + # cities layer\n    scale_color_viridis_c() + # continuous (gradient) color scale\n    labs(title = \"Minnesota Cities with Population &gt;= 10,000\") + \n    ggthemes::theme_map() +\n    theme(legend.position = \"bottom\") # move legend\n\n\n\n\n\n\n\nLook up the scale_color_viridis_c() documentation via the ggplot2 reference.\n\nRead the function description at the top. What is the advantage of using this function for making color palettes? This function turns the colors into a scale that is seeable for color blind individuals making your data more accessable to more people.\nLook through the examples section. What is the difference between the _d(), _c(), and _b() variants of this function?\n\nThe d stands for discrete scale, while the c stands for continuous scale (such as the one used above), and the b stands for a binned scale. You have to use a different function depending on what sort of data you are using.\nAdding Elevation Raster Data\nWhere are large cities located? Is there some relationship to local geography/terrain?\n\nTo investigate these questions, we can obtain elevation data to include on the map using the elevatr package. We encounter two new functions here—we can look up their documentation to make sense of the code by entering the following in the Console:\n\n\n?elevatr::get_elev_raster\n?terra::as.data.frame\n\n\nCodeelevation &lt;- elevatr::get_elev_raster(mn_counties, z = 5, clip = \"bbox\")\nraster::crs(elevation) &lt;- sf::st_crs(mn_counties)\n\n# Convert to data frame for plotting\nelev_df &lt;- elevation |&gt; terra::as.data.frame(xy = TRUE)\ncolnames(elev_df) &lt;- c(\"x\", \"y\", \"elevation\")\n\n\nBuild on our existing map by adding a raster layer for elevation as the background.\n\nLook up the documentation for geom_raster() to plot the elevation data from elev_df. This will be the first layer of the plot.\nLook at the documentation for scale_fill_gradient() to add the following elevation color scale: \"darkgreen\" represents the lowest elevations, and \"white\" represents the highest elevations.\nAdd in the layers from the map above to show the largest cities and the county outlines. To remove a background color, use fill = NA.\n\n\nCodeggplot() +\n    geom_raster(data = elev_df, aes(x = x, y = y, fill = elevation)) + \n    scale_fill_gradient(high = \"white\", low = \"darkgreen\") + \n    geom_sf(data = mn_counties, fill = \"NA\") + \n    geom_sf(data = mn_cities |&gt; filter(Population &gt;= 10000), mapping = aes(color = Population, size = Population)) + # cities layer\n    scale_color_viridis_c() + # continuous (gradient) color scale\n    labs(title = \"Minnesota Cities with Population &gt;= 10,000\") + \n    ggthemes::theme_map() +\n    theme(legend.position = \"bottom\")  # move legend \n\n\n\n\n\n\n\nZoom in to Twin Cities and Add Water\n\nThe bulk of the interesting information in this map is in the Twin Cities area. Let’s zoom in to this area.\n\n\nWe can use the st_bbox() function to get the bounding box for a spatial object—we do this after filtering to the 7 counties in the Twin Cities.\nWe then use st_crop() to trim a spatial object to a given bounding box.\n\n\nCodeseven_countyarea &lt;- mn_counties |&gt;\n    filter(name %in% c(\"Anoka\", \"Hennepin\", \"Ramsey\", \"Dakota\", \"Carver\", \"Washington\", \"Scott\")) |&gt; \n    st_bbox()\nseven_countyarea\n\n     xmin      ymin      xmax      ymax \n 419967.1 4924212.8  521254.8 5029157.4 \n\nCodeelevation &lt;- elevatr::get_elev_raster(mn_counties |&gt; st_crop(seven_countyarea), z = 9, clip = \"bbox\")\nraster::crs(elevation) &lt;- sf::st_crs(mn_counties)\n\n# Convert to data frame for plotting\nelev_df &lt;- elevation |&gt; terra::as.data.frame(xy = TRUE)\ncolnames(elev_df) &lt;- c(\"x\", \"y\", \"elevation\")\n\n\nIn the plot below, we add a layer for water information and a coord_sf() layer to restrict the x and y-axis limits to the Twin Cities bounding box. (Without this layer, the map would zoom back out to show all counties and bodies of water).\n\nCodeggplot() +\n    geom_raster(data = elev_df, aes(x = x, y = y, fill = elevation)) + \n    geom_sf(data = mn_counties, fill = NA, color = \"black\") + # county boundary layer\n    geom_sf(data = mn_water, fill = \"lightsteelblue1\", color = \"lightsteelblue1\") + # NEW: river/lake layer\n    geom_sf(data = mn_cities |&gt; filter(Population &gt;= 10000), mapping = aes(color = Population, size = Population)) + # cities layer\n    scale_color_viridis_c(option = \"magma\") + # continuous (gradient) color scale\n    scale_fill_gradient(low = \"darkgreen\", high = \"white\") + # continuous (gradient) fill scale\n    coord_sf(xlim = seven_countyarea[c(\"xmin\", \"xmax\")], ylim = seven_countyarea[c(\"ymin\", \"ymax\")]) + # NEW: crop map to Twin Cities bounding box\n    labs(title = \"Twin Cities with Population &gt;= 10,000\") + \n    ggthemes::theme_map() +\n    theme(legend.position = \"none\") # remove legend\n\n\n\n\n\n\n\nLet’s add to the above code chunk to save the map above to an image file called tc_map_zoom.png in the figures folder. The code example below shows a general template for saving a plot to file. Choose a reasonable width and height. (There are also jpeg() and pdf() functions for writing images.)\n\nCodepng(\"../figures/tc_map_zoom.png\", width = 500, height = 500)\nggplot() +\n    geom_raster(data = elev_df, aes(x = x, y = y, fill = elevation)) + \n    geom_sf(data = mn_counties, fill = NA, color = \"black\") + # county boundary layer\n    geom_sf(data = mn_water, fill = \"lightsteelblue1\", color = \"lightsteelblue1\") + # NEW: river/lake layer\n    geom_sf(data = mn_cities |&gt; filter(Population &gt;= 10000), mapping = aes(color = Population, size = Population)) + # cities layer\n    scale_color_viridis_c(option = \"magma\") + # continuous (gradient) color scale\n    scale_fill_gradient(low = \"darkgreen\", high = \"white\") + # continuous (gradient) fill scale\n    coord_sf(xlim = seven_countyarea[c(\"xmin\", \"xmax\")], ylim = seven_countyarea[c(\"ymin\", \"ymax\")]) + # NEW: crop map to Twin Cities bounding box\n    labs(title = \"Twin Cities with Population &gt;= 10,000\") + \n    ggthemes::theme_map() +\n    theme(legend.position = \"none\") \ndev.off()",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 Advanced Spatial Viz P1</span>"
    ]
  },
  {
    "objectID": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#going-beyond---twin-cities-map-with-leaflet",
    "href": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#going-beyond---twin-cities-map-with-leaflet",
    "title": "4 Advanced Spatial Viz P1",
    "section": "Going Beyond - Twin Cities Map with leaflet\n",
    "text": "Going Beyond - Twin Cities Map with leaflet\n\nBelow we show how to make the MN counties map in the leaflet package.\n\nCodelibrary(leaflet)\n\nmn_counties_leaf &lt;- mn_counties |&gt; st_transform(4326) # Leaflet expects this CRS for vectors\nmn_cities_leaf &lt;- mn_cities |&gt; st_transform(4326)\n\ncities_per_county &lt;- st_join(mn_cities_leaf, mn_counties_leaf) |&gt;\n    st_drop_geometry() |&gt; # removes geometry - makes the following calculation more efficient\n    count(name) \n\nmn_counties_leaf |&gt; \n    filter(name %in% c(\"Anoka\", \"Hennepin\", \"Ramsey\", \"Dakota\", \"Carver\", \"Washington\", \"Scott\")) |&gt;\n    left_join(cities_per_county) |&gt;\n    leaflet() |&gt; \n    addProviderTiles(\"CartoDB.Positron\") |&gt; \n    addPolygons(\n        color = \"#444444\", weight = 1, smoothFactor = 0.5, opacity = 1.0,\n        fillOpacity = 0.5, fillColor = ~colorQuantile(\"YlOrRd\", n)(n),\n        highlightOptions = highlightOptions(color = \"white\", weight = 2, bringToFront = TRUE)) |&gt;\n    addCircles(data = mn_cities_leaf |&gt; filter(County %in% paste(c(\"Anoka\", \"Hennepin\", \"Ramsey\", \"Dakota\", \"Carver\", \"Washington\", \"Scott\"), \"County\")), color = \"#444444\")",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 Advanced Spatial Viz P1</span>"
    ]
  },
  {
    "objectID": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#done",
    "href": "src/ica/04_adv_maps/code/Advanced_spatial_data_viz_P1.html#done",
    "title": "4 Advanced Spatial Viz P1",
    "section": "Done!",
    "text": "Done!\n\nCheck the ICA Instructions for how to (a) push your code to GitHub and (b) update your portfolio website",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 Advanced Spatial Viz P1</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P1.html",
    "href": "src/ica/Advanced_data_wrangling_P1.html",
    "title": "6 Adv Data Wrangling P1",
    "section": "",
    "text": "🧩 Learning Goals\nBy the end of this lesson, you should be able to:",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6 Adv Data Wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P1.html#learning-goals",
    "href": "src/ica/Advanced_data_wrangling_P1.html#learning-goals",
    "title": "6 Adv Data Wrangling P1",
    "section": "",
    "text": "Determine the class of a given object and identify concerns to be wary of when manipulating an object of that class (numerics, logicals, factors, dates, strings, data.frames)\nExplain what vector recycling is, when it can be a problem, and how to avoid those problems\nUse a variety of functions to wrangle numerical and logical data\nExtract date-time information using the lubridate package\nUse the forcats package to wrangle factor data",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6 Adv Data Wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P1.html#helpful-cheatsheets",
    "href": "src/ica/Advanced_data_wrangling_P1.html#helpful-cheatsheets",
    "title": "6 Adv Data Wrangling P1",
    "section": "Helpful Cheatsheets",
    "text": "Helpful Cheatsheets\nRStudio (Posit) maintains a collection of wonderful cheatsheets. The following will be helpful:\n\nData transformation with dplyr\nDates and times with lubridate\nFactors with forcats\n\nData Wrangling Verbs (from Stat/Comp 112)\n\n\nmutate(): creates/changes columns/elements in a data frame/tibble\n\nselect(): keeps subset of columns/elements in a data frame/tibble\n\nfilter(): keeps subsets of rows in a data frame/tibble\n\narrange(): sorts rows in a data frame/tibble\n\ngroup_by(): internally groups rows in data frame/tibble by values in 1 or more columsn/elements\n\nsummarize(): collapses/combines information across rows using functions such as n(), sum(), mean(), min(), max(), median(), sd()\n\n\ncount(): shortcut for group_by() |&gt; summarize(n = n())\n\n\nleft_join(): mutating join of two data frames/tibbles keeping all rows in left data frame\n\nfull_join(): mutating join of two data frames/tibbles keeping all rows in both data frames\n\ninner_join(): mutating join of two data frames/tibbles keeping rows in left data frame that find match in right\n\nsemi_join(): filtering join of two data frames/tibbles keeping rows in left data frame that find match in right\n\nanti_join(): filtering join of two data frames/tibbles keeping rows in left data frame that do not find match in right\n\npivot_wider(): rearrange values from two columns to many(one column becomes the names of new variables, one column becomes the values of the new variables)\n\npivot_longer(): rearrange values from many columns to two (the names of the columns go to one new variable, the values of the columns go to a second new variable)",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6 Adv Data Wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P1.html#vectors",
    "href": "src/ica/Advanced_data_wrangling_P1.html#vectors",
    "title": "6 Adv Data Wrangling P1",
    "section": "Vectors",
    "text": "Vectors\nAn atomic vector is a storage container in R where all elements in the container are of the same type. The types that are relevant to data science are:\n\n\nlogical (also known as boolean)\nnumbers\n\ninteger\n\nnumeric floating point (also known as double)\n\n\n\ncharacter string\n\nDate and date-time (saved as POSIXct)\nfactor\n\nFunction documentation will refer to vectors frequently.\nSee examples below:\n\n\nggplot2::scale_x_continuous()\n\n\nbreaks: A numeric vector of positions\n\nlabels: A character vector giving labels (must be same length as breaks)\n\n\n\nshiny::sliderInput()\n\n\nvalue: The initial value of the slider […] A length one vector will create a regular slider; a length two vector will create a double-ended range slider.\n\n\n\nWhen you need a vector, you can create one manually using\n\n\nc(): the combine function\n\nOr you can create one based on available data using\n\n\ndataset |&gt; mutate(newvar = variable &gt; 5) |&gt; pull(newvar): taking one column out of a dataset\n\ndataset |&gt; pull(variable) |&gt; unique(): taking one column out of a dataset and finding unique values\n\n\nCodec(\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\")\n\n[1] \"Fair\"      \"Good\"      \"Very Good\" \"Premium\"   \"Ideal\"    \n\nCodediamonds |&gt; pull(cut) |&gt; unique()\n\n[1] Ideal     Premium   Good      Very Good Fair     \nLevels: Fair &lt; Good &lt; Very Good &lt; Premium &lt; Ideal",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6 Adv Data Wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P1.html#logicals",
    "href": "src/ica/Advanced_data_wrangling_P1.html#logicals",
    "title": "6 Adv Data Wrangling P1",
    "section": "Logicals",
    "text": "Logicals\nNotes\nWhat does a logical vector look like?\n\nCodex &lt;- c(TRUE, FALSE, NA)\nx\n\n[1]  TRUE FALSE    NA\n\nCodeclass(x)\n\n[1] \"logical\"\n\n\nYou will often create logical vectors with comparison operators: &gt;, &lt;, &lt;=, &gt;=, ==, !=.\n\nCodex &lt;- c(1, 2, 9, 12)\nx &lt; 2\n\n[1]  TRUE FALSE FALSE FALSE\n\nCodex &lt;= 2\n\n[1]  TRUE  TRUE FALSE FALSE\n\nCodex &gt; 9\n\n[1] FALSE FALSE FALSE  TRUE\n\nCodex &gt;= 9\n\n[1] FALSE FALSE  TRUE  TRUE\n\nCodex == 12\n\n[1] FALSE FALSE FALSE  TRUE\n\nCodex != 12\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n\nWhen you want to check for set containment, the %in% operator is the correct way to do this (as opposed to ==).\n\nCodex &lt;- c(1, 2, 9, 4)\nx == c(1, 2, 4)\n\nWarning in x == c(1, 2, 4): longer object length is not a multiple of shorter\nobject length\n\n\n[1]  TRUE  TRUE FALSE FALSE\n\nCodex %in% c(1, 2, 4)\n\n[1]  TRUE  TRUE FALSE  TRUE\n\n\nThe Warning: longer object length is not a multiple of shorter object length is a manifestation of vector recycling.\nIn R, if two vectors are being combined or compared, the shorter one will be repeated to match the length of the longer one–even if longer object length isn’t a multiple of the shorter object length. We can see the exact recycling that happens below:\n\nCodex &lt;- c(1, 2, 9, 4)\nx == c(1, 2, 4)\n\n[1]  TRUE  TRUE FALSE FALSE\n\nCodex == c(1, 2, 4, 1) # This line demonstrates the recycling that happens on the previous line\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nLogical vectors can also be created with functions. is.na() is one useful example:\n\nCodex &lt;- c(1, 4, 9, NA)\nx == NA\n\n[1] NA NA NA NA\n\nCodeis.na(x)\n\n[1] FALSE FALSE FALSE  TRUE\n\n\nWe can negate a logical object with !. We can combine logical objects with & (and) and | (or).\n\nCodex &lt;- c(1, 2, 4, 9)\nx &gt; 1 & x &lt; 5\n\n[1] FALSE  TRUE  TRUE FALSE\n\nCode!(x &gt; 1 & x &lt; 5)\n\n[1]  TRUE FALSE FALSE  TRUE\n\nCodex &lt; 2 | x &gt; 8\n\n[1]  TRUE FALSE FALSE  TRUE\n\n\nWe can summarize logical vectors with:\n\n\nany(): Are ANY of the values TRUE?\n\nall(): Are ALL of the values TRUE?\n\nsum(): How many of the values are TRUE?\n\nmean(): What fraction of the values are TRUE?\n\n\nCodex &lt;- c(1, 2, 4, 9)\nany(x == 1)\n\n[1] TRUE\n\nCodeall(x &lt; 10)\n\n[1] TRUE\n\nCodesum(x == 1)\n\n[1] 1\n\nCodemean(x == 1)\n\n[1] 0.25\n\n\nif_else() and case_when() are functions that allow you to return values depending on the value of a logical vector. You’ll explore the documentation for these in the following exercises.\n\n\n\n\n\n\nNote: ifelse() (from base R) and if_else() (from tidyverse) are different functions. We prefer if_else() for many reasons (examples below).\n\nNoisy to make sure you catch issues/bugs\nCan explicitly handle missing values\nKeeps dates as dates\n\n\nExamples\n\nCodex &lt;- c(-1, -2, 4, 9, NA)\n\nifelse(x &gt; 0, 'positive', 'negative')\n\n[1] \"negative\" \"negative\" \"positive\" \"positive\" NA        \n\nCodeif_else(x &gt; 0, 'positive', 'negative')\n\n[1] \"negative\" \"negative\" \"positive\" \"positive\" NA        \n\nCodeifelse(x &gt; 0, 1, 'negative') # Bad: doesn't complain with combo of data types\n\n[1] \"negative\" \"negative\" \"1\"        \"1\"        NA        \n\nCodeif_else(x &gt; 0, 1, 'negative') # Good:noisy to make sure you catch issues\n\nError in `if_else()`:\n! Can't combine `true` &lt;double&gt; and `false` &lt;character&gt;.\n\nCodeif_else(x &gt; 0, 'positive', 'negative', missing = 'missing') # Good: can explicitly handle NA\n\n[1] \"negative\" \"negative\" \"positive\" \"positive\" \"missing\" \n\nCodefun_dates &lt;- mdy('1-1-2025') + 0:365\nifelse(fun_dates &lt; today(), fun_dates + years(), fun_dates) # Bad: converts dates to integers\n\n  [1] 20454 20455 20456 20457 20458 20459 20460 20461 20462 20463 20464 20465\n [13] 20466 20467 20468 20469 20470 20471 20472 20473 20474 20475 20476 20477\n [25] 20478 20479 20480 20481 20482 20483 20484 20485 20486 20487 20488 20489\n [37] 20490 20491 20492 20493 20494 20495 20496 20497 20498 20499 20500 20501\n [49] 20502 20503 20504 20505 20506 20507 20508 20509 20510 20511 20512 20513\n [61] 20514 20515 20516 20517 20518 20519 20520 20521 20522 20523 20524 20525\n [73] 20526 20527 20528 20529 20530 20531 20532 20533 20534 20535 20536 20537\n [85] 20538 20539 20540 20541 20542 20543 20544 20545 20546 20547 20548 20549\n [97] 20550 20551 20552 20553 20554 20555 20556 20557 20558 20559 20560 20561\n[109] 20562 20563 20564 20565 20566 20567 20568 20569 20570 20571 20572 20573\n[121] 20574 20575 20576 20577 20578 20579 20580 20581 20582 20583 20584 20585\n[133] 20586 20587 20588 20589 20590 20591 20592 20593 20594 20595 20596 20597\n[145] 20598 20599 20600 20601 20602 20603 20604 20605 20606 20607 20608 20609\n[157] 20610 20611 20612 20613 20614 20615 20616 20617 20618 20619 20620 20621\n[169] 20622 20623 20624 20625 20626 20627 20628 20629 20630 20631 20632 20633\n[181] 20634 20635 20636 20637 20638 20639 20640 20641 20642 20643 20644 20645\n[193] 20646 20647 20648 20649 20650 20651 20652 20653 20654 20655 20656 20657\n[205] 20658 20659 20660 20661 20662 20663 20664 20665 20666 20667 20668 20669\n[217] 20670 20671 20672 20673 20674 20675 20676 20677 20678 20679 20680 20681\n[229] 20682 20683 20684 20685 20686 20687 20688 20689 20690 20691 20692 20693\n[241] 20694 20695 20696 20697 20698 20699 20700 20701 20702 20703 20704 20705\n[253] 20706 20707 20708 20709 20710 20711 20712 20713 20714 20715 20716 20717\n[265] 20718 20719 20720 20721 20722 20723 20724 20725 20726 20727 20728 20729\n[277] 20730 20731 20732 20368 20369 20370 20371 20372 20373 20374 20375 20376\n[289] 20377 20378 20379 20380 20381 20382 20383 20384 20385 20386 20387 20388\n[301] 20389 20390 20391 20392 20393 20394 20395 20396 20397 20398 20399 20400\n[313] 20401 20402 20403 20404 20405 20406 20407 20408 20409 20410 20411 20412\n[325] 20413 20414 20415 20416 20417 20418 20419 20420 20421 20422 20423 20424\n[337] 20425 20426 20427 20428 20429 20430 20431 20432 20433 20434 20435 20436\n[349] 20437 20438 20439 20440 20441 20442 20443 20444 20445 20446 20447 20448\n[361] 20449 20450 20451 20452 20453 20454\n\nCodeif_else(fun_dates &lt; today(), fun_dates + years(), fun_dates) # Good: keeps dates as dates\n\n  [1] \"2026-01-01\" \"2026-01-02\" \"2026-01-03\" \"2026-01-04\" \"2026-01-05\"\n  [6] \"2026-01-06\" \"2026-01-07\" \"2026-01-08\" \"2026-01-09\" \"2026-01-10\"\n [11] \"2026-01-11\" \"2026-01-12\" \"2026-01-13\" \"2026-01-14\" \"2026-01-15\"\n [16] \"2026-01-16\" \"2026-01-17\" \"2026-01-18\" \"2026-01-19\" \"2026-01-20\"\n [21] \"2026-01-21\" \"2026-01-22\" \"2026-01-23\" \"2026-01-24\" \"2026-01-25\"\n [26] \"2026-01-26\" \"2026-01-27\" \"2026-01-28\" \"2026-01-29\" \"2026-01-30\"\n [31] \"2026-01-31\" \"2026-02-01\" \"2026-02-02\" \"2026-02-03\" \"2026-02-04\"\n [36] \"2026-02-05\" \"2026-02-06\" \"2026-02-07\" \"2026-02-08\" \"2026-02-09\"\n [41] \"2026-02-10\" \"2026-02-11\" \"2026-02-12\" \"2026-02-13\" \"2026-02-14\"\n [46] \"2026-02-15\" \"2026-02-16\" \"2026-02-17\" \"2026-02-18\" \"2026-02-19\"\n [51] \"2026-02-20\" \"2026-02-21\" \"2026-02-22\" \"2026-02-23\" \"2026-02-24\"\n [56] \"2026-02-25\" \"2026-02-26\" \"2026-02-27\" \"2026-02-28\" \"2026-03-01\"\n [61] \"2026-03-02\" \"2026-03-03\" \"2026-03-04\" \"2026-03-05\" \"2026-03-06\"\n [66] \"2026-03-07\" \"2026-03-08\" \"2026-03-09\" \"2026-03-10\" \"2026-03-11\"\n [71] \"2026-03-12\" \"2026-03-13\" \"2026-03-14\" \"2026-03-15\" \"2026-03-16\"\n [76] \"2026-03-17\" \"2026-03-18\" \"2026-03-19\" \"2026-03-20\" \"2026-03-21\"\n [81] \"2026-03-22\" \"2026-03-23\" \"2026-03-24\" \"2026-03-25\" \"2026-03-26\"\n [86] \"2026-03-27\" \"2026-03-28\" \"2026-03-29\" \"2026-03-30\" \"2026-03-31\"\n [91] \"2026-04-01\" \"2026-04-02\" \"2026-04-03\" \"2026-04-04\" \"2026-04-05\"\n [96] \"2026-04-06\" \"2026-04-07\" \"2026-04-08\" \"2026-04-09\" \"2026-04-10\"\n[101] \"2026-04-11\" \"2026-04-12\" \"2026-04-13\" \"2026-04-14\" \"2026-04-15\"\n[106] \"2026-04-16\" \"2026-04-17\" \"2026-04-18\" \"2026-04-19\" \"2026-04-20\"\n[111] \"2026-04-21\" \"2026-04-22\" \"2026-04-23\" \"2026-04-24\" \"2026-04-25\"\n[116] \"2026-04-26\" \"2026-04-27\" \"2026-04-28\" \"2026-04-29\" \"2026-04-30\"\n[121] \"2026-05-01\" \"2026-05-02\" \"2026-05-03\" \"2026-05-04\" \"2026-05-05\"\n[126] \"2026-05-06\" \"2026-05-07\" \"2026-05-08\" \"2026-05-09\" \"2026-05-10\"\n[131] \"2026-05-11\" \"2026-05-12\" \"2026-05-13\" \"2026-05-14\" \"2026-05-15\"\n[136] \"2026-05-16\" \"2026-05-17\" \"2026-05-18\" \"2026-05-19\" \"2026-05-20\"\n[141] \"2026-05-21\" \"2026-05-22\" \"2026-05-23\" \"2026-05-24\" \"2026-05-25\"\n[146] \"2026-05-26\" \"2026-05-27\" \"2026-05-28\" \"2026-05-29\" \"2026-05-30\"\n[151] \"2026-05-31\" \"2026-06-01\" \"2026-06-02\" \"2026-06-03\" \"2026-06-04\"\n[156] \"2026-06-05\" \"2026-06-06\" \"2026-06-07\" \"2026-06-08\" \"2026-06-09\"\n[161] \"2026-06-10\" \"2026-06-11\" \"2026-06-12\" \"2026-06-13\" \"2026-06-14\"\n[166] \"2026-06-15\" \"2026-06-16\" \"2026-06-17\" \"2026-06-18\" \"2026-06-19\"\n[171] \"2026-06-20\" \"2026-06-21\" \"2026-06-22\" \"2026-06-23\" \"2026-06-24\"\n[176] \"2026-06-25\" \"2026-06-26\" \"2026-06-27\" \"2026-06-28\" \"2026-06-29\"\n[181] \"2026-06-30\" \"2026-07-01\" \"2026-07-02\" \"2026-07-03\" \"2026-07-04\"\n[186] \"2026-07-05\" \"2026-07-06\" \"2026-07-07\" \"2026-07-08\" \"2026-07-09\"\n[191] \"2026-07-10\" \"2026-07-11\" \"2026-07-12\" \"2026-07-13\" \"2026-07-14\"\n[196] \"2026-07-15\" \"2026-07-16\" \"2026-07-17\" \"2026-07-18\" \"2026-07-19\"\n[201] \"2026-07-20\" \"2026-07-21\" \"2026-07-22\" \"2026-07-23\" \"2026-07-24\"\n[206] \"2026-07-25\" \"2026-07-26\" \"2026-07-27\" \"2026-07-28\" \"2026-07-29\"\n[211] \"2026-07-30\" \"2026-07-31\" \"2026-08-01\" \"2026-08-02\" \"2026-08-03\"\n[216] \"2026-08-04\" \"2026-08-05\" \"2026-08-06\" \"2026-08-07\" \"2026-08-08\"\n[221] \"2026-08-09\" \"2026-08-10\" \"2026-08-11\" \"2026-08-12\" \"2026-08-13\"\n[226] \"2026-08-14\" \"2026-08-15\" \"2026-08-16\" \"2026-08-17\" \"2026-08-18\"\n[231] \"2026-08-19\" \"2026-08-20\" \"2026-08-21\" \"2026-08-22\" \"2026-08-23\"\n[236] \"2026-08-24\" \"2026-08-25\" \"2026-08-26\" \"2026-08-27\" \"2026-08-28\"\n[241] \"2026-08-29\" \"2026-08-30\" \"2026-08-31\" \"2026-09-01\" \"2026-09-02\"\n[246] \"2026-09-03\" \"2026-09-04\" \"2026-09-05\" \"2026-09-06\" \"2026-09-07\"\n[251] \"2026-09-08\" \"2026-09-09\" \"2026-09-10\" \"2026-09-11\" \"2026-09-12\"\n[256] \"2026-09-13\" \"2026-09-14\" \"2026-09-15\" \"2026-09-16\" \"2026-09-17\"\n[261] \"2026-09-18\" \"2026-09-19\" \"2026-09-20\" \"2026-09-21\" \"2026-09-22\"\n[266] \"2026-09-23\" \"2026-09-24\" \"2026-09-25\" \"2026-09-26\" \"2026-09-27\"\n[271] \"2026-09-28\" \"2026-09-29\" \"2026-09-30\" \"2026-10-01\" \"2026-10-02\"\n[276] \"2026-10-03\" \"2026-10-04\" \"2026-10-05\" \"2026-10-06\" \"2025-10-07\"\n[281] \"2025-10-08\" \"2025-10-09\" \"2025-10-10\" \"2025-10-11\" \"2025-10-12\"\n[286] \"2025-10-13\" \"2025-10-14\" \"2025-10-15\" \"2025-10-16\" \"2025-10-17\"\n[291] \"2025-10-18\" \"2025-10-19\" \"2025-10-20\" \"2025-10-21\" \"2025-10-22\"\n[296] \"2025-10-23\" \"2025-10-24\" \"2025-10-25\" \"2025-10-26\" \"2025-10-27\"\n[301] \"2025-10-28\" \"2025-10-29\" \"2025-10-30\" \"2025-10-31\" \"2025-11-01\"\n[306] \"2025-11-02\" \"2025-11-03\" \"2025-11-04\" \"2025-11-05\" \"2025-11-06\"\n[311] \"2025-11-07\" \"2025-11-08\" \"2025-11-09\" \"2025-11-10\" \"2025-11-11\"\n[316] \"2025-11-12\" \"2025-11-13\" \"2025-11-14\" \"2025-11-15\" \"2025-11-16\"\n[321] \"2025-11-17\" \"2025-11-18\" \"2025-11-19\" \"2025-11-20\" \"2025-11-21\"\n[326] \"2025-11-22\" \"2025-11-23\" \"2025-11-24\" \"2025-11-25\" \"2025-11-26\"\n[331] \"2025-11-27\" \"2025-11-28\" \"2025-11-29\" \"2025-11-30\" \"2025-12-01\"\n[336] \"2025-12-02\" \"2025-12-03\" \"2025-12-04\" \"2025-12-05\" \"2025-12-06\"\n[341] \"2025-12-07\" \"2025-12-08\" \"2025-12-09\" \"2025-12-10\" \"2025-12-11\"\n[346] \"2025-12-12\" \"2025-12-13\" \"2025-12-14\" \"2025-12-15\" \"2025-12-16\"\n[351] \"2025-12-17\" \"2025-12-18\" \"2025-12-19\" \"2025-12-20\" \"2025-12-21\"\n[356] \"2025-12-22\" \"2025-12-23\" \"2025-12-24\" \"2025-12-25\" \"2025-12-26\"\n[361] \"2025-12-27\" \"2025-12-28\" \"2025-12-29\" \"2025-12-30\" \"2025-12-31\"\n[366] \"2026-01-01\"\n\n\n\n\n\n\nExercises\nLoad the diamonds dataset, and filter to the first 1000 diamonds.\n\nCodedata(diamonds)\ndiamonds &lt;- diamonds |&gt; \n    slice_head(n = 1000)\n\n\nUsing tidyverse functions, complete the following:\n\nSubset to diamonds that are less than 400 dollars or more than 10000 dollars.\nSubset to diamonds that are between 500 and 600 dollars (inclusive).\nHow many diamonds are of either Fair, Premium, or Ideal cut (a total count)? What fraction of diamonds are of Fair, Premium, or Ideal cut?\n\nFirst, do this a wrong way with ==. Predict the warning message that you will receive.\nSecond, do this the correct way with an appropriate logical operator.\n\n\nAre there any diamonds of Fair cut that are more than $3000? Are all diamonds of Ideal cut more than $2000?\nCreate two new categorized versions of price by looking up the documentation for if_else() and case_when():\n\n\nprice_cat1: “low” if price is less than 500 and “high” otherwise\n\nprice_cat2: “low” if price is less than 500, “medium” if price is between 500 and 1000 dollars inclusive, and “high” otherwise.\n\n\n\n\nCode#1\ndiamonds |&gt; \n  filter(price &lt; 400 | price &gt; 10000)\n\n# A tibble: 30 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 20 more rows\n\nCode#2\ndiamonds |&gt; \n  filter(price &gt;= 500 & price &lt;= 600)\n\n# A tibble: 90 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.35 Ideal     I     VS1      60.9  57     552  4.54  4.59  2.78\n 2  0.3  Premium   D     SI1      62.6  59     552  4.23  4.27  2.66\n 3  0.3  Ideal     D     SI1      62.5  57     552  4.29  4.32  2.69\n 4  0.3  Ideal     D     SI1      62.1  56     552  4.3   4.33  2.68\n 5  0.42 Premium   I     SI2      61.5  59     552  4.78  4.84  2.96\n 6  0.28 Ideal     G     VVS2     61.4  56     553  4.19  4.22  2.58\n 7  0.32 Ideal     I     VVS1     62    55.3   553  4.39  4.42  2.73\n 8  0.31 Very Good G     SI1      63.3  57     553  4.33  4.3   2.73\n 9  0.31 Premium   G     SI1      61.8  58     553  4.35  4.32  2.68\n10  0.24 Premium   E     VVS1     60.7  58     553  4.01  4.03  2.44\n# ℹ 80 more rows\n\nCode#3\ndiamonds |&gt; \n  mutate(is_fpi = cut %in% c(\"Fair\", \"Premium\", \"Ideal\")) |&gt; \n  summarise(Total_fpi = sum(is_fpi), Frac_fpi = mean(is_fpi))\n\n# A tibble: 1 × 2\n  Total_fpi Frac_fpi\n      &lt;int&gt;    &lt;dbl&gt;\n1       685    0.685\n\nCode#4\ndiamonds |&gt; \n  filter(cut == \"Fair\") |&gt; \n  summarise(price_high = any(price &gt; 3000))\n\n# A tibble: 1 × 1\n  price_high\n  &lt;lgl&gt;     \n1 FALSE     \n\nCodediamonds |&gt; \n  filter(cut == \"Ideal\") |&gt; \n  summarise(price_high = all(price &gt; 2000))\n\n# A tibble: 1 × 1\n  price_high\n  &lt;lgl&gt;     \n1 FALSE     \n\nCode#5\ndiamonds |&gt; \n  mutate(\n  price_cat1 = if_else(price &lt; 500, \"low\", \"high\"), \n  price_cat2 = case_when(\n    price &lt; 500 ~ \"low\",\n    price &gt;= 500 & price &lt;= 1000 ~ \"medium\",\n    price &gt; 1000 ~ \"high\")\n  )\n\n# A tibble: 1,000 × 12\n   carat cut       color clarity depth table price     x     y     z price_cat1\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43 low       \n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31 low       \n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31 low       \n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63 low       \n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75 low       \n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48 low       \n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47 low       \n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53 low       \n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49 low       \n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39 low       \n# ℹ 990 more rows\n# ℹ 1 more variable: price_cat2 &lt;chr&gt;",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6 Adv Data Wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P1.html#numerics",
    "href": "src/ica/Advanced_data_wrangling_P1.html#numerics",
    "title": "6 Adv Data Wrangling P1",
    "section": "Numerics",
    "text": "Numerics\nNotes\nNumerical data can be of class integer or numeric (representing real numbers).\n\nCodex &lt;- 1:3\nx\n\n[1] 1 2 3\n\nCodeclass(x)\n\n[1] \"integer\"\n\nCodex &lt;- c(1+1e-9, 2, 3)\nx\n\n[1] 1 2 3\n\nCodeclass(x)\n\n[1] \"numeric\"\n\n\nThe Numbers chapter in R4DS covers the following functions that are all useful for wrangling numeric data:\n\n\nn(), n_distinct(): Counting and counting the number of unique values\n\nsum(is.na()): Counting the number of missing values\n\nmin(), max()\n\n\npmin(), pmax(): Get the min and max across several vectors\nInteger division: %/%. Remainder: %%\n\n\n121 %/% 100 = 1 and 121 %% 100 = 21\n\n\n\n\nround(), floor(), ceiling(): Rounding functions (to a specified number of decimal places, to the largest integer below a number, to the smallest integer above a number)\n\ncut(): Cut a numerical vector into categories\n\ncumsum(), cummean(), cummin(), cummax(): Cumulative functions\n\nrank(): Provide the ranks of the numbers in a vector\n\nlead(), lag(): shift a vector by padding with NAs\nNumerical summaries: mean, median, min, max, quantile, sd, IQR\n\nNote that all numerical summary functions have an na.rm argument that should be set to TRUE if you have missing data.\n\n\nExercises\nExercises will be on HW4.\nThe best way to add these functions and operators to your vocabulary is to need to recall them. Refer to the list of functions above as you try the exercises.\nYou will need to reference function documentation to look at arguments and look in the Examples section.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6 Adv Data Wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P1.html#dates",
    "href": "src/ica/Advanced_data_wrangling_P1.html#dates",
    "title": "6 Adv Data Wrangling P1",
    "section": "Dates",
    "text": "Dates\nNotes\nThe lubridate package contains useful functions for working with dates and times. The lubridate function reference is a useful resource for finding the functions you need. We’ll take a brief tour of this reference page.\nWe’ll use the lakers dataset in the lubridate package to illustrate some examples.\n\nCodelakers &lt;- as_tibble(lakers)\nhead(lakers)\n\n# A tibble: 6 × 13\n     date opponent game_type time  period etype team  player result points type \n    &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;\n1  2.01e7 POR      home      12:00      1 jump… OFF   \"\"     \"\"          0 \"\"   \n2  2.01e7 POR      home      11:39      1 shot  LAL   \"Pau … \"miss…      0 \"hoo…\n3  2.01e7 POR      home      11:37      1 rebo… LAL   \"Vlad… \"\"          0 \"off\"\n4  2.01e7 POR      home      11:25      1 shot  LAL   \"Dere… \"miss…      0 \"lay…\n5  2.01e7 POR      home      11:23      1 rebo… LAL   \"Pau … \"\"          0 \"off\"\n6  2.01e7 POR      home      11:22      1 shot  LAL   \"Pau … \"made\"      2 \"hoo…\n# ℹ 2 more variables: x &lt;int&gt;, y &lt;int&gt;\n\n\nBelow we use date-time parsing functions to represent the date and time variables with date-time classes:\n\nCodelakers &lt;- lakers |&gt;\n    mutate(\n        date = ymd(date),\n        time = ms(time)\n    )\n\n\nBelow we use extraction functions to get components of the date-time objects:\n\nCodelakers_clean &lt;- lakers |&gt;\n    mutate(\n        year = year(date),\n        month = month(date),\n        day = day(date),\n        day_of_week = wday(date, label = TRUE),\n        minute = minute(time),\n        second = second(time)\n    )\nlakers_clean |&gt; select(year:second)\n\n# A tibble: 34,624 × 6\n    year month   day day_of_week minute second\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;ord&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1  2008    10    28 Tue             12      0\n 2  2008    10    28 Tue             11     39\n 3  2008    10    28 Tue             11     37\n 4  2008    10    28 Tue             11     25\n 5  2008    10    28 Tue             11     23\n 6  2008    10    28 Tue             11     22\n 7  2008    10    28 Tue             11     22\n 8  2008    10    28 Tue             11     22\n 9  2008    10    28 Tue             11      0\n10  2008    10    28 Tue             10     53\n# ℹ 34,614 more rows\n\nCodelakers_clean &lt;- lakers_clean |&gt;\n    group_by(date, opponent, period) |&gt;\n    arrange(date, opponent, period, desc(time)) |&gt;\n    mutate(\n        diff_btw_plays_sec = as.numeric(time - lag(time, 1))\n    )\nlakers_clean |&gt; select(date, opponent, time, period, diff_btw_plays_sec)\n\n# A tibble: 34,624 × 5\n# Groups:   date, opponent, period [314]\n   date       opponent time     period diff_btw_plays_sec\n   &lt;date&gt;     &lt;chr&gt;    &lt;Period&gt;  &lt;int&gt;              &lt;dbl&gt;\n 1 2008-10-28 POR      12M 0S        1                 NA\n 2 2008-10-28 POR      11M 39S       1                -21\n 3 2008-10-28 POR      11M 37S       1                 -2\n 4 2008-10-28 POR      11M 25S       1                -12\n 5 2008-10-28 POR      11M 23S       1                 -2\n 6 2008-10-28 POR      11M 22S       1                 -1\n 7 2008-10-28 POR      11M 22S       1                  0\n 8 2008-10-28 POR      11M 22S       1                  0\n 9 2008-10-28 POR      11M 0S        1                -22\n10 2008-10-28 POR      10M 53S       1                 -7\n# ℹ 34,614 more rows\n\n\nExercises\nExercises will be on HW4.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6 Adv Data Wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P1.html#factors",
    "href": "src/ica/Advanced_data_wrangling_P1.html#factors",
    "title": "6 Adv Data Wrangling P1",
    "section": "Factors",
    "text": "Factors\nNotes\nCreating factors\nIn R, factors are made up of two components: the actual values of the data and the possible levels within the factor. Creating a factor requires supplying both pieces of information.\n\nCodemonths &lt;- c(\"Mar\", \"Dec\", \"Jan\",  \"Apr\", \"Jul\")\n\n\nHowever, if we were to sort this vector, R would sort this vector alphabetically.\n\nCode# alphabetical sort\nsort(months)\n\n[1] \"Apr\" \"Dec\" \"Jan\" \"Jul\" \"Mar\"\n\n\nWe can fix this sorting by creating a factor version of months. The levels argument is a character vector that specifies the unique values that the factor can take. The order of the values in levels defines the sorting of the factor.\n\nCodemonths_fct &lt;- factor(months, levels = month.abb) # month.abb is a built-in variable\nmonths_fct\n\n[1] Mar Dec Jan Apr Jul\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\nCodesort(months_fct)\n\n[1] Jan Mar Apr Jul Dec\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\n\nWhat if we try to create a factor with values that aren’t in the levels? (e.g., a typo in a month name)\n\nCodemonths2 &lt;- c(\"Jna\", \"Mar\")\nfactor(months2, levels = month.abb)\n\n[1] &lt;NA&gt; Mar \nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\n\nBecause the NA is introduced silently (without any error or warnings), this can be dangerous. It might be better to use the fct() function in the forcats package instead:\n\nCodefct(months2, levels = month.abb)\n\nError in `fct()`:\n! All values of `x` must appear in `levels` or `na`\nℹ Missing level: \"Jna\"\n\n\nReordering factors\nWe’ll use a subset of the General Social Survey (GSS) dataset available in the forcats pacakges.\n\nCodedata(gss_cat)\nhead(gss_cat)\n\n# A tibble: 6 × 9\n   year marital         age race  rincome        partyid     relig denom tvhours\n  &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;       &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n1  2000 Never married    26 White $8000 to 9999  Ind,near r… Prot… Sout…      12\n2  2000 Divorced         48 White $8000 to 9999  Not str re… Prot… Bapt…      NA\n3  2000 Widowed          67 White Not applicable Independent Prot… No d…       2\n4  2000 Never married    39 White Not applicable Ind,near r… Orth… Not …       4\n5  2000 Divorced         25 White Not applicable Not str de… None  Not …       1\n6  2000 Married          25 White $20000 - 24999 Strong dem… Prot… Sout…      NA\n\n\nReordering the levels of a factor can be useful in plotting when categories would benefit from being sorted in a particular way:\n\nCoderelig_summary &lt;- gss_cat |&gt;\n    group_by(relig) |&gt;\n    summarize(\n        tvhours = mean(tvhours, na.rm = TRUE),\n        n = n()\n    )\n\nggplot(relig_summary, aes(x = tvhours, y = relig)) + \n    geom_point() +\n    theme_classic()\n\n\n\n\n\n\n\nWe can use fct_reorder() in forcats.\n\nThe first argument is the factor that you want to reorder the levels of\nThe second argument determines how the factor is sorted (analogous to what you put inside arrange() when sorting the rows of a data frame.)\n\n\nCodeggplot(relig_summary, aes(x = tvhours, y = fct_reorder(relig, tvhours))) +\n    geom_point() +\n    theme_classic()\n\n\n\n\n\n\n\nFor bar plots, we can use fct_infreq() to reorder levels from most to least common. This can be combined with fct_rev() to reverse the order (least to most common):\n\nCodegss_cat |&gt;\n    ggplot(aes(x = marital)) +\n    geom_bar() +\n    theme_classic()\n\n\n\n\n\n\nCodegss_cat |&gt;\n    mutate(marital = marital |&gt; fct_infreq() |&gt; fct_rev()) |&gt;\n    ggplot(aes(x = marital)) +\n    geom_bar() +\n    theme_classic()\n\n\n\n\n\n\n\nModifying factor levels\nWe talked about reordering the levels of a factor–what about changing the values of the levels themselves?\nFor example, the names of the political parties in the GSS could use elaboration (“str” isn’t a great label for “strong”) and clean up:\n\nCodegss_cat |&gt; count(partyid)\n\n# A tibble: 10 × 2\n   partyid                n\n   &lt;fct&gt;              &lt;int&gt;\n 1 No answer            154\n 2 Don't know             1\n 3 Other party          393\n 4 Strong republican   2314\n 5 Not str republican  3032\n 6 Ind,near rep        1791\n 7 Independent         4119\n 8 Ind,near dem        2499\n 9 Not str democrat    3690\n10 Strong democrat     3490\n\n\nWe can use fct_recode() on partyid with the new level names going on the left and the old levels on the right. Any levels that aren’t mentioned explicitly (i.e., “Don’t know” and “Other party”) will be left as is:\n\nCodegss_cat |&gt;\n    mutate(\n        partyid = fct_recode(partyid,\n            \"Republican, strong\"    = \"Strong republican\",\n            \"Republican, weak\"      = \"Not str republican\",\n            \"Independent, near rep\" = \"Ind,near rep\",\n            \"Independent, near dem\" = \"Ind,near dem\",\n            \"Democrat, weak\"        = \"Not str democrat\",\n            \"Democrat, strong\"      = \"Strong democrat\"\n        )\n    ) |&gt;\n    count(partyid)\n\n# A tibble: 10 × 2\n   partyid                   n\n   &lt;fct&gt;                 &lt;int&gt;\n 1 No answer               154\n 2 Don't know                1\n 3 Other party             393\n 4 Republican, strong     2314\n 5 Republican, weak       3032\n 6 Independent, near rep  1791\n 7 Independent            4119\n 8 Independent, near dem  2499\n 9 Democrat, weak         3690\n10 Democrat, strong       3490\n\n\nTo combine groups, we can assign multiple old levels to the same new level (“Other” maps to “No answer”, “Don’t know”, and “Other party”):\n\nCodegss_cat |&gt;\n    mutate(\n        partyid = fct_recode(partyid,\n            \"Republican, strong\"    = \"Strong republican\",\n            \"Republican, weak\"      = \"Not str republican\",\n            \"Independent, near rep\" = \"Ind,near rep\",\n            \"Independent, near dem\" = \"Ind,near dem\",\n            \"Democrat, weak\"        = \"Not str democrat\",\n            \"Democrat, strong\"      = \"Strong democrat\",\n            \"Other\"                 = \"No answer\",\n            \"Other\"                 = \"Don't know\",\n            \"Other\"                 = \"Other party\"\n        )\n    )\n\n# A tibble: 21,483 × 9\n    year marital         age race  rincome        partyid    relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2000 Never married    26 White $8000 to 9999  Independe… Prot… Sout…      12\n 2  2000 Divorced         48 White $8000 to 9999  Republica… Prot… Bapt…      NA\n 3  2000 Widowed          67 White Not applicable Independe… Prot… No d…       2\n 4  2000 Never married    39 White Not applicable Independe… Orth… Not …       4\n 5  2000 Divorced         25 White Not applicable Democrat,… None  Not …       1\n 6  2000 Married          25 White $20000 - 24999 Democrat,… Prot… Sout…      NA\n 7  2000 Never married    36 White $25000 or more Republica… Chri… Not …       3\n 8  2000 Divorced         44 White $7000 to 7999  Independe… Prot… Luth…      NA\n 9  2000 Married          44 White $25000 or more Democrat,… Prot… Other       0\n10  2000 Married          47 White $25000 or more Republica… Prot… Sout…       3\n# ℹ 21,473 more rows\n\n\nWe can use fct_collapse() to collapse many levels:\n\nCodegss_cat |&gt;\n    mutate(\n        partyid = fct_collapse(partyid,\n            \"Other\" = c(\"No answer\", \"Don't know\", \"Other party\"),\n            \"Republican\" = c(\"Strong republican\", \"Not str republican\"),\n            \"Independent\" = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n            \"Democrat\" = c(\"Not str democrat\", \"Strong democrat\")\n        )\n    ) |&gt;\n    count(partyid)\n\n# A tibble: 4 × 2\n  partyid         n\n  &lt;fct&gt;       &lt;int&gt;\n1 Other         548\n2 Republican   5346\n3 Independent  8409\n4 Democrat     7180\n\n\nExercises\n\nCreate a factor version of the following data with the levels in a sensible order.\n\n\nCoderatings &lt;- c(\"High\", \"Medium\", \"Low\")\nratings_fct &lt;- fct(ratings, levels = c(\"Low\", \"Medium\", \"High\"))\nratings_fct\n\n[1] High   Medium Low   \nLevels: Low Medium High\n\n\nMore exercises will be on HW4.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6 Adv Data Wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P1.html#done",
    "href": "src/ica/Advanced_data_wrangling_P1.html#done",
    "title": "6 Adv Data Wrangling P1",
    "section": "Done!",
    "text": "Done!\n\nCheck the ICA Instructions for how to (a) push your code to GitHub and (b) update your portfolio website",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6 Adv Data Wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P2.html",
    "href": "src/ica/Advanced_data_wrangling_P2.html",
    "title": "7 Adv Data wrangling P2",
    "section": "",
    "text": "🧩 Learning Goals\nBy the end of this lesson, you should be able to:",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Adv Data wrangling P2</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P2.html#learning-goals",
    "href": "src/ica/Advanced_data_wrangling_P2.html#learning-goals",
    "title": "7 Adv Data wrangling P2",
    "section": "",
    "text": "Manipulate and explore strings using the stringr package\nConstruct regular expressions to find patterns in strings",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Adv Data wrangling P2</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P2.html#helpful-cheatsheets",
    "href": "src/ica/Advanced_data_wrangling_P2.html#helpful-cheatsheets",
    "title": "7 Adv Data wrangling P2",
    "section": "Helpful Cheatsheets",
    "text": "Helpful Cheatsheets\nThe stringr cheatsheet (HTML, PDF) will be useful to have open and reference.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Adv Data wrangling P2</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P2.html#motivation-30-years-of-american-anxieties",
    "href": "src/ica/Advanced_data_wrangling_P2.html#motivation-30-years-of-american-anxieties",
    "title": "7 Adv Data wrangling P2",
    "section": "Motivation: 30 Years of American Anxieties",
    "text": "Motivation: 30 Years of American Anxieties\nIn 2018 the data journalism organization The Pudding featured a story called 30 Years of American Anxieties about themes in 30 years of posts to the Dear Abby column (an American advice column).\nOne way to understand themes in text data is to conduct a qualitative analysis, a methodology in which multiple readers read through instances of text several times to reach a consensus about themes.\nAnother way to understand themes in text data is computational text analysis.\n\nThis is what we will explore today.\n\nBoth qualitative analysis and computational tools can be used in tandem. Often, using computational tools can help focus a close reading of select texts, which parallels the spirit of a qualitative analysis.\nTo prepare ourselves for a computational analysis, let’s learn about strings.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Adv Data wrangling P2</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P2.html#strings",
    "href": "src/ica/Advanced_data_wrangling_P2.html#strings",
    "title": "7 Adv Data wrangling P2",
    "section": "Strings",
    "text": "Strings\nStrings are objects of the character class (abbreviated as &lt;chr&gt; in tibbles).\nWhen you print out strings, they display with double quotes:\n\nCodesome_string &lt;- \"banana\"\nsome_string\n\n[1] \"banana\"\n\n\nWorking with strings generally will involve the use of regular expressions, a tool for finding patterns in strings.\nRegular expressions (regex, for short) look like the following:\n\"^the\" (Strings that start with \"the\")\n\"end$\" (Strings that end with \"end\")\nBefore getting to regular expressions, let’s go over some fundamentals about working with strings. The stringr package (available within tidyverse) is great for working with strings.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Adv Data wrangling P2</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P2.html#creating-strings",
    "href": "src/ica/Advanced_data_wrangling_P2.html#creating-strings",
    "title": "7 Adv Data wrangling P2",
    "section": "Creating strings",
    "text": "Creating strings\nCreating strings by hand is useful for testing out regular expressions.\nTo create a string, type any text in either double quotes (\") or single quotes '. Using double or single quotes doesn’t matter unless your string itself has single or double quotes.\n\nCodestring1 &lt;- \"This is a string\"\nstring2 &lt;- 'If I want to include a \"quote\" inside a string, I use single quotes'\nstring3 &lt;- c(string1, string2) # string / character vector (of greater than length 1)\n\n\nclass(string1)\n\n[1] \"character\"\n\nCodeclass(string2)\n\n[1] \"character\"\n\nCodeclass(string3)\n\n[1] \"character\"\n\nCodelength(string1)\n\n[1] 1\n\nCodelength(string2)\n\n[1] 1\n\nCodelength(string3)\n\n[1] 2\n\n\nWe can view these strings “naturally” (without the opening and closing quotes) with str_view():\n\nCodestr_view(string1)\n\n[1] │ This is a string\n\nCodestr_view(string2)\n\n[1] │ If I want to include a \"quote\" inside a string, I use single quotes\n\nCodestr_view(string3)\n\n[1] │ This is a string\n[2] │ If I want to include a \"quote\" inside a string, I use single quotes\n\n\nExercise: Create the string It's Thursday. What happens if you put the string inside single quotes? Double quotes?\n\nCode# Your code\nstring4 &lt;- \"It's Thursday\"\nstring5 &lt;- 'It\\'s Thursday'\n\nstr_view(string4)\n\n[1] │ It's Thursday\n\nCodestr_view(string5)\n\n[1] │ It's Thursday\n\n\nBecause \" and ' are special characters in the creation of strings, R offers another way to put them inside a string. We can escape these special characters by putting a \\ in front of them:\n\nCodestring1 &lt;- \"This is a string with \\\"double quotes\\\"\"\nstring2 &lt;- \"This is a string with \\'single quotes\\'\"\nstr_view(string1)\n\n[1] │ This is a string with \"double quotes\"\n\nCodestr_view(string2)\n\n[1] │ This is a string with 'single quotes'\n\n\nGiven that \\ is a special character, how can we put the \\ character in strings? We have to escape it with \\\\.\nExercise: Create the string C:\\Users. What happens when you don’t escape the \\?\n\nCode# Your code\nstring7 &lt;- \"(:\\\\Users\"\n\n\nIf you don’t put in the double it gets an error because it is trying to read s a special reference that needs a hex code.\nOther special characters include:\n\n\n\\t (Creates a tab)\n\n\\n (Creates a newline)\n\nBoth can be useful in plots to more neatly arrange text.\n\nCodestring1 &lt;- \"Record temp:\\t102\"\nstring2 &lt;- \"Record temp:\\n102\"\n\nstr_view(string1)\n\n[1] │ Record temp:{\\t}102\n\nCodestr_view(string2)\n\n[1] │ Record temp:\n    │ 102\n\n\nCan we get str_view() to show the tab instead of {\\t}? We can use the html argument to have the string displayed as if on a webpage:\n\nCodestr_view(string1, html = TRUE)\n\n\nOften we will want to create new strings within data frames. We can use str_c() or str_glue(), both of which are vectorized functions (meaning they take vectors as inputs and provide vectors as outputs - can be used within mutate()):\n\nWith str_c() the strings to be combined are all separate arguments separated by commas.\nWith str_glue() the desired string is written as a template with variable names inside curly braces {}.\n\n\nCodedf &lt;- tibble(\n    first_name = c(\"Arya\", \"Olenna\", \"Tyrion\", \"Melisandre\"),\n    last_name = c(\"Stark\", \"Tyrell\", \"Lannister\", NA)\n)\ndf\n\n# A tibble: 4 × 2\n  first_name last_name\n  &lt;chr&gt;      &lt;chr&gt;    \n1 Arya       Stark    \n2 Olenna     Tyrell   \n3 Tyrion     Lannister\n4 Melisandre &lt;NA&gt;     \n\nCodedf |&gt;\n    mutate(\n        full_name1 = str_c(first_name, \" \", last_name),\n        full_name2 = str_glue(\"{first_name} {last_name}\")\n    )\n\n# A tibble: 4 × 4\n  first_name last_name full_name1       full_name2      \n  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;            &lt;glue&gt;          \n1 Arya       Stark     Arya Stark       Arya Stark      \n2 Olenna     Tyrell    Olenna Tyrell    Olenna Tyrell   \n3 Tyrion     Lannister Tyrion Lannister Tyrion Lannister\n4 Melisandre &lt;NA&gt;      &lt;NA&gt;             Melisandre NA   \n\n\nExercise: In the following data frame, create a full date string in month-day-year format using both str_c() and str_glue().\n\nCodedf_dates &lt;- tibble(\n    year = c(2000, 2001, 2002),\n    month = c(\"Jan\", \"Feb\", \"Mar\"),\n    day = c(3, 4, 5)\n)\n\ndf_dates |&gt; \n  mutate(\n    full_date1 = str_c(month, \"-\", day, \"-\", year),\n    full_date2 = str_glue(\"{month}-{day}-{year}\")\n  )\n\n# A tibble: 3 × 5\n   year month   day full_date1 full_date2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;glue&gt;    \n1  2000 Jan       3 Jan-3-2000 Jan-3-2000\n2  2001 Feb       4 Feb-4-2001 Feb-4-2001\n3  2002 Mar       5 Mar-5-2002 Mar-5-2002",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Adv Data wrangling P2</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P2.html#extracting-information-from-strings",
    "href": "src/ica/Advanced_data_wrangling_P2.html#extracting-information-from-strings",
    "title": "7 Adv Data wrangling P2",
    "section": "Extracting information from strings",
    "text": "Extracting information from strings\nThe str_length() counts the number of characters in a string.\n\nCodecomments &lt;- tibble(\n    name = c(\"Alice\", \"Bob\"),\n    comment = c(\"The essay was well organized around the core message and had good transitions.\", \"Good job!\")\n)\n\ncomments |&gt;\n    mutate(\n        comment_length = str_length(comment)\n    )\n\n# A tibble: 2 × 3\n  name  comment                                                   comment_length\n  &lt;chr&gt; &lt;chr&gt;                                                              &lt;int&gt;\n1 Alice The essay was well organized around the core message and…             78\n2 Bob   Good job!                                                              9\n\n\nThe str_sub() function gets a substring of a string. The 2nd and 3rd arguments indicate the beginning and ending position to extract.\n\nNegative positions indicate the position from the end of the word. (e.g., -3 indicates “3rd letter from the end”)\nSpecifying a position that goes beyond the word won’t result in an error. str_sub() will just go as far as possible.\n\n\nCodex &lt;- c(\"Apple\", \"Banana\", \"Pear\")\n\nstr_sub(x, start = 1, end = 3)\n\n[1] \"App\" \"Ban\" \"Pea\"\n\nCodestr_sub(x, start = -3, end = -1)\n\n[1] \"ple\" \"ana\" \"ear\"\n\nCodestr_sub(x, start = 2, end = -1)\n\n[1] \"pple\"  \"anana\" \"ear\"  \n\nCodestr_sub(\"a\", start = 1, end = 15)\n\n[1] \"a\"\n\n\nExercise: Using str_sub(), create a new variable with only the middle letter of each word in the data frame below. (Challenge: How would you handle words with an even number of letters?)\n\nCodedf &lt;- tibble(\n    word_id = 1:3,\n    word = c(\"replace\", \"match\", \"pattern\")\n)\n\ndf |&gt; \n  mutate(\n    mid_letter = str_sub(word, start = ((str_length(word) + 1)/ 2), end = ((str_length(word) + 1) / 2))\n  )\n\n# A tibble: 3 × 3\n  word_id word    mid_letter\n    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     \n1       1 replace l         \n2       2 match   t         \n3       3 pattern t",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Adv Data wrangling P2</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P2.html#finding-patterns-in-strings-with-regular-expressions",
    "href": "src/ica/Advanced_data_wrangling_P2.html#finding-patterns-in-strings-with-regular-expressions",
    "title": "7 Adv Data wrangling P2",
    "section": "Finding patterns in strings with regular expressions",
    "text": "Finding patterns in strings with regular expressions\nSuppose that you’re exploring text data looking for places where people describe happiness. There are many ways to search. We could search for the word “happy” but that excludes “happiness” so we might search for “happi”.\nRegular expressions (regex) are a powerful language for describing patterns within strings.\n. . .\n\nCodedata(fruit)\ndata(words)\ndata(sentences)\n\n\nWe can use str_view() with the pattern argument to see what parts of a string match the regex supplied in the pattern argument. (Matches are enclosed in &lt;&gt;.)\n\nCodestr_view(fruit, \"berry\")\n\n [6] │ bil&lt;berry&gt;\n [7] │ black&lt;berry&gt;\n[10] │ blue&lt;berry&gt;\n[11] │ boysen&lt;berry&gt;\n[19] │ cloud&lt;berry&gt;\n[21] │ cran&lt;berry&gt;\n[29] │ elder&lt;berry&gt;\n[32] │ goji &lt;berry&gt;\n[33] │ goose&lt;berry&gt;\n[38] │ huckle&lt;berry&gt;\n[50] │ mul&lt;berry&gt;\n[70] │ rasp&lt;berry&gt;\n[73] │ salal &lt;berry&gt;\n[76] │ straw&lt;berry&gt;\n\n\nEssentials of forming a regex\n\nLetters and numbers in a regex are matched exactly and are called literal characters.\nMost punctuation characters, like ., +, *, [, ], and ?, have special meanings and are called metacharacters.\n\nQuantifiers come after a regex and control how many times a pattern can match:\n\n\n?: match the preceding pattern 0 or 1 times\n\n+: match the preceding pattern at least once\n\n*: match the preceding pattern at least 0 times (any number of times)\n\n\n\n. . .\nExercise: Before running the code below, predict what matches will be made. Run the code to check your guesses. Note that in all regex’s below the ?, +, * applies to the b only (not the a).\n\nCodestr_view(c(\"a\", \"ab\", \"abb\"), \"ab?\")\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab+\")\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab*\")\n\n\n\nWe can match any of a set of characters with [] (called a character class), e.g., [abcd] matches “a”, “b”, “c”, or “d”.\n\nWe can invert the match by starting with ^: [^abcd] matches anything except “a”, “b”, “c”, or “d”.\n\n\n\n\nCode# Match words that have vowel-x-vowel\nstr_view(words, \"[aeiou]x[aeiou]\")\n\n[284] │ &lt;exa&gt;ct\n[285] │ &lt;exa&gt;mple\n[288] │ &lt;exe&gt;rcise\n[289] │ &lt;exi&gt;st\n\nCode# Match words that have not_vowel-y-not_vowel\nstr_view(words, \"[^aeiou]y[^aeiou]\")\n\n[836] │ &lt;sys&gt;tem\n[901] │ &lt;typ&gt;e\n\n\nExercise Using the words data, find words that have two vowels in a row followed by an “m”.\n\nCode# Your code\nstr_view(words, \"[aeiou][aeiou]m\")\n\n[154] │ cl&lt;aim&gt;\n[714] │ r&lt;oom&gt;\n[735] │ s&lt;eem&gt;\n[844] │ t&lt;eam&gt;\n\n\n\nThe alternation operator | can be read just like the logical operator | (“OR”) to pick between one or more alternative patterns. e.g., apple|banana searches for “apple” or “banana”.\n\n\nCodestr_view(fruit, \"apple|melon|nut\")\n\n [1] │ &lt;apple&gt;\n[13] │ canary &lt;melon&gt;\n[20] │ coco&lt;nut&gt;\n[52] │ &lt;nut&gt;\n[62] │ pine&lt;apple&gt;\n[72] │ rock &lt;melon&gt;\n[80] │ water&lt;melon&gt;\n\n\nExercise: Using the fruit data, find fruits that have a repeated vowel (“aa”, “ee”, “ii”, “oo”, or “uu”.)\n\nCode# Your code\nstr_view(fruit, \"aa|ee|ii|oo|uu\")\n\n [9] │ bl&lt;oo&gt;d orange\n[33] │ g&lt;oo&gt;seberry\n[47] │ lych&lt;ee&gt;\n[66] │ purple mangost&lt;ee&gt;n\n\n\n\nThe ^ operator indicates the beginning of a string, and the $ operator indicates the end of a string. e.g., ^a matches strings that start with “a”, and a$ matches words that end with “a”.\nParentheses group together parts of a regular expression that should be taken as a bundle. (Much like parentheses in arithmetic statements.)\n\ne.g., ab+ is a little confusing. Does it match “ab” one or more times? Or does it match “a” first, then just “b” one or more times? (The latter, as we saw in an earlier example.) We can be very explicit and use a(b)+.\n\n\n\nExercise: Using the words data, find (1) words that start with “y” and (2) words that don’t start with “y”.\n\nCode# Your code\nstr_view(words, \"^y\")\n\n[975] │ &lt;y&gt;ear\n[976] │ &lt;y&gt;es\n[977] │ &lt;y&gt;esterday\n[978] │ &lt;y&gt;et\n[979] │ &lt;y&gt;ou\n[980] │ &lt;y&gt;oung\n\nCodestr_view(words, \"^(?!Y).*$\")\n\n [1] │ &lt;a&gt;\n [2] │ &lt;able&gt;\n [3] │ &lt;about&gt;\n [4] │ &lt;absolute&gt;\n [5] │ &lt;accept&gt;\n [6] │ &lt;account&gt;\n [7] │ &lt;achieve&gt;\n [8] │ &lt;across&gt;\n [9] │ &lt;act&gt;\n[10] │ &lt;active&gt;\n[11] │ &lt;actual&gt;\n[12] │ &lt;add&gt;\n[13] │ &lt;address&gt;\n[14] │ &lt;admit&gt;\n[15] │ &lt;advertise&gt;\n[16] │ &lt;affect&gt;\n[17] │ &lt;afford&gt;\n[18] │ &lt;after&gt;\n[19] │ &lt;afternoon&gt;\n[20] │ &lt;again&gt;\n... and 960 more",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Adv Data wrangling P2</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P2.html#exploring-stringr-functions",
    "href": "src/ica/Advanced_data_wrangling_P2.html#exploring-stringr-functions",
    "title": "7 Adv Data wrangling P2",
    "section": "Exploring stringr functions",
    "text": "Exploring stringr functions\nRead in the “Dear Abby” data underlying The Pudding’s 30 Years of American Anxieties article.\n\nCodeposts &lt;- read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/dearabby/raw_da_qs.csv\")\n\n\nTake a couple minutes to scroll through the 30 Years of American Anxieties article to get ideas for themes that you might want to search for using regular expressions.\n\nThe following are core stringr functions that use regular expressions:\n\n\nstr_view() - View the first occurrence in a string that matches the regex\n\nstr_count() - Count the number of times a regex matches within a string\n\nstr_detect() - Determine if (TRUE/FALSE) the regex is found within string\n\nstr_subset() - Return subset of strings that match the regex\n\nstr_extract(), str_extract_all() - Return portion of each string that matches the regex. str_extract() extracts the first instance of the match. str_extract_all() extracts all matches.\n\nstr_replace(), str_replace_all() - Replace portion of string that matches the regex with something else. str_replace() replaces the first instance of the match. str_replace_all() replaces all instances of the match.\n\nstr_remove(), str_remove_all() - Removes the portion of the string that matches the pattern. Equivalent to str_replace(x, \"THE REGEX PATTERN\", \"\")\n\n\nExercise: Starting from str_count(), explore each of these functions by pulling up the function documentation page and reading through the arguments. Try out each function using the posts data.\n\nCode#turned these into comments bc website was taking foever to publish\n#str_count(posts, \"sex\")\n#str_detect(posts, \"taboo\")\n#str_subset(posts, \"strange\")\n#str_extract(posts, \"dating\")\n#str_replace(posts, \"beautiful\", \"stunning\")\n#str_remove(posts, \"anxiety\")",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Adv Data wrangling P2</span>"
    ]
  },
  {
    "objectID": "src/ica/Advanced_data_wrangling_P2.html#done",
    "href": "src/ica/Advanced_data_wrangling_P2.html#done",
    "title": "7 Adv Data wrangling P2",
    "section": "Done!",
    "text": "Done!\n\nCheck the ICA Instructions for how to (a) push your code to GitHub and (b) update your portfolio website",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Adv Data wrangling P2</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html",
    "href": "src/ica/08-missing-data-notes.html",
    "title": "8 Missing Data",
    "section": "",
    "text": "🧩 Learning Goals\nBy the end of this lesson, you should be able to:",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#learning-goals",
    "href": "src/ica/08-missing-data-notes.html#learning-goals",
    "title": "8 Missing Data",
    "section": "",
    "text": "Go through a data quality checklist when data wrangling\nExplain the difference between MCAR, MAR, and MNAR missing data mechanisms\nAssess what missing data mechanisms might be at play in a given dataset\nUse visualizations to explore missing data patterns\nExplain why multiple imputation is preferred to single imputation\nExplain how a simulation study can be used to investigate properties of statistical methods",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#data-quality-checklist",
    "href": "src/ica/08-missing-data-notes.html#data-quality-checklist",
    "title": "8 Missing Data",
    "section": "Data Quality Checklist",
    "text": "Data Quality Checklist\nWhen wrangling / cleaning data, make sure to check the assumptions you make about the data to ensure you don’t lose data quality.\n\nData Parsing (reading data into a different data format)\n\nAlways keep the original, raw data (don’t manually change it).\nUse Test Cases: Find rows or write test cases to double check the wrangling works as expected\nDATES: When using lubridate to parse dates and times, ensure the strings are consistently ordered and formatted correctly, eg, mm/dd/yy vs. dd/mm/yy.\nSTRINGS: When using stringr to parse strings with regular expressions, check example rows to ensure that the pattern captured all of the examples you want and excluded the patterns you don’t want.\nAlways check for missing values to see if the missing ones are expected given the original data.\n\n\nData Joining\n\n\nIdentify missing values in the key variables and decide how to handle them before the merge process (e.g., omitting rows with missing values, imputing missing values).\nDecide on the correct join type (left, right, inner, full, etc.) OR if the data structure is the same use list_rbind() to bind rows or list_cbind() to bind columns.\nIf doing a join, make sure that the key variables (by) have the same meaning in both datasets and are represented in the same way (e.g., id = 1 to 20 in first dataset will match id = 1 - 20 in undesirable ways)\nPredict the number of rows that will result from the join and double check the anti_join() to see which rows did not find a match.\nCheck for duplicate records within each dataset and ensure they are handled appropriately before merging.\nVerify that the merged dataset maintains consistency with the original datasets in terms of data values, variable names, and variable types.\nPerform some preliminary analysis or validation checks on the merged dataset to ensure that it meets the requirements of your analysis.\n\n\nSanity Check: Visualize your data!!!\n\n\nDo the right number of points appear?\nDo the values seem reasonable?",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#explicit-v.-implicit-missing-data",
    "href": "src/ica/08-missing-data-notes.html#explicit-v.-implicit-missing-data",
    "title": "8 Missing Data",
    "section": "Explicit v. Implicit Missing Data",
    "text": "Explicit v. Implicit Missing Data\nExplicit missing data is data that is explicitly marked as missing. In R, this is done with NA.\nImplicit missing data is data that is missing but not explicitly marked as such.\n\nThis can happen when an entire row is missing from a dataset.\n\nFor example, if a study participant doesn’t attend a follow-up visit. It maybe not even be recorded in the data.\n\n\n\nWe need to make implicit missingness explicit before we can work with the data.\n\nConsider the combinations of variables that you’d expect to be fully present in a complete dataset.\n\nIf a combination is missing, you can create a new row with explicit missing values.\n\nFor example, if you expect every participant (each has a unique pid) to have an observation for each visit, you could use the function complete() to create that new row and plug in values of NA for the missing data.\n\n\n\n\n\nstudy_data_full &lt;- study_data |&gt; \n  complete(pid, visit)",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#dealing-with-missing-data",
    "href": "src/ica/08-missing-data-notes.html#dealing-with-missing-data",
    "title": "8 Missing Data",
    "section": "Dealing with missing data",
    "text": "Dealing with missing data\nIf you have explicit missing data, there are 3 main ways of proceeding:\n\n\nDrop the cases or rows with any missing data from the analysis–a complete case analysis\n\nPro: Easy to implement\nCon: reduces sample size, introduces bias if the missing data is not “missing completely at random”\n\n\nCreate a category for missing values.\n\nExample: The survey question “What is your gender?” might only provide two possible responses: “male” and “female”. Missing values for this could indicate that the respondent uses a non-binary designation. Instead of dropping these cases, treating the missing data as its own category (“Does not wish to answer”) would be more appropriate.\nPros: Maintains sample size, may help us if data is “missing not at random”\nCons: Not directly applicable to continuous outcomes (could include interactions with a categorical version in models to account for it)\n\n\n\nImpute (or fill in values for) the missing data using imputation algorithms.\n\nImputation algorithms can be as simple as replacing missing values with the mean of the non-missing values (very simplistic).\nRegression imputation algorithms use models to predict missing values as a function of other variables in the data.\nPros: Maintains sample size, multiple regression imputation minimizes bias if “missing at random”\nCons: Computationally intensive\n\n\n\n\nDeciding between these options and proceeding with choosing finer details within an option requires an understanding of the mechanism by which data become missing.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#missing-data-mechanisms",
    "href": "src/ica/08-missing-data-notes.html#missing-data-mechanisms",
    "title": "8 Missing Data",
    "section": "Missing Data Mechanisms",
    "text": "Missing Data Mechanisms\nThe reasons for which a variable might have missing data are divided into 3 mechanisms: MCAR, MAR, and MNAR.\nWithin a dataset, multiple mechanisms may be present–we need to consider the missingness mechanism for each variable individually.\n\n\n\nMissing completely at random (MCAR):\n\nThe probability of missing data for a variable is the same for all cases. Implies that causes of the missing data are unrelated to the data. (https://stefvanbuuren.name/fimd/sec-MCAR.html)\nExamples:\n\nMeasurement device that runs out of batteries causes the remainder of observations for the day to be missing.\nData entry software requires a particular field to be typo-free, and missing values are introduced when there are typos.\n\n\nImplications for downstream work:\n\nIf a variable has MCAR missingness, a complete case analysis will be unbiased (still valid).\nHowever, with a lot of missing observations, a complete case analysis will suffer from loss of statistical power (ability to detect a real difference), and imputation will be useful to retain the original sample size.\n\n\n\n\n\nMissing at random (MAR):\n\nThe probability of missing data is related to observed variables but unrelated to unobserved information.\nExamples:\n\nBlood pressure measurements tend to be missing in patients in worse health. (Those in worse health are more likely to miss clinic visits.) Better and worse health can be measured by a variety of indicators in their health record.\nIn a survey, older people are more likely to report their income than younger people. Missingness is related to the observed age variable, but not to unobserved information.\n\n\nImplications for downstream work:\n\nTry to use imputation methods that predict the value of the missing variables from other observed variables. Assessing whether this can be done accurately takes some exploration–we’ll explore this shortly.\n\n\n\n\n\nMissing not at random (MNAR):\n\nThe probability of missing data is related to unobserved variables (and probably observed variables too).\nExamples:\n\nBlood pressure measurements are more likely to be missing for those with the highest blood pressure. This is MNAR rather than MAR because the missing data on blood pressure is related to the unobserved values themselves.\nHigh-income individuals may be less likely to report their income.\n\n\nImplications for downstream work:\n\nIdeally, we would learn more about the causes for the missingness. This could allow us to use more informed imputation models.\n\nExample: Biological measurements that tend to be missing because of concentrations that are too low (a phenomenon known as left-censoring). Imputation methods specifically suited to left-censoring are useful here.\n\n\nWe can use imputation methods with different assumptions about the missing data and try out a variety of assumptions. This lets us see how sensitive the results are under various scenarios.\n\nExample: If higher incomes are more likely to be missing, we can make different assumptions about what “high” could be to fill in the missing values and see how our results change under these different assumptions.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#exercise",
    "href": "src/ica/08-missing-data-notes.html#exercise",
    "title": "8 Missing Data",
    "section": "Exercise",
    "text": "Exercise\n\n\nMissing data mechanism For each of the following situations, propose what missing data mechanism you think is most likely at play.\n\n\nIn a clinical trial, some patients dropped out before the end of the study. Their reasons for dropping out were not recorded. In this case it is most likely MNAR because the reason for dropping out and the missing data is due to a variable that is not recorded or observed, their reason for dropping out.\nA weather station records temperature, humidity, and wind speed every hour. Some of the recorded values are missing. This is missing completely at random or MCAR because there is no recorded variable that can point to the missing variables they simply are missing.\nA social media platform collects data on user interactions, such as likes, comments, and shares. Some interactions are not recorded due to bugs in the code. This is missing at random or MAR because it is connected to a recorded event or variable bugs in the code, and not pinpointed on something not recorded.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#exploring-missing-data",
    "href": "src/ica/08-missing-data-notes.html#exploring-missing-data",
    "title": "8 Missing Data",
    "section": "Exploring Missing Data",
    "text": "Exploring Missing Data\nGuiding question: How can we use visualizations and tabulations to explore what missing data mechanisms may be at play?\nWe’ll look at the airquality dataset available in base R, which gives daily air quality measurements in New York from May to September 1973. You can pull up the codebook with ?airquality in the Console.\n\nCodedata(airquality)",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#missingness-by-variable",
    "href": "src/ica/08-missing-data-notes.html#missingness-by-variable",
    "title": "8 Missing Data",
    "section": "Missingness by Variable",
    "text": "Missingness by Variable\nWe can explore how much missingness there is for each variable with the following functions:\n\nCodesummary(airquality) # Summary statistics in addition to number of NA's\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\nCodenaniar::vis_miss(airquality) # Where are NA's located?\n\n\n\n\n\n\nCodenaniar::miss_var_summary(airquality) # Information from vis_miss() in table form\n\n# A tibble: 6 × 3\n  variable n_miss pct_miss\n  &lt;chr&gt;     &lt;int&gt;    &lt;num&gt;\n1 Ozone        37    24.2 \n2 Solar.R       7     4.58\n3 Wind          0     0   \n4 Temp          0     0   \n5 Month         0     0   \n6 Day           0     0",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#missingness-by-case",
    "href": "src/ica/08-missing-data-notes.html#missingness-by-case",
    "title": "8 Missing Data",
    "section": "Missingness by Case",
    "text": "Missingness by Case\nWe can explore how much missingness there is for each case with naniar::miss_case_summary(). For each case, this function calculates the number and percentage of variables with a missing value.\nImpact of Information: If the pct_miss column is large for a case, we likely won’t be able to impute any of its missing values because there just isn’t enough known information–this case will have to be dropped from the analysis.\n\nCodenaniar::miss_case_summary(airquality)\n\n# A tibble: 153 × 3\n    case n_miss pct_miss\n   &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n 1     5      2     33.3\n 2    27      2     33.3\n 3     6      1     16.7\n 4    10      1     16.7\n 5    11      1     16.7\n 6    25      1     16.7\n 7    26      1     16.7\n 8    32      1     16.7\n 9    33      1     16.7\n10    34      1     16.7\n# ℹ 143 more rows",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#exploring-missingness-mechanisms",
    "href": "src/ica/08-missing-data-notes.html#exploring-missingness-mechanisms",
    "title": "8 Missing Data",
    "section": "Exploring Missingness Mechanisms",
    "text": "Exploring Missingness Mechanisms\nAssessing missingness mechanisms involves checking if missingness in a variable is related to other variables.\nNote: Through our available data, we are really only able to explore the potential for MCAR or MAR mechanisms.\nImpact of Information: There is always the chance that unobserved information (unobserved other variables or unobserved values of the variables we do have) is related to missingness for our variables, so to think through the potential for MNAR, more contextual information is necessary.\nTo explore these relationships, we can create TRUE/FALSE indicators of whether a variable is missing. In the plots below, we use is.na(Ozone) to explore whether cases with missing ozone values are noticeably different from cases with observed ozone values in terms of Solar.R.\n\nCodeggplot(airquality, aes(x = is.na(Ozone), y = Solar.R)) + \n    geom_boxplot()\n\n\n\n\n\n\nCodeggplot(airquality, aes(x = Solar.R, color = is.na(Ozone))) + \n    geom_density()\n\n\n\n\n\n\n\nThe above boxplots and density plots suggest that missing ozone is not strongly related to solar radiation levels.\nWe still should check if ozone missingness is related to the Wind, Temp, Month, and Day variables (to be done in Exercises).\nIn addition to checking if the chance of ozone missingness is related to Solar.R, we should check if the values of ozone could be predicted by Solar.R.\nIn the scaterrplot below, we look at the relationship between Ozone and Solar.R and use vertical lines to indicate the Solar.R values for cases that are missing Ozone.\n\n\nImpact of Information: We see that missing Ozone cases are within the observed span of Solar.R, so we would be ok with predicting Ozone from Solar.R because there would be no extrapolation.\n\n\nCodeggplot(airquality, aes(x = Solar.R, y = Ozone)) +\n    geom_point() +\n    geom_smooth() +\n    geom_vline(data = airquality |&gt; filter(is.na(Ozone)), mapping = aes(xintercept = Solar.R))",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#exercises",
    "href": "src/ica/08-missing-data-notes.html#exercises",
    "title": "8 Missing Data",
    "section": "Exercises",
    "text": "Exercises\n\n\nMechanism detection practice Look at the boxplot + scatterplot pairs for Alternate Situations 1 and 2 below. How do these situations compare to our actual situation and to each other? What concerns might arise from using a model to impute Ozone?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn these alternative situations there is no overlap when it comes to the box plots that are looking at where there are NA values and their relationship to solar radiation. Furthermore when you look at the first line graph you can see that the NA lines all cluster near the top but are in the range, where as in the second scenario they fall outside of the observed range. In our actual situation we could conclude there was no correlation between NA values and Solar Radiation, but in these scenarios there clearly is. If we tried to use a model to represent ozone then we could be misrepresenting the relationships and extrapolating the data.\n\n\nOzone mechanism detection Continue the investigation of missingness for Ozone.\n\nWe want to see how Month, Wind, and Temp relate to the chance of missingness for Ozone and to the value of Ozone.\nDoes it look like a linear regression model (perhaps with variable transformations) could be effective in imputing the missing ozone data?\n\nCodeggplot(airquality, aes(fill = is.na(Ozone), x = factor(Month))) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\nCodeggplot(airquality, aes(x = is.na(Ozone), y = Wind)) + \n  geom_boxplot()\n\n\n\n\n\n\nCodeggplot(airquality, aes(x = Wind, y = Ozone)) +\n    geom_point() +\n    geom_smooth() +\n    geom_vline(data = airquality |&gt; filter(is.na(Ozone)), mapping = aes(xintercept = Wind))\n\n\n\n\n\n\nCodeggplot(airquality, aes(x = is.na(Ozone), y = Temp)) + \n  geom_boxplot()\n\n\n\n\n\n\nCodeggplot(airquality, aes(x = Temp, y = Ozone)) +\n    geom_point() +\n    geom_smooth() +\n    geom_vline(data = airquality |&gt; filter(is.na(Ozone)), mapping = aes(xintercept = Temp))\n\n\n\n\n\n\n\nWind and Temp don’t really have a strong relationship with missing Ozone values, only one NA case falls outside of the observed range of Wind and Temp. However, month does have a relationship and June has over 50% missing data values, while the others all rest beneath 25% per month. This is most likely a MAR type of variable, where it can be correlated with an observed factor, month. It does look like a linear regression model could be effective in representing the missing ozone data as the wind and temp factors will not strongly extrapolate the data, although data in the month of June might be too similar.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#regression-imputation",
    "href": "src/ica/08-missing-data-notes.html#regression-imputation",
    "title": "8 Missing Data",
    "section": "Regression Imputation",
    "text": "Regression Imputation\nWhen a model is built and used to generate a single set of predictions for missing values, this is known as single imputation.\n\nWhen using singly imputed data in subsequent modeling, the uncertainty in estimates tends to be underestimated. This means that:\n\nStandard errors are lower than they should be.\nConfidence intervals won’t contain the true parameter value the “advertised” percentage of times\n\ne.g., 95% confidence intervals will not contain the truth in 95% of samples–the coverage probability will be less than 95%\n\n\n\n\n\nIn multiple imputation, multiple imputed datasets are generated with different values for the filled-in data.\n\nSubsequent models are fit on each of these datasets, and both estimates and uncertainty measures are pooled across all of these fits.\nMultiple imputation more accurately estimates uncertainty measures.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#simulation-studies",
    "href": "src/ica/08-missing-data-notes.html#simulation-studies",
    "title": "8 Missing Data",
    "section": "Simulation Studies",
    "text": "Simulation Studies\nWe can use a simulation study to investigate the statistical properties described above.\n\n\nGenerate (simulate) data where we are in control of missing data mechanisms and the true relationship between an outcome and the predictors.\nOn that simulated data, use single imputation to fill in the missing values. Fit the desired model, and obtain a confidence interval for a coefficient of interest.\nOn that simulated data, use multiple imputation to fill in the missing values. Fit the desired models on all imputed datasets, pool results, and obtain a confidence interval for a coefficient of interest.\nSteps 1 - 3 are repeated a lot of times (num_simulations &lt;- 1000) to see how things work out in lots of different samples.\nSummarize the performance of single and multiple imputation across the num_simulations simulations.\n\n\nWe will slowly step through the simulation study code below. We will pause frequently for you to add comments documenting what is happening.\n\nCodeset.seed(224)\nnum_simulations &lt;- 1000\nci_list &lt;- vector(\"list\", length = num_simulations)\n\nsystem.time({\nfor (i in 1:num_simulations) {\n    # Simulate data\n    n &lt;- 1000\n    sim_data &lt;- tibble(\n        x1 = runif(n, min = 0, max = 1),\n        x2 = x1 + rnorm(n, mean = 0, sd = 1),\n        x2_miss_bool = rbinom(n, size = 1, prob = x1/2),\n        x2_NA = if_else(x2_miss_bool == 1, NA, x2),\n        y = x1 + x2 + rnorm(n, mean = 0, sd = 1)\n    )\n    \n    # Single imputation ---------------\n    mice_obj &lt;- mice(sim_data |&gt; select(x1, x2_NA, y), m = 1, method = \"norm\", printFlag = FALSE)\n    si_mod &lt;- with(mice_obj, lm(y ~ x1 + x2_NA))\n    ci_single &lt;- si_mod$analyses[[1]] |&gt; confint(level = 0.95)\n    ci_single &lt;- ci_single[\"x2_NA\",]\n    \n    # Multiple imputation -------------\n    mice_obj &lt;- mice(sim_data |&gt; select(x1, x2_NA, y), m = 10, method = \"norm\", printFlag = FALSE)\n    mi_mods &lt;- with(mice_obj, lm(y ~ x1 + x2_NA))\n    pooled_res &lt;- pool(mi_mods)\n    summ_pooled_res &lt;- summary(pooled_res, conf.int = TRUE, conf.level = 0.95)\n    ci_multiple_lower &lt;- summ_pooled_res |&gt; filter(term==\"x2_NA\") |&gt; pull(`2.5 %`)\n    ci_multiple_upper &lt;- summ_pooled_res |&gt; filter(term==\"x2_NA\") |&gt; pull(`97.5 %`)\n    \n    # Store CI information\n    ci_list[[i]] &lt;- tibble(\n        ci_lower = c(\n            ci_single[1],\n            ci_multiple_lower\n        ),\n        ci_upper = c(\n            ci_single[2],\n            ci_multiple_upper\n        ),\n        which_imp = c(\"single\", \"multiple\")\n    )\n}\n})\n\n   user  system elapsed \n108.368   1.239 109.937 \n\n\nBelow we compute the confidence interval (CI) coverage probability (fraction of times the CI contains the true value of 1) for the CIs generated from single and multiple imputation:\n\nCodeci_data &lt;- bind_rows(ci_list)\nci_data |&gt; \n    mutate(contains_truth = ci_lower &lt; 1 & ci_upper &gt; 1) |&gt; \n    group_by(which_imp) |&gt; \n    summarize(frac_contains_truth = mean(contains_truth))\n\n# A tibble: 2 × 2\n  which_imp frac_contains_truth\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 multiple                0.947\n2 single                  0.902",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#done",
    "href": "src/ica/08-missing-data-notes.html#done",
    "title": "8 Missing Data",
    "section": "Done!",
    "text": "Done!\n\nCheck the ICA Instructions for how to (a) push your code to GitHub and (b) update your portfolio website",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/09-functions-notes.html",
    "href": "src/ica/09-functions-notes.html",
    "title": "9 Functions",
    "section": "",
    "text": "🧩 Learning Goals\nBy the end of this lesson, you should be able to:",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9 Functions</span>"
    ]
  },
  {
    "objectID": "src/ica/09-functions-notes.html#learning-goals",
    "href": "src/ica/09-functions-notes.html#learning-goals",
    "title": "9 Functions",
    "section": "",
    "text": "Recognize when it would be useful to write a function\nIdentify the core components of a function definition and explain their role (the function() directive, arguments, argument defaults, function body, return value)\nIdentify the scoping of a function in accessing objects in R\nDescribe the difference between argument matching by position and by name\nWrite if-else, if-else if-else statements to conditionally execute code\nWrite your own function to carry out a repeated task\nProvide feedback on functions written by others",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9 Functions</span>"
    ]
  },
  {
    "objectID": "src/ica/09-functions-notes.html#functions-and-control-structures",
    "href": "src/ica/09-functions-notes.html#functions-and-control-structures",
    "title": "9 Functions",
    "section": "Functions and Control Structures",
    "text": "Functions and Control Structures\nWhy functions?\nGetting really good at writing useful and reusable functions is one of the best ways to increase your expertise in data science. It requires a lot of practice.\nIf you’ve copied and pasted code 3 or more times, it’s time to write a function. Try to avoid repeating yourself.\n\n\n\nReducing errors: Copy + paste + modify is prone to errors (e.g., forgetting to change a variable name)\n\nEfficiency: If you need to update code, you only need to do it one place. This allows reuse of code within and across projects.\n\nReadability: Encapsulating code within a function with a descriptive name makes code more readable.\n\n\nCore parts of a function\nWhen you define a function, what does it look like?\n\nCode# Defining a function\nfunction_name &lt;- function(input_name_1, input_name_2 = default_value_2){\n\n  # function body - code that does something\n\n  return(output)\n}\n\n# Calling a function (all valid ways)\nfunction_name(input_name_1 = 2, input_name_2 = 4)\nfunction_name(2, 4)\nfunction_name(input_name_2 = 4, input_name_1 = 2)\nfunction_name(2)\n\n\nThe core parts of defining a function include:\n\n\nfunction() directive\n\nThis is what allows tells R to create a function.\n\n\n\nArguments/Inputs: the input_name_1 and input_name_2 – these are names of the function inputs\n\n\ndefault_value_2 is a default value that is used if no input values are provided when the function is called\n\n\n\nFunction body\n\nThe code inside the curly braces { } is where all the work happens. This code uses the function arguments to perform computations.\n\n\n\nReturn value\n\nWe can explicitly return an object by putting it inside return().\n\n\n\nHere are three examples that can help us learn a few properties of functions:\n\nCodeaverage &lt;- function(x, remove_nas) {\n    return(sum(x, na.rm = remove_nas)/length(x))\n}\naverage2 &lt;- function(x, remove_nas = TRUE) {\n    return(sum(x, na.rm = remove_nas)/length(x))\n}\n\naverage3 &lt;- function(x, remove_nas) {\n    sum(x, na.rm = remove_nas)/length(x)\n}\n\n\nNote:\n\nIn average2, the remove_nas argument has a default value of TRUE.\n\nWhen a function has default values for arguments, they don’t have to be provided when you call the function if you want to use the default value. See below:\n\nCode# Does throw an error\naverage(c(1, 2, 3, NA))\n\nError in average(c(1, 2, 3, NA)): argument \"remove_nas\" is missing, with no default\n\nCodeaverage(c(1, 2, 3, NA), remove_nas = TRUE)\n\n[1] 1.5\n\nCode# Doesn't throw an error\naverage2(c(1, 2, 3, NA))\n\n[1] 1.5\n\nCodeaverage2(c(1, 2, 3, NA), remove_nas = FALSE)\n\n[1] NA\n\n\n\nIf we don’t provide a return(), the last value that gets evaluated in the function body and isn’t stored as an object is what the function returns. (This is generally the last line without an assignment operator &lt;-.)\n\n\naverage3() is one example, but this can easily lead to errors.\nWe should explicitly return an object by putting it inside return().",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9 Functions</span>"
    ]
  },
  {
    "objectID": "src/ica/09-functions-notes.html#scope",
    "href": "src/ica/09-functions-notes.html#scope",
    "title": "9 Functions",
    "section": "Scope",
    "text": "Scope\nIn programming, scope refers to the area of a program where a named object is recognized and can be used.\n\nR uses lexical scoping, which means that the scope of a variable is determined by where it is created in the code.\n\nLexical scoping in R:\n\nWhen code refers to an object (e.g. data set, function, vector, etc.), R looks for the object in the current, local environment.\nIf it doesn’t find it, it continues to search by looking in the parent environment, and so on. The top-level environment is the global environment, the location where all interactive (i.e. outside of a function) computation and storage takes place.\n\nIf you save an object in R, outside a function local environment, it is stored in the global environment.\n\n\nIf R can’t find it in the global environment, it will look for it in loaded packages.\n\nWhen writing and using functions, the local environment is within the function itself.\n\nIf you define an object in the function, it won’t be accessible outside that function (unless you pass it as the output). See below:\n\n\nCodeaverage_new &lt;- function(x, remove_nas = TRUE) {\n    sum(x, na.rm = remove_nas)/length(x)\n  fun_new_thing_within_function &lt;- \"Fun times!\"\n}\naverage_new(1:3)\nfun_new_thing_within_function\n\nError: object 'fun_new_thing_within_function' not found\n\n\nScoping is important to consider if you try to refer to objects that aren’t passed as arguments.\n\nThis can be dangerous because if you re-use names of variables, it is easy to accidentally refer to a variable that is not the one you intended. See below:\n\n\nCodev &lt;- c(1,2,3)\n\naverage_new2 &lt;- function(x, remove_nas = TRUE) {\n    sum(x, na.rm = remove_nas)/length(x) + v\n}\n\n\naverage_new2(1:3)\n\n[1] 3 4 5\n\nCodev &lt;- c(4,5,6)\naverage_new2(1:3)\n\n[1] 6 7 8\n\n\nAnything created/saved/updated within a function that you want accessible outside that function needs to be passed to return(). See below:\n\nCodev &lt;- c(1,2,3)\n\naverage_new3 &lt;- function(x, remove_nas = TRUE) {\n  v &lt;- v + 10 # this is not changing v outside the function... \n  return(sum(x, na.rm = remove_nas)/length(x) )\n}\n\naverage_new3(1:3)\n\n[1] 2\n\nCodev\n\n[1] 1 2 3\n\n\nFor more about lexical scoping in R, see R Programming for Data Science",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9 Functions</span>"
    ]
  },
  {
    "objectID": "src/ica/09-functions-notes.html#writing-functions",
    "href": "src/ica/09-functions-notes.html#writing-functions",
    "title": "9 Functions",
    "section": "Writing Functions",
    "text": "Writing Functions\nTips for writing functions:\n\nWrite the code for the body of the function (the task that you are wanting to repeat)\nIdentify the parts of the function that could/would change (these are the inputs)\n\nUpdate the body of the function to be in terms of the inputs\nAvoid manually typing anything that is specific to one input; use code to get that (such as unique values of a variable)\n\n\nIdentify the output you want to return\n\nPair programming exercises: There are 2 exercises for each section below.\n\nRead through the introduction of the concept and then work on the exercises together.\n\nYou’ll swap driver and navigator roles between exercises.\nRemember: The driver writes the code. The navigator oversees and provides guidance.\nFor the first exercise, the person whose birthday is coming up sooner will be the driver first. Swap role for the second exercise and continue in this manner for all exercises.\n\n\n\nRescaling function Write a function that rescales a numeric vector to be between 0 and 1. Here are some test cases with the expected output. Test out your function on the following inputs:\n\n\n\nx = 2:4. Expected output: 0.0 0.5 1.0\n\n\nx = c(-1, 0, 5). Expected output: 0.0000000 0.1666667 1.0000000\n\n\nx = -3:-1. Expected output: 0.0 0.5 1.0\n\n\n\nCoderescale2 &lt;- function(x) {\n  min_x = min(x)\n  return((x-min_x)/(max(x)- min_x))\n}\n\nrescale2(x = 2:4)\n\n[1] 0.0 0.5 1.0\n\nCoderescale2(x = c(-1, 0, 5))\n\n[1] 0.0000000 0.1666667 1.0000000\n\nCoderescale2(x = -3:-1)\n\n[1] 0.0 0.5 1.0\n\n\n\n\nPhone digits Write a function that formats a 10-digit phone number nicely as (###) ###-####. Your function should work on the following test cases: c(\"651-330-8661\", \"6516966000\", \"800 867 5309\"). It may help to refer to the stringr cheatsheet.\n\n\nCodephone_form &lt;- function(num) {\n  num &lt;- str_remove_all(num, \"[^[:alnum:]]\")\n  first_3 &lt;- str_sub(num, 1, 3)\n  sec_3 &lt;- str_sub(num, 4, 6)\n  thr_4 &lt;- str_sub(num, 7, 10)\n  return(str_glue(\"({first_3}) {sec_3}-{thr_4}\"))\n}\n\nphone_form(c(\"651-330-8661\", \"6516966000\", \"800 867 5309\"))\n\n(651) 330-8661\n(651) 696-6000\n(800) 867-5309",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9 Functions</span>"
    ]
  },
  {
    "objectID": "src/ica/09-functions-notes.html#calling-a-function",
    "href": "src/ica/09-functions-notes.html#calling-a-function",
    "title": "9 Functions",
    "section": "Calling a Function",
    "text": "Calling a Function\nWhen you supply arguments to a function, they can be matched by position and/or by name.\nWhen you call a function without argument = value inside the parentheses, you are using positional matching.\n\nCodeggplot(diamonds, aes(x = carat, y = price)) + geom_point()\n\n\nThe above works because the first argument of ggplot is data and the second is mapping. (Pull up the documentation on ggplot with ?ggplot in the Console.) So the following doesn’t work:\n\nCodeggplot(aes(x = carat, y = price), diamonds) + geom_point()\n\nError in `ggplot()`:\n! `mapping` must be created by `aes()`.\n✖ You've supplied a tibble.\n\n\nBut if we named the arguments (name matching), we would be fine:\n\nCodeggplot(mapping = aes(x = carat, y = price), data = diamonds) + geom_point()\n\n\nSomewhat confusingly, we can name some arguments and not others. Below, mapping is named, but data isn’t. This works because when an argument is matched by name, it is “removed” from the argument list, and the remaining unnamed arguments are matched in the order that they are listed in the function definition. Just because this is possible doesn’t mean it’s a good idea–don’t do this!\n\nCodeggplot(mapping = aes(x = carat, y = price), diamonds) + geom_point()\n\n\n\n\n\n\n\n\nArgument matching\n\n\n\nIn general, it is safest to match arguments by name and position for your peace of mind. For functions that you are very familiar with (and know the argument order), it’s ok to just use positional matching.\n\n\n\n\nError Messages Diagnose the error message in the example below:\n\nggplot() |&gt;\n    geom_sf(census_data, aes(fill = population))\n    \nError in `layer_sf()`:\n! `mapping` must be created by `aes()`\nThe error message is that the positioning in geom_sf if wrong becuase it takes the mapping before it takes data, so right now it thinks the data is the mapping argument. In order to fix this either switch their locations or add the explict name to each of these.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9 Functions</span>"
    ]
  },
  {
    "objectID": "src/ica/09-functions-notes.html#the-if-else-if-else-control-structure",
    "href": "src/ica/09-functions-notes.html#the-if-else-if-else-control-structure",
    "title": "9 Functions",
    "section": "The if-else if-else Control Structure",
    "text": "The if-else if-else Control Structure\nOften in functions, you will want to execute code chunks conditionally. In a programming language, control structures are parts of the language that allow you to control what code is executed. By far the most common is the `if-else if-else structure.\n\nCodeif (logical_condition) {\n    # some code\n} else if (other_logical_condition) {\n    # some code\n} else {\n    # some code\n}\n\nmiddle &lt;- function(x) {\n    mean_x &lt;- mean(x, na.rm = TRUE)\n    median_x &lt;- median(x, na.rm = TRUE)\n    seems_skewed &lt;- (mean_x &gt; 1.5*median_x) | (mean_x &lt; (1/1.5)*median_x)\n    if (seems_skewed) {\n        median_x\n    } else {\n        mean_x\n    }\n}\n\n\n\n\n\n\n\n\nTo vectorize or not to vectorize\n\n\n\nThe if () else {} code is not vectorized; the logical condition in if() cannot be a logical vector of length longer than 1. If you want a vectorized version of conditional execution of code use if_else() or case_when(); these can be used in mutate().\n\n\n\n\nConvert Temp Write a function for converting temperatures that takes as input a numeric value and a unit (either “C” for Celsius or “F” for Fahrenheit). The function should convert the temperature from one unit to the other based on the following formulas:\n\n\nTo convert Celsius to Fahrenheit: (Celsius * 9/5) + 32\n\nTo convert Fahrenheit to Celsius: (Fahrenheit - 32) * 5/9\n\n\n\nCodeconvert_temp &lt;- function(num, unit) {\n  if (unit == \"C\") {\n     new_val &lt;- (num * 9/5) + 32\n     str_glue(\"{new_val} F\")\n  } \n  else if (unit == \"F\") {\n     new_val &lt;- (num - 32) * 5/9\n     str_glue(\"{new_val} C\")\n  }\n}\n\nconvert_temp(20, \"C\")\n\n68 F\n\nCodeconvert_temp(67, \"F\")\n\n19.4444444444444 C\n\n\n\n\nDomain Name Write a function that extracts the domain name of a supplied email address. The function should return the domain name (e.g., “gmail.com”). If the input is not a valid email address, return “Invalid Email”. (A valid email ends in “dot something”.)\n\n\nCodedomain_name &lt;- function(email) {\n  is_valid &lt;- (str_detect(email, \"@.+\\\\..+\"))\n  if (is_valid) {\n    str_extract(email, \"@.+$\")\n  }\n  else {\n    \"Invalid Email\"\n  }\n} \n\ndomain_name(\"tduckwor@macalester.edu\")\n\n[1] \"@macalester.edu\"\n\nCodedomain_name(\"tduckworth@gmail\")\n\n[1] \"Invalid Email\"\n\nCodedomain_name(\"tduckwor.com\")\n\n[1] \"Invalid Email\"\n\nCodedomain_name(\"tduckwor@gmail.\")\n\n[1] \"Invalid Email\"",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9 Functions</span>"
    ]
  },
  {
    "objectID": "src/ica/09-functions-notes.html#writing-functions-with-tidyverse-verbs",
    "href": "src/ica/09-functions-notes.html#writing-functions-with-tidyverse-verbs",
    "title": "9 Functions",
    "section": "Writing Functions with tidyverse Verbs",
    "text": "Writing Functions with tidyverse Verbs\nPerhaps we are using group_by() and summarize() a lot to compute group means. We might write this function:\n\nCodegroup_means &lt;- function(df, group_var, mean_var) {\n    df |&gt;\n        group_by(group_var) |&gt;\n        summarize(mean = mean(mean_var))\n}\n\n\nLet’s use it on the diamonds dataset to compute the mean size (carat) by diamond cut:\n\nCodegroup_means(diamonds, group_var = cut, mean_var = carat)\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `group_var` is not found.\n\n\nWhat if the problem is that the variable names need to be in quotes?\n\nCodegroup_means(diamonds, group_var = \"cut\", mean_var = \"carat\")\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `group_var` is not found.\n\n\nWhat’s Going On?\nThe tidyverse uses something called tidy evaluation: this allows you to refer to a variable by typing it directly (e.g., no need to put it in quotes).\n\n\ngroup_by(group_var) is expecting a variable that is actually called group_var, and mean(mean_var) is expecting a variable that is actually called mean_var.\n\nTo fix this we need to embrace the variables inside the function with { var }:\n\nCodegroup_means &lt;- function(df, group_var, mean_var) {\n    df |&gt;\n        group_by({{ group_var }}) |&gt;\n        summarize(mean = mean({{ mean_var }}))\n}\n\n\nThe { var } tells R to look at what the variable specified by the input var rather than look for a variable called var.\n\nCodegroup_means(diamonds, group_var = cut, mean_var = carat)\n\n# A tibble: 5 × 2\n  cut        mean\n  &lt;ord&gt;     &lt;dbl&gt;\n1 Fair      1.05 \n2 Good      0.849\n3 Very Good 0.806\n4 Premium   0.892\n5 Ideal     0.703\n\n\nLet’s group by both cut and color:\n\nCodegroup_means(diamonds, group_var = c(cut, color), mean_var = carat)\n\nError in `group_by()`:\nℹ In argument: `c(cut, color)`.\nCaused by error:\n! `c(cut, color)` must be size 53940 or 1, not 107880.\n\n\nOh No! What Now?!\nWhen c(cut, color) is put inside { c(cut, color) } within the function, R is actually running the code inside { }.\n\nThis combines the columns for those 2 variables into one long vector. What we really meant by c(cut, color) is “group by both cut and color”.\n\nTo fix this, we need the pick() function to get R to see { group_var } as a vector of separate variables (like the way select() works).\n\nCodegroup_means &lt;- function(df, group_var, mean_var) {\n    df |&gt;\n        group_by(pick({{ group_var }})) |&gt;\n        summarize(mean = mean({{ mean_var }}))\n}\n\n\n\n\nProp function Create a new version of dplyr::count() that also shows proportions instead of just sample sizes. The function should be able to handle counting by multiple variables. Test your function with two different sets of arguments using the diamonds dataset.\n\n\nCodeproportions &lt;- function(df, count_var) {\n  df |&gt; \n    count(pick({{count_var}})) |&gt; \n    mutate(prop = n/sum(n))\n}\n\nproportions(diamonds, cut)\n\n# A tibble: 5 × 3\n  cut           n   prop\n  &lt;ord&gt;     &lt;int&gt;  &lt;dbl&gt;\n1 Fair       1610 0.0298\n2 Good       4906 0.0910\n3 Very Good 12082 0.224 \n4 Premium   13791 0.256 \n5 Ideal     21551 0.400 \n\nCodeproportions(diamonds, c(cut, color))\n\n# A tibble: 35 × 4\n   cut   color     n    prop\n   &lt;ord&gt; &lt;ord&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 Fair  D       163 0.00302\n 2 Fair  E       224 0.00415\n 3 Fair  F       312 0.00578\n 4 Fair  G       314 0.00582\n 5 Fair  H       303 0.00562\n 6 Fair  I       175 0.00324\n 7 Fair  J       119 0.00221\n 8 Good  D       662 0.0123 \n 9 Good  E       933 0.0173 \n10 Good  F       909 0.0169 \n# ℹ 25 more rows\n\n\n\n\nScatterplot + Smooth function Create a function that creates a scatterplot from a user-supplied dataset with user-supplied x and y variables. The plot should also show a curvy smoothing line in blue, and a linear smoothing line in red. Test your function using the diamonds dataset.\n\n\nCodescatter_smooth &lt;- function(df, x, y) {\n  df |&gt; \n    ggplot(aes(x = {{x}}, y = {{y}})) + \n    geom_point() + \n    geom_smooth(alpha = 0, color = \"blue\") + \n    geom_smooth(alpha = 0, method = \"lm\", color = \"red\")\n}\n\nscatter_smooth(diamonds, carat, price)",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9 Functions</span>"
    ]
  },
  {
    "objectID": "src/ica/09-functions-notes.html#done",
    "href": "src/ica/09-functions-notes.html#done",
    "title": "9 Functions",
    "section": "Done!",
    "text": "Done!\n\nCheck the ICA Instructions for how to (a) push your code to GitHub and (b) update your portfolio website",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9 Functions</span>"
    ]
  },
  {
    "objectID": "src/ica/10-base-r-notes.html",
    "href": "src/ica/10-base-r-notes.html",
    "title": "10 Base R",
    "section": "",
    "text": "🧩 Learning Goals\nBy the end of this lesson, you should be able to:\nCodelibrary(tibble)\nlibrary(tidyverse)",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10 Base R</span>"
    ]
  },
  {
    "objectID": "src/ica/10-base-r-notes.html#learning-goals",
    "href": "src/ica/10-base-r-notes.html#learning-goals",
    "title": "10 Base R",
    "section": "",
    "text": "Identify and define the properties of common structures in R\nSubset vectors and lists with [ by index, name, logical vector, and indirectly with objects\nSubset data frames and lists with $ and [[\n\nUse the str() function to examine the structure of an unfamiliar object and extract components from the object\nApply printing strategies to streamline the debugging and development process",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10 Base R</span>"
    ]
  },
  {
    "objectID": "src/ica/10-base-r-notes.html#common-r-object-structures",
    "href": "src/ica/10-base-r-notes.html#common-r-object-structures",
    "title": "10 Base R",
    "section": "Common R Object Structures",
    "text": "Common R Object Structures\nVector: A vector is a collection of elements of the same type (e.g., numeric, integer, character, logical).\n\nCodenum_vec &lt;- vector(\"numeric\", length = 2) #empty vector: Zeros\nnum_vec\n\n[1] 0 0\n\nCodeclass(num_vec)\n\n[1] \"numeric\"\n\nCodelength(num_vec)\n\n[1] 2\n\n\n\nCodelog_vec &lt;- vector(\"logical\", length = 3) #empty vector: FALSE\nlog_vec\n\n[1] FALSE FALSE FALSE\n\nCodeclass(log_vec)\n\n[1] \"logical\"\n\nCodelength(log_vec)\n\n[1] 3\n\n\n\nCodechr_vec &lt;- vector(\"character\", length = 4) #empty vector: empty strings\nchr_vec\n\n[1] \"\" \"\" \"\" \"\"\n\nCodeclass(chr_vec)\n\n[1] \"character\"\n\nCodelength(chr_vec)\n\n[1] 4\n\n\n\nFun Fact: A vector can have names for each of its elements.\n\n\nCodenamed_vec &lt;- c('name1' = 1, 'name2' = 2) # Named numeric vector\nnamed_vec\n\nname1 name2 \n    1     2 \n\nCodeclass(named_vec)\n\n[1] \"numeric\"\n\nCodelength(named_vec)\n\n[1] 2\n\n\nList: A list is a collection of elements (e.g., vectors, matrices, data frames, other lists).\n\nA list can have different types of elements.\nA list can have names for its elements.\n\n\nCodeex_list &lt;- list(a = 1:3, b = c(\"a\", \"b\", \"c\"), c = matrix(1:6, nrow = 2))\nex_list\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\" \"b\" \"c\"\n\n$c\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nCodeclass(ex_list)\n\n[1] \"list\"\n\nCodelength(ex_list) # number of elements in a list\n\n[1] 3",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10 Base R</span>"
    ]
  },
  {
    "objectID": "src/ica/10-base-r-notes.html#other-common-r-object-structures",
    "href": "src/ica/10-base-r-notes.html#other-common-r-object-structures",
    "title": "10 Base R",
    "section": "Other Common R Object Structures",
    "text": "Other Common R Object Structures\nArray: An array is a vector with a dimension attribute.\n\nLike a vector, an array can only have one type of data (e.g., numeric, character).\n\n\nCodeary &lt;- array(NA, dim = c(2,3,4))\nary\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,]   NA   NA   NA\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,]   NA   NA   NA\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,]   NA   NA   NA\n\n, , 4\n\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,]   NA   NA   NA\n\nCodeclass(ary)\n\n[1] \"array\"\n\nCodelength(ary) # The number of elements in a array is the product of its dimensions\n\n[1] 24\n\nCodedim(ary) # Get the dimensions of the array\n\n[1] 2 3 4\n\n\nMatrix: A matrix is an array with only 2 dimensions (rows, columns).\n\nLike a vector, a matrix can only have one type of data (e.g., numeric, character).\n\n\nCodem &lt;- matrix(NA, nrow = 2, ncol = 3)\nm\n\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,]   NA   NA   NA\n\nCodeclass(m)\n\n[1] \"matrix\" \"array\" \n\nCodelength(m) # The number of elements in a matrix is the product of its dimensions\n\n[1] 6\n\nCodedim(m) # Get the dimensions of the matrix\n\n[1] 2 3\n\n\nData Frame/tibble: A data frame is a named list with elements of equal length.\n\nEach element is a “column” in the data frame.\nThe columns can be of different types (e.g., character, numeric, logical, lists, etc.).\nData frames are the most common way to store data in R.\nTibbles do less and complain more than base data.frames\n\n\nCodemod_df &lt;- tibble(x = 1:10, y = 1:10 + rnorm(10))\n\ndf &lt;- tibble(a = 1:3, b = c(\"constant\", \"x\", \"x squared\"), d = list(lm(y ~ 1, data = mod_df), lm(y ~ x, data = mod_df), lm(y ~ x + I(x^2), data = mod_df)))\ndf\n\n# A tibble: 3 × 3\n      a b         d     \n  &lt;int&gt; &lt;chr&gt;     &lt;list&gt;\n1     1 constant  &lt;lm&gt;  \n2     2 x         &lt;lm&gt;  \n3     3 x squared &lt;lm&gt;  \n\nCodelength(df) # number of \"elements\" in a data frame is the number of \"columns\"\n\n[1] 3",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10 Base R</span>"
    ]
  },
  {
    "objectID": "src/ica/10-base-r-notes.html#base-r-subsetting",
    "href": "src/ica/10-base-r-notes.html#base-r-subsetting",
    "title": "10 Base R",
    "section": "Base R Subsetting",
    "text": "Base R Subsetting\nThe content here comes from Chapter 27 of R4DS, with some small additions.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10 Base R</span>"
    ]
  },
  {
    "objectID": "src/ica/10-base-r-notes.html#selecting-elements-with",
    "href": "src/ica/10-base-r-notes.html#selecting-elements-with",
    "title": "10 Base R",
    "section": "Selecting elements with [\n",
    "text": "Selecting elements with [\n\nWe can subset common R structures and maintain the class structure with [ ].\nThere are four main types of things that you can subset with, i.e., that can be the i in x[i]:\n\n\nA vector of positive integers. Subsetting with positive integers keeps the elements at those positions:\n\n\nCode# Vectors\nx &lt;- c(\"one\", \"two\", \"three\", \"four\", \"five\")\nx[c(3, 2, 5)]\n\n[1] \"three\" \"two\"   \"five\" \n\nCodex[2:4]\n\n[1] \"two\"   \"three\" \"four\" \n\nCodeclass(x[2:4]) # result is a character vector\n\n[1] \"character\"\n\n\n\nCode# Lists\ny &lt;- list(a = 1:3, b = c(\"a\", \"b\", \"c\"), c = matrix(1:6, nrow = 2))\ny[c(1, 2)]\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\" \"b\" \"c\"\n\nCodeclass(y[c(1)]) # result is a list\n\n[1] \"list\"\n\n\nBy repeating a position, you can actually make a longer output than input, making the term “subsetting” a bit of a misnomer.\n\nCode# Vector\nx[c(1, 1, 2)]\n\n[1] \"one\" \"one\" \"two\"\n\nCode# List\ny[c(1, 1, 2)]\n\n$a\n[1] 1 2 3\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\" \"b\" \"c\"\n\n\n\n\nA vector of negative integers. Negative values drop the elements at the specified positions:\n\n\nCode# Vector\nx[c(-1, -3, -5)]\n\n[1] \"two\"  \"four\"\n\nCode# List\ny[c(-1)]\n\n$b\n[1] \"a\" \"b\" \"c\"\n\n$c\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\n\nA logical vector. Subsetting with a logical vector only keeps values corresponding to TRUE. This is generally used with comparison functions and operators.\n\n\nCode# Vector\nx &lt;- c(10, 3, NA, 5, 8, 1, NA)\n\n# All non-missing values of x\nx[!is.na(x)]\n\n[1] 10  3  5  8  1\n\nCode# All values greater than 5, with NAs\nx[x &gt; 5]\n\n[1] 10 NA  8 NA\n\nCode# All non-missing values greater than 5\nx[x &gt; 5 & !is.na(x)]\n\n[1] 10  8\n\n\nUnlike filter(), NA indices will be included in the output as NAs unless you explicitly remove them (filter() removes instances of missing values by default.\n\nCode# Compare with filter \nfilter(tibble(x = x), x &gt; 5)\n\n# A tibble: 2 × 1\n      x\n  &lt;dbl&gt;\n1    10\n2     8\n\n\n\nCode# List\ny[c(TRUE, FALSE, TRUE)]\n\n$a\n[1] 1 2 3\n\n$c\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nCodey[y |&gt; map_lgl(~ is.numeric(.x))] # example of a map function!\n\n$a\n[1] 1 2 3\n\n$c\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nCodey[y |&gt; map_lgl(~ is.character(.x))]\n\n$b\n[1] \"a\" \"b\" \"c\"\n\n\n\n\nA character vector. If you have a named vector or list, you can subset it with a character vector:\n\n\nCode# Named Vector\nx &lt;- c(abc = 1, def = 2, xyz = 5)\nx[c(\"xyz\", \"def\")]\n\nxyz def \n  5   2 \n\nCodex[c(\"xyz\",\"xyz\",\"xyz\", \"def\")]\n\nxyz xyz xyz def \n  5   5   5   2 \n\nCode#Named List\ny[c('a','a','c')]\n\n$a\n[1] 1 2 3\n\n$a\n[1] 1 2 3\n\n$c\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nAs with subsetting with positive integers, you can use a character vector to duplicate individual entries.\nBe very wary of vector recycling when doing this! The number of things that you’re inserting should either be 1 or the size of the x[i] subset.\n\nCodex &lt;- c(first = \"one\", second = \"two\", third = \"three\", fourth = \"four\")\nx\n\n  first  second   third  fourth \n  \"one\"   \"two\" \"three\"  \"four\" \n\n\n\nCodex[c(1, 3)] &lt;- \"new\" # Replacement length is 1\nx\n\n first second  third fourth \n \"new\"  \"two\"  \"new\" \"four\" \n\n\n\nCodex &lt;- c(first = \"one\", second = \"two\", third = \"three\", fourth = \"four\")\nx[c(1, 3)] &lt;- c(\"new1\", \"new2\") # Replacement length is 2, and length of subset is 2\nx\n\n first second  third fourth \n\"new1\"  \"two\" \"new2\" \"four\" \n\n\n\nCodex &lt;- c(first = \"one\", second = \"two\", third = \"three\", fourth = \"four\")\nx[c(1, 3, 4)] &lt;- c(\"new1\", \"new2\") # BAD! Replacement length is 2, and length of subset is 3\nx\n\n first second  third fourth \n\"new1\"  \"two\" \"new2\" \"new1\" \n\n\n\nCodex &lt;- c(first = \"one\", second = \"two\", third = \"three\", fourth = \"four\")\nx[c(1, 3)] &lt;- c(\"new1\", \"new2\", \"new3\") # BAD! Replacement length is 3, and length of subset is 2\nx\n\n first second  third fourth \n\"new1\"  \"two\" \"new2\" \"four\"",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10 Base R</span>"
    ]
  },
  {
    "objectID": "src/ica/10-base-r-notes.html#subsetting-matricies-and-data-frames-with",
    "href": "src/ica/10-base-r-notes.html#subsetting-matricies-and-data-frames-with",
    "title": "10 Base R",
    "section": "Subsetting Matricies and Data Frames with [\n",
    "text": "Subsetting Matricies and Data Frames with [\n\nAll of the above subsetting options can be used for subsetting matrices and data frames (named list of elements of equal length).\n\nCodem &lt;- matrix(1:12, nrow = 3, ncol = 4)\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nCodem[1:5] # Matrix = vector (down the columns) with dimensions\n\n[1] 1 2 3 4 5\n\n\nYou can use a comma to subset by rows and columns separately.\n\nCodem[1,] # Get 1st row\n\n[1]  1  4  7 10\n\nCodem[,1] # Get 1st column\n\n[1] 1 2 3\n\n\n. . .\n\nCodem[1,3] # Get 1st row and 3rd column\n\n[1] 7\n\nCodem[c(1,3),] # Get 1st and 3rd rows\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    3    6    9   12\n\nCodem[,c(1,3)] # Get 1st and 3rd columns\n\n     [,1] [,2]\n[1,]    1    7\n[2,]    2    8\n[3,]    3    9\n\nCodem[c(1,3),c(1,3)] # Get 1st and 3rd rows and 1st and 3rd columns\n\n     [,1] [,2]\n[1,]    1    7\n[2,]    3    9\n\nCodem[-1,] # Get all rows except 1st\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    5    8   11\n[2,]    3    6    9   12\n\nCodem[c(TRUE, FALSE, FALSE),] # Get the 1st row via a logical\n\n[1]  1  4  7 10\n\nCode# Add row and column names to the matrix\ncolnames(m) &lt;- str_c(\"col\", 1:4)\nrownames(m) &lt;- str_c(\"row\", 1:3)\nm[\"row1\",]\n\ncol1 col2 col3 col4 \n   1    4    7   10",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10 Base R</span>"
    ]
  },
  {
    "objectID": "src/ica/10-base-r-notes.html#selecting-a-single-element-with-and",
    "href": "src/ica/10-base-r-notes.html#selecting-a-single-element-with-and",
    "title": "10 Base R",
    "section": "Selecting a single element with $ and [[\n",
    "text": "Selecting a single element with $ and [[\n\nWe can use $ and [[ to extract a single column of a data frame or an element within a list. This breaks out of the original class structure.\n\nCodemtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\nCodemtcars$mpg\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\nCodemtcars[[\"mpg\"]]\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\nCodemtcars |&gt; pull(mpg)\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10 Base R</span>"
    ]
  },
  {
    "objectID": "src/ica/10-base-r-notes.html#exercises",
    "href": "src/ica/10-base-r-notes.html#exercises",
    "title": "10 Base R",
    "section": "Exercises",
    "text": "Exercises\n\n\nSubsetting Functions For each of the tasks below, write a function that take a vector as input returns the desired output:\n\n\nThe elements at even-numbered positions. (Hint: use the seq() function.)\n\n\nCodeeven_position &lt;- function(vector) {\n  if (length(vector) &lt;= 1) {\n     return(\"No even positions\")\n  }\n  else {\n     seq &lt;- seq(2, length(vector), by = 2)\n     vector[seq]\n  }\n}\n\neven_position(1)\n\n[1] \"No even positions\"\n\nCodeeven_position(1:10)\n\n[1]  2  4  6  8 10\n\nCodeeven_position(23:30)\n\n[1] 24 26 28 30\n\n\n\nEvery element except the last value.\n\n\nCodeexcept_last &lt;- function(vector) {\n  vector[-length(vector)]\n}\n\nexcept_last(1:9)\n\n[1] 1 2 3 4 5 6 7 8\n\nCodeexcept_last(c(\"two\", \"hi\", \"boo\", \"class\"))\n\n[1] \"two\" \"hi\"  \"boo\"\n\n\n\nOnly even values (and no missing values).\n\n\nCodeeven_values &lt;- function(vector) {\n  vector[vector %% 2 == 0 & !is.na(vector)]\n}\n\neven_values(c(1, NA, 9, 10, 17, 12, NA, 6, 4))\n\n[1] 10 12  6  4",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10 Base R</span>"
    ]
  },
  {
    "objectID": "src/ica/10-base-r-notes.html#exploring-the-structure-of-an-object-with-str",
    "href": "src/ica/10-base-r-notes.html#exploring-the-structure-of-an-object-with-str",
    "title": "10 Base R",
    "section": "Exploring the structure of an object with str()\n",
    "text": "Exploring the structure of an object with str()\n\nThe str() function shows you the structure of an object and is useful for exploring model objects and objects created from packages that are new to you.\nIn the output of str() dollar signs indicate named components of a list that can be extracted via $ or [[.\n. . .\nWe see that both mod and mod_summ are lists, so we can also interactively view these objects with View(mod) and View(mod_summ) in the Console.\n\nCodemod &lt;- lm(mpg ~ hp + wt, data = mtcars)\nmod_summ &lt;- summary(mod)\n\nstr(mod)\n\nList of 12\n $ coefficients : Named num [1:3] 37.2273 -0.0318 -3.8778\n  ..- attr(*, \"names\")= chr [1:3] \"(Intercept)\" \"hp\" \"wt\"\n $ residuals    : Named num [1:32] -2.572 -1.583 -2.476 0.135 0.373 ...\n  ..- attr(*, \"names\")= chr [1:32] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Datsun 710\" \"Hornet 4 Drive\" ...\n $ effects      : Named num [1:32] -113.65 -26.046 -15.894 0.447 0.662 ...\n  ..- attr(*, \"names\")= chr [1:32] \"(Intercept)\" \"hp\" \"wt\" \"\" ...\n $ rank         : int 3\n $ fitted.values: Named num [1:32] 23.6 22.6 25.3 21.3 18.3 ...\n  ..- attr(*, \"names\")= chr [1:32] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Datsun 710\" \"Hornet 4 Drive\" ...\n $ assign       : int [1:3] 0 1 2\n $ qr           :List of 5\n  ..$ qr   : num [1:32, 1:3] -5.657 0.177 0.177 0.177 0.177 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:32] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Datsun 710\" \"Hornet 4 Drive\" ...\n  .. .. ..$ : chr [1:3] \"(Intercept)\" \"hp\" \"wt\"\n  .. ..- attr(*, \"assign\")= int [1:3] 0 1 2\n  ..$ qraux: num [1:3] 1.18 1.08 1.09\n  ..$ pivot: int [1:3] 1 2 3\n  ..$ tol  : num 1e-07\n  ..$ rank : int 3\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 29\n $ xlevels      : Named list()\n $ call         : language lm(formula = mpg ~ hp + wt, data = mtcars)\n $ terms        :Classes 'terms', 'formula'  language mpg ~ hp + wt\n  .. ..- attr(*, \"variables\")= language list(mpg, hp, wt)\n  .. ..- attr(*, \"factors\")= int [1:3, 1:2] 0 1 0 0 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:3] \"mpg\" \"hp\" \"wt\"\n  .. .. .. ..$ : chr [1:2] \"hp\" \"wt\"\n  .. ..- attr(*, \"term.labels\")= chr [1:2] \"hp\" \"wt\"\n  .. ..- attr(*, \"order\")= int [1:2] 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(mpg, hp, wt)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:3] \"numeric\" \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:3] \"mpg\" \"hp\" \"wt\"\n $ model        :'data.frame':  32 obs. of  3 variables:\n  ..$ mpg: num [1:32] 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n  ..$ hp : num [1:32] 110 110 93 110 175 105 245 62 95 123 ...\n  ..$ wt : num [1:32] 2.62 2.88 2.32 3.21 3.44 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ hp + wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, hp, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:3, 1:2] 0 1 0 0 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:3] \"mpg\" \"hp\" \"wt\"\n  .. .. .. .. ..$ : chr [1:2] \"hp\" \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr [1:2] \"hp\" \"wt\"\n  .. .. ..- attr(*, \"order\")= int [1:2] 1 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, hp, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:3] \"numeric\" \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:3] \"mpg\" \"hp\" \"wt\"\n - attr(*, \"class\")= chr \"lm\"\n\nCodestr(mod_summ)\n\nList of 11\n $ call         : language lm(formula = mpg ~ hp + wt, data = mtcars)\n $ terms        :Classes 'terms', 'formula'  language mpg ~ hp + wt\n  .. ..- attr(*, \"variables\")= language list(mpg, hp, wt)\n  .. ..- attr(*, \"factors\")= int [1:3, 1:2] 0 1 0 0 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:3] \"mpg\" \"hp\" \"wt\"\n  .. .. .. ..$ : chr [1:2] \"hp\" \"wt\"\n  .. ..- attr(*, \"term.labels\")= chr [1:2] \"hp\" \"wt\"\n  .. ..- attr(*, \"order\")= int [1:2] 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(mpg, hp, wt)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:3] \"numeric\" \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:3] \"mpg\" \"hp\" \"wt\"\n $ residuals    : Named num [1:32] -2.572 -1.583 -2.476 0.135 0.373 ...\n  ..- attr(*, \"names\")= chr [1:32] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Datsun 710\" \"Hornet 4 Drive\" ...\n $ coefficients : num [1:3, 1:4] 37.22727 -0.03177 -3.87783 1.59879 0.00903 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"(Intercept)\" \"hp\" \"wt\"\n  .. ..$ : chr [1:4] \"Estimate\" \"Std. Error\" \"t value\" \"Pr(&gt;|t|)\"\n $ aliased      : Named logi [1:3] FALSE FALSE FALSE\n  ..- attr(*, \"names\")= chr [1:3] \"(Intercept)\" \"hp\" \"wt\"\n $ sigma        : num 2.59\n $ df           : int [1:3] 3 29 3\n $ r.squared    : num 0.827\n $ adj.r.squared: num 0.815\n $ fstatistic   : Named num [1:3] 69.2 2 29\n  ..- attr(*, \"names\")= chr [1:3] \"value\" \"numdf\" \"dendf\"\n $ cov.unscaled : num [1:3, 1:3] 3.80e-01 2.21e-05 -1.09e-01 2.21e-05 1.21e-05 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"(Intercept)\" \"hp\" \"wt\"\n  .. ..$ : chr [1:3] \"(Intercept)\" \"hp\" \"wt\"\n - attr(*, \"class\")= chr \"summary.lm\"",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10 Base R</span>"
    ]
  },
  {
    "objectID": "src/ica/10-base-r-notes.html#exercise",
    "href": "src/ica/10-base-r-notes.html#exercise",
    "title": "10 Base R",
    "section": "Exercise",
    "text": "Exercise\n\n\nCI Function Write a function that fits a linear model on the dataset using the given outcome and predictor variables and return a data frame (tibble) with the coefficient estimate and CI for the predictor of interest. It should take the following inputs:\n\n\n\ndata: A dataset\n\nyvar: Outcome variable to be used in a linear model (a length-1 character vector)\n\npreds: Predictor variables to be used in a linear model (a character vector)\n\npred_of_interest: The variable whose coefficient estimate and confidence interval are of interest (a length-1 character vector and should be one of preds)\n\nDevelopment tip: As you develop, it will help to create objects for the arguments so that you can see what output looks like interactively:\nTest your function on the mtcars dataset.\n\nCodedata &lt;- mtcars\nyvar &lt;- \"mpg\"\npreds &lt;- c(\"hp\", \"wt\")\npred_of_interest &lt;- \"hp\"\n\n\nWhen you’re done developing your function, remove these objects to declutter your environment by entering rm(data, yvar, preds, pred_of_interest) in the Console.\n\nCodefit_mod_and_extract &lt;- function(data, yvar, preds, pred_of_interest) {\n  # Use str_c to create a string (formula_str) that looks like \"yvar ~ pred1 + pred2\"\n    # Look at the documentation for a helpful argument\n    mod_formula_str &lt;- str_c(yvar, \"~\", str_c(preds, collapse = \"+\"))\n    mod_form &lt;- as.formula(mod_formula_str)\n    \n    # Fit a linear model using the constructed formula and given data\n    mod &lt;- lm(mod_form, data = data)\n    \n    # Obtain 95% confidence interval\n    ci &lt;- confint(mod, level = 0.95)\n    \n    # Return the coefficient estimate and CI for the predictor of interest\n    tibble(\n        which_pred = pred_of_interest,\n        estimate = mod$coefficients[pred_of_interest],\n        ci_lower = ci[pred_of_interest, \"2.5 %\"],\n        ci_upper = ci[pred_of_interest, \"97.5 %\"]\n    )\n}\n\n\n\nCodefit_mod_and_extract(mtcars, \"mpg\", c(\"hp\", \"wt\"), \"hp\")",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10 Base R</span>"
    ]
  },
  {
    "objectID": "src/ica/10-base-r-notes.html#debugging-strategies",
    "href": "src/ica/10-base-r-notes.html#debugging-strategies",
    "title": "10 Base R",
    "section": "Debugging Strategies",
    "text": "Debugging Strategies\nWhen writing functions and working with functions that you wrote, you may encounter errors that are hard to figure out.\nHere are some strategies to help you debug the issues you encounter:\n\nUse print() and cat() to print out intermediate results and messages within a function.\n\nExamples: print(x), cat(\"The value of x is\", x, \"\\n\")\n\n\n\n\n\nCodeMy_own_sum &lt;- function(x){\n  print(x)\n  return(sum(x))\n}\nMy_own_sum(c(1,2,3))\n\nMy_own_sum &lt;- function(x){\n  cat(\"The value of x is\", x, \"\\n\")\n  cat(\"The class of x is\", class(x), \"\\n\")\n  return(sum(x))\n}\n\nMy_own_sum(c(1,2,3))\n\n\n\nUse browser() to pause the function at a certain point and interactively explore the environment. Press “Next” or type n to run the next line of code. Type the name of an object in the Console to see its value at this point in the function. You can type Q to quit the browser.\n\nExample below:\n\n\n\n\nCodefit_mod_and_extract &lt;- function(data, yvar, preds, pred_of_interest) {\n    # Use str_c to create a string (formula_str) that looks like \"yvar ~ pred1 + pred2\"\n    # Look at the documentation for a helpful argument\n    mod_formula_str &lt;- str_c(yvar, \"~\", str_c(preds, collapse = \"+\"))\n    mod_form &lt;- as.formula(mod_formula_str)\n    \n    # Add browser() to where in the function you'd like to pause and interact in the function environment using the Console\n    browser()\n    \n    # Fit a linear model using the constructed formula and given data\n    mod &lt;- lm(mod_form, data = data)\n    \n    # Obtain 95% confidence interval\n    ci &lt;- confint(mod, level = 0.95)\n    \n    # Return the coefficient estimate and CI for the predictor of interest\n    tibble(\n        which_pred = pred_of_interest,\n        estimate = mod$coefficients[pred_of_interest],\n        ci_lower = ci[pred_of_interest, \"2.5 %\"],\n        ci_upper = ci[pred_of_interest, \"97.5 %\"]\n    )\n}\n\n\nfit_mod_and_extract(data = mtcars, yvar = \"mpg\", preds = c(\"hp\", \"wt\"), pred_of_interest = \"hp\")\n\n\n\nUse try() to catch errors and print out a message when an error occurs.\n\nExample below:\n\n\n\n\nCodeMy_own_sum &lt;- function(x){\n  return(sum(x))\n}\n\nresults &lt;- My_own_sum(c(\"a\",\"b\",\"c\"))\nclass(results)\n\nresults &lt;- try(My_own_sum(c(\"a\",\"b\",\"c\")), silent = TRUE)\nclass(results)\n\n\n\nInclude if else statements within a function to ensure that you are passing the right type of input to a function. You can create you own custom error message with stop().\n\nExample below:\n\n\n\n\nCodeMy_own_sum &lt;- function(x){\n  if(!is.numeric(x)){\n    stop(\"Input must be numeric\")\n  }\n  return(sum(x))\n}\n\nresults &lt;- My_own_sum(c(\"a\",\"b\",\"c\"))\nclass(results)\n\nresults &lt;- try(My_own_sum(c(\"a\",\"b\",\"c\")), silent = TRUE)\nclass(results)",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10 Base R</span>"
    ]
  },
  {
    "objectID": "src/ica/10-base-r-notes.html#done",
    "href": "src/ica/10-base-r-notes.html#done",
    "title": "10 Base R",
    "section": "Done!",
    "text": "Done!\n\nCheck the ICA Instructions for how to (a) push your code to GitHub and (b) update your portfolio website",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10 Base R</span>"
    ]
  },
  {
    "objectID": "src/ica/11-iteration-1-notes.html",
    "href": "src/ica/11-iteration-1-notes.html",
    "title": "11 Loops+Iter P1",
    "section": "",
    "text": "🧩 Learning Goals\n##Libraries\nBy the end of this lesson, you should be able to:",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11 Loops+Iter P1</span>"
    ]
  },
  {
    "objectID": "src/ica/11-iteration-1-notes.html#learning-goals",
    "href": "src/ica/11-iteration-1-notes.html#learning-goals",
    "title": "11 Loops+Iter P1",
    "section": "",
    "text": "Write a for loop in R to handle repeated tasks\n\nUse seq_along() and seq_len() to iterate over vectors or rows of a data frame\nStore output from a for loop in a list or other storage container\n\n\nRecognize that vectorized functions help us avoid for loops in many situations\nUse the across() function to wrangle multiple variables simultaneously",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11 Loops+Iter P1</span>"
    ]
  },
  {
    "objectID": "src/ica/11-iteration-1-notes.html#iteration",
    "href": "src/ica/11-iteration-1-notes.html#iteration",
    "title": "11 Loops+Iter P1",
    "section": "Iteration",
    "text": "Iteration\nWhen we want to reuse code, we can use:\n\nfunctions to save the steps of the task\niteration to apply that function to new inputs\n\nLet’s talk about iteration in R.\n\nIterating using a for loop & how to do it appropriately in R (today)\nIterating across data frame columns (today)\nIterating using map and walk for iteration, instead of a for loop (next class)",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11 Loops+Iter P1</span>"
    ]
  },
  {
    "objectID": "src/ica/11-iteration-1-notes.html#purposes-of-iteration",
    "href": "src/ica/11-iteration-1-notes.html#purposes-of-iteration",
    "title": "11 Loops+Iter P1",
    "section": "Purposes of Iteration",
    "text": "Purposes of Iteration\nCreating or saving a new object\n\nPerforming an action with a return output\nEx. creating new variables/datasets, fitting models, etc.\nWe’ll need to consider storage (memory allocation).\nNext class, we’ll use map() for this…\n\nSide effects\n\nPerforming an action without a R return output\nEx: saving data using write_csv() or saving a plot with png()\n\nWe don’t need to worry about storage in R.\nWe could use walk() for this…",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11 Loops+Iter P1</span>"
    ]
  },
  {
    "objectID": "src/ica/11-iteration-1-notes.html#for-loops",
    "href": "src/ica/11-iteration-1-notes.html#for-loops",
    "title": "11 Loops+Iter P1",
    "section": "\nfor loops",
    "text": "for loops\nIn R, for loops have the following general structure:\n\nCodefor (i in some_vector) {\n    # Code to do stuff based on i\n}\n\n\nsome_vector can be any vector, including:\n\nAn indexing integer vector: 1:3 [Most common!]\nA character vector: c(\"group1\", \"group2\", \"group3\")\n\nA vector of any other class\n\n\nCodegroups &lt;- c(\"group1\", \"group2\", \"group3\") # a vector to iterate over\n \nfor (i in 1:3) { # i: index\n    print(groups[i])\n}\n\n[1] \"group1\"\n[1] \"group2\"\n[1] \"group3\"\n\nCodefor (g in groups) { # iterating over the vector\n    print(g)\n}\n\n[1] \"group1\"\n[1] \"group2\"\n[1] \"group3\"\n\n\nTo iterate over an indexing vector, it is useful to know about the following two functions:\n\n\nseq_along(v): generates an integer sequence from 1 to the length of the vector v supplied\n\nseq_len(n): generates an integer sequence from 1 to n\n\n\n\nCodefor (i in seq_along(groups)) {\n    print(groups[i])\n}\n\n[1] \"group1\"\n[1] \"group2\"\n[1] \"group3\"\n\nCodefor (i in seq_len(length(groups))) {\n    print(groups[i])\n}\n\n[1] \"group1\"\n[1] \"group2\"\n[1] \"group3\"\n\n\nA nice feature of seq_along() is that it generates an empty iteration vector if the vector you’re iterating over itself has length 0.\n\nCodeno_groups &lt;- c()\nseq_along(no_groups)\n\ninteger(0)\n\nCodefor (i in seq_along(no_groups)) {\n    print(no_groups[i])\n}\n\n\nseq_len() is useful for iterating over the rows of a data frame because seq_along() would iterate over columns:\n\nCodesmall_data &lt;- tibble(a = 1:2, b = 2:3, c = 4:5)\n\nfor (col in small_data) { # each element is the column \n    print(col)\n}\n\n[1] 1 2\n[1] 2 3\n[1] 4 5\n\nCodefor (r in seq_len(nrow(small_data))) {\n    print(r)\n}\n\n[1] 1\n[1] 2\n\n\nIndexing is particularly important when we want to create/save new objects as we need to store output created during a for loop.\n\n\n\n\n\n\nWhen using a for loop to create/save output, we should create storage containers ahead of time with the vector() function. This allocates memory for the output and is a more efficient use of memory.\n\n\n\n\nCodechar_storage &lt;- vector(\"character\", length = 3)\nnum_storage &lt;- vector(\"numeric\", length = 3)\nlist_storage &lt;- vector(\"list\", length = 3)\n\nchar_storage\n\n[1] \"\" \"\" \"\"\n\nCodenum_storage\n\n[1] 0 0 0\n\nCodelist_storage\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\nCodefor (i in seq_len(3)) {\n    char_storage[i] &lt;- str_c(\"Number: \", i)\n    num_storage[i] &lt;- 2*i\n    list_storage[[i]] &lt;- i # Note the [[ for subsetting here\n}\n\nchar_storage\n\n[1] \"Number: 1\" \"Number: 2\" \"Number: 3\"\n\nCodenum_storage\n\n[1] 2 4 6\n\nCodelist_storage\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n\n\nfor loops and memory allocation\nIf you don’t create a storage container ahead of time, you will regret it. Let’s bench mark our code to see the difference!\n\nCode# Example 1: No storage container; append new value to existing vector\nx &lt;- integer()\nbench::mark(\n  for (i in 1:1e5) {\n    x &lt;- c(x, i)\n  }\n)\n\n# Example 2: Using a storage container!\n#Notice median (time to run), mem_alloc (how much memory you are using), and gc (garbage collection)\nx &lt;- vector('integer', length = 1e5)\nbench::mark(\n  for (i in 1:1e5) {\n    x[i] &lt;- i\n  }\n)\n\n\n# Example 3: Vectorized function\n#Notice median (time to run), mem_alloc (how much memory you are using), and gc (garbage collection)\nbench::mark(\n  x &lt;- seq(1, 1e5, by = 1)\n)",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11 Loops+Iter P1</span>"
    ]
  },
  {
    "objectID": "src/ica/11-iteration-1-notes.html#exercises",
    "href": "src/ica/11-iteration-1-notes.html#exercises",
    "title": "11 Loops+Iter P1",
    "section": "Exercises",
    "text": "Exercises\nPair programming exercises: Whoever has most recently eaten dessert (broadly interpreted) should be the driver first. Switch after Exercise 2.\nWrite for-loops that do each of the following:\n\nPrints the even numbers from 2:20\n\nThen, produce the same output with the seq() function\n\n\n\n\nCode# for loop\nfor (i in seq_len(10)) {\n  print(2*i)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n[1] 10\n[1] 12\n[1] 14\n[1] 16\n[1] 18\n[1] 20\n\nCode# vectorized approach with `seq()`\nseq(from = 2, to = 20, by = 2)\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\n\nIterates over the month.name vector and stores a character vector of output containing strings like “Month 1: January”, “Month 2: February”.\n\nThen, produce the same output with str_c() only.\n\n\n\n\nCodemonth_string &lt;- vector(\"character\", length = length(month.name))\n\n# for loop\nfor (i in seq_along(month.name)) {\n  month_string[i] &lt;- str_c(\"Month \", i, \" : \", month.name[i])\n}\n\nmonth_string\n\n [1] \"Month 1 : January\"   \"Month 2 : February\"  \"Month 3 : March\"    \n [4] \"Month 4 : April\"     \"Month 5 : May\"       \"Month 6 : June\"     \n [7] \"Month 7 : July\"      \"Month 8 : August\"    \"Month 9 : September\"\n[10] \"Month 10 : October\"  \"Month 11 : November\" \"Month 12 : December\"\n\nCode# vectorized approach with `str_c()`\nstr_c(\"Month \", 1:12, \": \", month.name)\n\n [1] \"Month 1: January\"   \"Month 2: February\"  \"Month 3: March\"    \n [4] \"Month 4: April\"     \"Month 5: May\"       \"Month 6: June\"     \n [7] \"Month 7: July\"      \"Month 8: August\"    \"Month 9: September\"\n[10] \"Month 10: October\"  \"Month 11: November\" \"Month 12: December\"\n\n\n\nOn the diamonds dataset, fit stratified models of price vs. carat, model separately estimated for each value of cut, and store the fitted models in a list storage container.\n\n\nCodedata(diamonds)\n\n# for loop\ncut_type &lt;- diamonds |&gt; pull(cut) |&gt; levels()\ncut_vector &lt;- vector(\"list\", length = length(cut_type))\n\nfor (i in seq_along(cut_type)) {\n  specific_cut &lt;- cut_type[i]\n  sub_diamonds &lt;- diamonds |&gt; \n    filter(cut == specific_cut)\n  cut_vector[[i]] &lt;- lm(price ~ carat, data = sub_diamonds)\n}",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11 Loops+Iter P1</span>"
    ]
  },
  {
    "objectID": "src/ica/11-iteration-1-notes.html#iteration-across-data-frame-columns",
    "href": "src/ica/11-iteration-1-notes.html#iteration-across-data-frame-columns",
    "title": "11 Loops+Iter P1",
    "section": "Iteration across data frame columns",
    "text": "Iteration across data frame columns\nOften we will have to perform the same data wrangling on many variables (e.g., rounding numbers)\n\nCodediamonds |&gt;\n    mutate(\n        carat = round(carat, 1),\n        x = round(x, 1),\n        y = round(y, 1),\n        z = round(z, 1)\n    )\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   0.2 Ideal     E     SI2      61.5    55   326   4     4     2.4\n 2   0.2 Premium   E     SI1      59.8    61   326   3.9   3.8   2.3\n 3   0.2 Good      E     VS1      56.9    65   327   4     4.1   2.3\n 4   0.3 Premium   I     VS2      62.4    58   334   4.2   4.2   2.6\n 5   0.3 Good      J     SI2      63.3    58   335   4.3   4.3   2.8\n 6   0.2 Very Good J     VVS2     62.8    57   336   3.9   4     2.5\n 7   0.2 Very Good I     VVS1     62.3    57   336   4     4     2.5\n 8   0.3 Very Good H     SI1      61.9    55   337   4.1   4.1   2.5\n 9   0.2 Fair      E     VS2      65.1    61   337   3.9   3.8   2.5\n10   0.2 Very Good H     VS1      59.4    61   338   4     4     2.4\n# ℹ 53,930 more rows\n\n\ndplyr provides the across() function for performing these repeated function calls:\n\nCode# Option 1: Create our own named function\nround_to_one &lt;- function(x) {\n    round(x, digits = 1)\n}\ndiamonds |&gt; \n    mutate(across(.cols = c(carat, x, y, z), .fns = round_to_one))\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   0.2 Ideal     E     SI2      61.5    55   326   4     4     2.4\n 2   0.2 Premium   E     SI1      59.8    61   326   3.9   3.8   2.3\n 3   0.2 Good      E     VS1      56.9    65   327   4     4.1   2.3\n 4   0.3 Premium   I     VS2      62.4    58   334   4.2   4.2   2.6\n 5   0.3 Good      J     SI2      63.3    58   335   4.3   4.3   2.8\n 6   0.2 Very Good J     VVS2     62.8    57   336   3.9   4     2.5\n 7   0.2 Very Good I     VVS1     62.3    57   336   4     4     2.5\n 8   0.3 Very Good H     SI1      61.9    55   337   4.1   4.1   2.5\n 9   0.2 Fair      E     VS2      65.1    61   337   3.9   3.8   2.5\n10   0.2 Very Good H     VS1      59.4    61   338   4     4     2.4\n# ℹ 53,930 more rows\n\n\n\nCode# Option 2: Use an \"anonymous\" or \"lambda\" function that isn't named/saved\ndiamonds |&gt; \n    mutate(across(.cols = c(carat, x, y, z), .fns = function(x) {round(x, digits = 1)} ))\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   0.2 Ideal     E     SI2      61.5    55   326   4     4     2.4\n 2   0.2 Premium   E     SI1      59.8    61   326   3.9   3.8   2.3\n 3   0.2 Good      E     VS1      56.9    65   327   4     4.1   2.3\n 4   0.3 Premium   I     VS2      62.4    58   334   4.2   4.2   2.6\n 5   0.3 Good      J     SI2      63.3    58   335   4.3   4.3   2.8\n 6   0.2 Very Good J     VVS2     62.8    57   336   3.9   4     2.5\n 7   0.2 Very Good I     VVS1     62.3    57   336   4     4     2.5\n 8   0.3 Very Good H     SI1      61.9    55   337   4.1   4.1   2.5\n 9   0.2 Fair      E     VS2      65.1    61   337   3.9   3.8   2.5\n10   0.2 Very Good H     VS1      59.4    61   338   4     4     2.4\n# ℹ 53,930 more rows\n\nCode# Anonymous functions can be written in a more concise way\n# function(x){ length(unique(x)) } OR ~ length(unique(.x))\n# function(x,y){ length(unique(x + y)) } OR ~ length(unique(.x + .y))\n\n\nWhen we look at the documentation for across(), we see that the .cols argument specifies which variables we want to transform, and it has a &lt;tidy-select&gt; tag. This means that the syntax we use for .cols follows certain rules.\nLet’s click this link to explore the possibilities for selecting variables.\n\nRead through the “Overview of selection features” section to get an overall sense of the many ways to select variables.\nNavigate back to the across() documentation and read through the Examples section at the bottom. Click the “Run examples” link to view the output for all of the examples.\n\nWhat if we wanted to perform multiple transformations on each of many variables?\nWithin the different values of diamond cut, let’s summarize the mean, median, and standard deviation of the numeric variables. When we look at the .fns argument in the across() documentation, we see that we can provide a list of functions:\n\nCodediamonds |&gt;\n    group_by(cut) |&gt; \n    summarize(across(.cols = where(is.numeric), .fns = list(mean = mean, med = median, sd = sd)))\n\n# A tibble: 5 × 22\n  cut     carat_mean carat_med carat_sd depth_mean depth_med depth_sd table_mean\n  &lt;ord&gt;        &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 Fair         1.05       1       0.516       64.0      65      3.64        59.1\n2 Good         0.849      0.82    0.454       62.4      63.4    2.17        58.7\n3 Very G…      0.806      0.71    0.459       61.8      62.1    1.38        58.0\n4 Premium      0.892      0.86    0.515       61.3      61.4    1.16        58.7\n5 Ideal        0.703      0.54    0.433       61.7      61.8    0.719       56.0\n# ℹ 14 more variables: table_med &lt;dbl&gt;, table_sd &lt;dbl&gt;, price_mean &lt;dbl&gt;,\n#   price_med &lt;dbl&gt;, price_sd &lt;dbl&gt;, x_mean &lt;dbl&gt;, x_med &lt;dbl&gt;, x_sd &lt;dbl&gt;,\n#   y_mean &lt;dbl&gt;, y_med &lt;dbl&gt;, y_sd &lt;dbl&gt;, z_mean &lt;dbl&gt;, z_med &lt;dbl&gt;,\n#   z_sd &lt;dbl&gt;\n\n\nWhat does the list of functions look like? What is the structure of this list object?\n\nCodelist_of_fcts &lt;- list(mean = mean, med = median, sd = sd)\nlist_of_fcts\n\n$mean\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x10a2ba060&gt;\n&lt;environment: namespace:base&gt;\n\n$med\nfunction (x, na.rm = FALSE, ...) \nUseMethod(\"median\")\n&lt;bytecode: 0x109780550&gt;\n&lt;environment: namespace:stats&gt;\n\n$sd\nfunction (x, na.rm = FALSE) \nsqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x), \n    na.rm = na.rm))\n&lt;bytecode: 0x12fc87158&gt;\n&lt;environment: namespace:stats&gt;\n\nCodestr(list_of_fcts)\n\nList of 3\n $ mean:function (x, ...)  \n $ med :function (x, na.rm = FALSE, ...)  \n $ sd  :function (x, na.rm = FALSE)",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11 Loops+Iter P1</span>"
    ]
  },
  {
    "objectID": "src/ica/11-iteration-1-notes.html#exercises-1",
    "href": "src/ica/11-iteration-1-notes.html#exercises-1",
    "title": "11 Loops+Iter P1",
    "section": "Exercises",
    "text": "Exercises\nUsing the diamonds dataset:\n\nTransform the x, y, and z columns so that the units of millimeters are displayed (e.g., “4.0 mm”).\n\n\nCodediamonds |&gt; \n  summarize(across(.cols = c(x, y, z), .fns = function(x) {str_c(x, \" mn\")}))\n\n# A tibble: 53,940 × 3\n   x       y       z      \n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n 1 3.95 mn 3.98 mn 2.43 mn\n 2 3.89 mn 3.84 mn 2.31 mn\n 3 4.05 mn 4.07 mn 2.31 mn\n 4 4.2 mn  4.23 mn 2.63 mn\n 5 4.34 mn 4.35 mn 2.75 mn\n 6 3.94 mn 3.96 mn 2.48 mn\n 7 3.95 mn 3.98 mn 2.47 mn\n 8 4.07 mn 4.11 mn 2.53 mn\n 9 3.87 mn 3.78 mn 2.49 mn\n10 4 mn    4.05 mn 2.39 mn\n# ℹ 53,930 more rows\n\n\n\nConvert all numeric columns into character columns.\n\nHint: type is. and hit Tab in the Console. Scroll through the function options. Do the same with as.\n\n\n\n\n\nCodediamonds |&gt; \n  mutate(across(.cols = where(is.numeric), .fns = as.character))\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price x     y     z    \n   &lt;chr&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 0.23  Ideal     E     SI2     61.5  55    326   3.95  3.98  2.43 \n 2 0.21  Premium   E     SI1     59.8  61    326   3.89  3.84  2.31 \n 3 0.23  Good      E     VS1     56.9  65    327   4.05  4.07  2.31 \n 4 0.29  Premium   I     VS2     62.4  58    334   4.2   4.23  2.63 \n 5 0.31  Good      J     SI2     63.3  58    335   4.34  4.35  2.75 \n 6 0.24  Very Good J     VVS2    62.8  57    336   3.94  3.96  2.48 \n 7 0.24  Very Good I     VVS1    62.3  57    336   3.95  3.98  2.47 \n 8 0.26  Very Good H     SI1     61.9  55    337   4.07  4.11  2.53 \n 9 0.22  Fair      E     VS2     65.1  61    337   3.87  3.78  2.49 \n10 0.23  Very Good H     VS1     59.4  61    338   4     4.05  2.39 \n# ℹ 53,930 more rows",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11 Loops+Iter P1</span>"
    ]
  },
  {
    "objectID": "src/ica/11-iteration-1-notes.html#case-study-performing-many-different-versions-of-an-analysis",
    "href": "src/ica/11-iteration-1-notes.html#case-study-performing-many-different-versions-of-an-analysis",
    "title": "11 Loops+Iter P1",
    "section": "Case study: performing many different versions of an analysis",
    "text": "Case study: performing many different versions of an analysis\nOne of my most common use cases for writing functions and iteration/looping is to perform some exploration or modeling repeatedly for several different “settings”.\nFor example, our broad goal might be to fit a linear regression model to our data. However, there are often multiple choices that we have to make in practice:\n\nKeep missing values or fill them in (imputation)?\nFit the model only on a certain group of cases?\nFilter out outliers in one or more variables?\nTransform certain variables? (e.g., log transformation)\n\nWe can map any number of these choices to arguments in a custom model-fitting function:\n\n\nimpute: TRUE or FALSE\n\nfilter_to: This could be a string description like “All cases”, “Group 1”, or “Groups 1 and 2”\n\n\nCodefit_model &lt;- function(data, impute, filter_to) {\n    if (impute) {\n        data &lt;- some_imputation_function(data)\n    }\n    \n    if (filter_to==\"Group 1\") {\n        data &lt;- data |&gt; \n            filter(group == 1)\n    } else if (filter_to == \"Groups 1 and 2\") {\n        data &lt;- data |&gt; \n            filter(group %in% c(1,2))\n    }\n    \n    lm(y ~ x1 + x2, data = data)\n}\n\n\nThe tidyr package has a useful function called crossing() that is useful for generating argument combinations.\nFor each argument, we specify all possible values for that argument. crossing() generates all combinations:\n\nCodedf_arg_combos &lt;- crossing(\n    impute = c(TRUE, FALSE),\n    filter_to = c(\"All cases\", \"Group 1\", \"Groups 1 and 2\")\n)\ndf_arg_combos\n\n# A tibble: 6 × 2\n  impute filter_to     \n  &lt;lgl&gt;  &lt;chr&gt;         \n1 FALSE  All cases     \n2 FALSE  Group 1       \n3 FALSE  Groups 1 and 2\n4 TRUE   All cases     \n5 TRUE   Group 1       \n6 TRUE   Groups 1 and 2\n\nCode# Another example\ncrossing(\n    option1 = 1:3,\n    option2 = c(TRUE, FALSE),\n    option3 = c(\"low\", \"medium\", \"high\")\n)\n\n# A tibble: 18 × 3\n   option1 option2 option3\n     &lt;int&gt; &lt;lgl&gt;   &lt;chr&gt;  \n 1       1 FALSE   high   \n 2       1 FALSE   low    \n 3       1 FALSE   medium \n 4       1 TRUE    high   \n 5       1 TRUE    low    \n 6       1 TRUE    medium \n 7       2 FALSE   high   \n 8       2 FALSE   low    \n 9       2 FALSE   medium \n10       2 TRUE    high   \n11       2 TRUE    low    \n12       2 TRUE    medium \n13       3 FALSE   high   \n14       3 FALSE   low    \n15       3 FALSE   medium \n16       3 TRUE    high   \n17       3 TRUE    low    \n18       3 TRUE    medium \n\n\nWith iteration in our toolbox, we can iterate the fit_model() function over the combinations in df_arg_combos.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11 Loops+Iter P1</span>"
    ]
  },
  {
    "objectID": "src/ica/11-iteration-1-notes.html#exercises-2",
    "href": "src/ica/11-iteration-1-notes.html#exercises-2",
    "title": "11 Loops+Iter P1",
    "section": "Exercises",
    "text": "Exercises\nGoal: In the diamonds dataset, we want to understand the relationship between price and size (carat). We want to explore variation along two choices:\n\nThe variables included in the model. We’ll explore 3 sets of variables:\n\nNo further variables (just price and carat)\nAdjusting for cut\n\nAdjusting for cut and clarity\n\nAdjusting for cut, clarity, and color\n\n\n\nWhether or not to remove outliers in the carat variable. We’ll define outliers as cases whose carat is over 3 SDs away from the mean.\n\nWork with your partner on the following exercises. As you work, make note of what is challenging and any helpful thought processes/strategies that arise from the collaboration.\n\nUse crossing() to create the data frame of argument combinations for our analyses. Call it df_arg_combos. Note that you can create a list of formula objects in R with c(y ~ x1, y ~ x1 + x2). (Something like this will be the right hand side of an argument to crossing().)\n\n\nCodedf_arg_combos &lt;- crossing(\n mod_formula = c(price ~ carat, price ~ carat + cut, price ~ carat + cut + clarity, price ~ carat + cut + clarity + color), \n  remove_outliers = c(TRUE, FALSE)\n)\n\n\n\nWrite a function called remove_outliers that removes outliers in a dataset. The user should be able to supply the dataset (data), the variable to remove outliers in (what_var), and a threshold on the number of SDs away from the mean used to define outliers (sd_thresh). Write your function so that it runs as follows: remove_outliers(diamonds, what_var = carat, sd_thresh = 3).\n\n\nCoderemove_outliers &lt;- function(data, what_var, sd_thresh) {\n  data |&gt; \n    mutate(z_score = ({{what_var}} - mean({{what_var}}, na.rm = TRUE)) / sd({{what_var}}, na.rm = TRUE)) |&gt; \n    filter(z_score &lt;= sd_thresh)\n}\n\nremove_outliers(diamonds, what_var = carat, sd_thresh = 3)\n\n# A tibble: 53,501 × 11\n   carat cut       color clarity depth table price     x     y     z z_score\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43   -1.20\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31   -1.24\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31   -1.20\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63   -1.07\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75   -1.03\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48   -1.18\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47   -1.18\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53   -1.13\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49   -1.22\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39   -1.20\n# ℹ 53,491 more rows\n\n\n\nWrite a function called fit_model that implements the different settings for our diamonds analysis.\n\n\nCodefit_model &lt;- function(data, mod_formula, remove_outliers) {\n    # remove_outliers is a TRUE/FALSE flag of whether or not to remove outliers\n  if(remove_outliers) {\n    data_clean &lt;- remove_outliers(data, what_var = carat, sd_thresh = 3)  # This function implements our specific use case: outliers are cases that are 3 SDs away from the mean for the carat variable\n  } else {\n    data_clean &lt;- data\n  }\n   lm(mod_formula, data = data_clean)\n    # Use mod_formula as the first argument of lm()\n}\n\n\n\nWrite a for loop that stores the fitted linear models from all versions of the analysis.\n\nRecall that you can pull out the contents of a single data frame column in many ways. For a data frame df with a variable named x:\n\ndf$x\ndf |&gt; pull(x)\ndf[[\"x\"]]\n\nNote that in df_arg_combos:\n\n\nmod_formula: this column is a list and you can extract the 1st element with [[1]]\n\n\nremove_outliers: this column is a vector and you can extract the 1st element with [1]\n\n\n\nCodevec_for_mod &lt;- vector(mode = \"list\", length = nrow(df_arg_combos))\n\nfor (i in seq_along(vec_for_mod)) {\n  this_formula &lt;- df_arg_combos$mod_formula[[i]] # Double [[ for the **list** of formulas\n  this_remove_outliers &lt;- df_arg_combos$remove_outliers[i] # Single [ for the **atomic vector** of logicals\n    vec_for_mod[[i]] &lt;- fit_model(\n        data = diamonds,\n        mod_formula = this_formula,\n        remove_outliers = this_remove_outliers\n    )\n}",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11 Loops+Iter P1</span>"
    ]
  },
  {
    "objectID": "src/ica/11-iteration-1-notes.html#done",
    "href": "src/ica/11-iteration-1-notes.html#done",
    "title": "11 Loops+Iter P1",
    "section": "Done!",
    "text": "Done!\n\nCheck the ICA Instructions for how to (a) push your code to GitHub and (b) update your portfolio website",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11 Loops+Iter P1</span>"
    ]
  },
  {
    "objectID": "src/exams/Exam_summary_sheets.html",
    "href": "src/exams/Exam_summary_sheets.html",
    "title": "Exam Summary Sheets",
    "section": "",
    "text": "exam_1_summary_sheet.jpeg",
    "crumbs": [
      "Exams",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exam Summary Sheets</span>"
    ]
  },
  {
    "objectID": "src/exams/exam_1/exam_1.html",
    "href": "src/exams/exam_1/exam_1.html",
    "title": "Exam 1: Exploring Compound CRSs",
    "section": "",
    "text": "Setup\nCodelibrary(tidyverse)\nlibrary(sf) # tools for working with spatial vector data (GIS functionality, mapping)\nlibrary(elevatr) # access to raster elevation maps\nlibrary(terra)\nlibrary(stars)\nlibrary(tidycensus) # spatial data for the US with census information\nlibrary(USAboundaries) # access to boundaries for US states, counties, zip codes, and congressional districts\nCompound Coordinate Reference systems combine two or more single non-compound coordinate reference systems to represent locations, typically a horizontal and vertical CRS. The two systems have different datum and projections but are organized into an ordered tuple. They help to ensure a more accurate and consistent representation across different systems and applications, create complex referencing and describe 3D locations.",
    "crumbs": [
      "Exams",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Exam 1: Exploring Compound CRSs</span>"
    ]
  },
  {
    "objectID": "src/exams/exam_1/exam_1.html#creating-a-compound-crs",
    "href": "src/exams/exam_1/exam_1.html#creating-a-compound-crs",
    "title": "Exam 1: Exploring Compound CRSs",
    "section": "Creating a Compound CRS",
    "text": "Creating a Compound CRS\nYou first have to define the two different maping components, the vertical and the horizontal CRS. EPSGs or WKT (well-known text) both work just fine as references. In the example below I just plugged two different CRS’s in and saved them, but you would want to specify which horizontal and which vertical systems you wanted to use for your data. Horizontal normally comes with 2 dimensions latitude and longitude while vertical comes with one dimension elevation. The CS variable of the CRS should tell you what type of model and dimensions it is using. For example NAD83 is ellipsoidal, 2, while NAVD88 height or EPSG:5703 is vertical, 1.\n\nCodehorizontal &lt;- st_crs(\"NAD83\")\n\nvertical &lt;- st_crs(5703)\n\n\nYou would then combine the two different CRS’s using their WKT into a compound CRS manually. To do so you need to paste “COMPOUNDCRS[” that tells the system it is a compound CRS, then the two different WKT of the CRS’s you wish to use sperated by a comma. In order to get the WKT of each of those CRS’s you can use st_as_text(). Then end the string with ”]” and seperate with ““. Then turn the string into an actual CRS by using st_crs.\n\nCodecompound_wkt &lt;- paste(\"COMPOUNDCRS[\",\n                      st_as_text(horizontal), \",\",\n                      st_as_text(vertical),\n                      \"]\", sep = \"\")\n\ncompound_crs &lt;- st_crs(compound_wkt)\n\n\nYou can then use this CRS to define the coordinate system for an sf object with coordinates in 3 dimensions (x,y,z); you can assign the compound CRS using st_set_crs(). For example below I created a small data frame and turned the drame into a sf object using st_as_sf. I then assigned the CRS object to the sf data, and you can see in the call that it has 3 dimensions and calls on my compound CRS.\n\nCodecrs_data &lt;- data.frame(\n  x = 2560, 7000, \n  y = 735, 100, \n  z = 53, 10\n)\n\nsf_point &lt;- st_as_sf(crs_data, coords = c(\"x\", \"y\", \"z\"))\n\nst_crs(sf_point) &lt;- compound_crs\n\nsf_point\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 2560 ymin: 735 xmax: 2560 ymax: 735\nGeodetic CRS:  GEOGCS\n  X7000 X100 X10              geometry\n1  7000  100  10 POINT Z (2560 735 53)\n\n\nYou can then use the compound CRS to create a map\n\nCodeggplot() + \n  geom_sf(data = sf_point) +\n  ggtitle(\"3D Point Mapped with a Compound CRS\")\n\n\n\n\n\n\n\nIt looks 2D because, well a computer is 2D. However if you are thinking about certain locations where height is a factor that needs to be considered such as surveying it increases the accuracy and gives more comprehensive descriptios of features as they are more spatially and/or temporally consistent.",
    "crumbs": [
      "Exams",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Exam 1: Exploring Compound CRSs</span>"
    ]
  },
  {
    "objectID": "src/exams/exam_1/exam_1.html#transforming-compound-crs-data",
    "href": "src/exams/exam_1/exam_1.html#transforming-compound-crs-data",
    "title": "Exam 1: Exploring Compound CRSs",
    "section": "Transforming Compound CRS data",
    "text": "Transforming Compound CRS data\nIf you have data that does have 3 dimensions using sf_transform() you can change it to your compound CRS. In this case I used 4979, which is WGS 84, but you would put your data in where the number is to extract its CRS. This also works if it only has a horizontal component and you want to add a vertical CRS, which is called reprojecting the data.\n\nCode#source_crs &lt;- st_crs(4979)\n#transformed_data &lt;- st_transform(source_crs, crs = compound_crs)\n\n\nsf_transform() will attempt to preserve the vertical component if your a transforming your data into a compound CRS with one, but vertical data is lost if you translate to a 2D CRS. st_transform often works if you only need to transform the horizontal part of the of the compound CRS, but if your two vertical components don’t match up that becomes a little more tricky. Often times it requires specialized tools beyond the scope of sf.",
    "crumbs": [
      "Exams",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Exam 1: Exploring Compound CRSs</span>"
    ]
  },
  {
    "objectID": "src/appx/appx-sample1.html",
    "href": "src/appx/appx-sample1.html",
    "title": "Appendix A — Appendix Sample 1",
    "section": "",
    "text": "Add content here",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix Sample 1</span>"
    ]
  },
  {
    "objectID": "src/appx/appx-sample2.html",
    "href": "src/appx/appx-sample2.html",
    "title": "Appendix B — Appendix Sample 2",
    "section": "",
    "text": "Add content here",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Appendix Sample 2</span>"
    ]
  },
  {
    "objectID": "mm/mm.html",
    "href": "mm/mm.html",
    "title": "Appendix C — Mind Maps",
    "section": "",
    "text": "Creativity",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Mind Maps</span>"
    ]
  },
  {
    "objectID": "mm/mm.html#creativity",
    "href": "mm/mm.html#creativity",
    "title": "Appendix C — Mind Maps",
    "section": "",
    "text": "0808-mind_map_example.jpg\n\n\n\n\n\n0905-Review_mind_map.jpeg\n\n\n\n\n\n0908-Data_Visualization.jpeg\n\n\n\n\n\n0910-Advanced_spatial_data_viz.jpeg\n\n\n\n\n\n0916-Adv_Data_wrangling_P1.jpeg\n\n\n\n\n\n0922-Adv_Data_wrangling_P2.jpeg\n\n\n\n\n\n0929-Missing_Data.jpeg\n\n\n\n\n\n1001-Functions.jpeg\n\n\n\n\n\n1003-BaseR.jpeg\n\n\n\n\n\n1006-Iterations.jpeg",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Mind Maps</span>"
    ]
  }
]